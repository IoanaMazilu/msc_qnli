#!/bin/bash

#SBATCH --partition=gpu
#SBARCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --gpus=1
#SBATCH --job-name=qnli_fine_tuning
#SBATCH --time=30:00:00
#SBATCH --output=slurm_output_%A.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=ioana.mazilu@student.uva.nl

module purge
module load 2022
module load Python/3.10.4-GCCcore-11.3.0

# create input and output directories on the scratch disk
mkdir -p "$TMPDIR"/output/"$SLURM_JOBID"/
mkdir -p "$TMPDIR"/input/completion/
mkdir -p "$TMPDIR"/input/cached_pretrained_model/

# move dataset files to scratch
cd $HOME/qnli/quant_nli
cp -r data/finetuning/completion/train_all.csv "$TMPDIR"/input/completion/train_all.csv
cp -r data/finetuning/completion/val_all.csv "$TMPDIR"/input/completion/val_all.csv

# set some variables
MODEL_NAME="codellama-instruct"
MODEL_SIZE="13B"
HF_REPO="codellama/CodeLlama-13b-Instruct-hf"
LEARNING_RATE="2e-5"
WARMUP_RATIO="0.025"
PAD_TOKEN="pad"
EPOCHS="6"
PROMPT_TEMPLATE="inputs_only"

# run fine-tuning script
cd $HOME/qnli/quant_nli/src/finetuning
export HF_TOKEN="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"
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
python finetune.py --epochs "$EPOCHS" \
                  --pad-token "$PAD_TOKEN" \
                  --job-id "$SLURM_JOBID" \
                  --output-path "$TMPDIR" \
                  --input-path "$TMPDIR" \
                  --model-name "$MODEL_NAME" \
                  --model-size "$MODEL_SIZE" \
                  --hf-repo "$HF_REPO" \
                  --learning-rate "$LEARNING_RATE" \
                  --warmup-ratio "$WARMUP_RATIO" \
                  --prompt-template "$PROMPT_TEMPLATE"

# copy output files from scratch back to the home directory, also copy job output file
mkdir -p $HOME/qnli/quant_nli/results/"$MODEL_NAME"/"$MODEL_SIZE"/"$SLURM_JOBID"/
cd "$TMPDIR"/output/"$SLURM_JOBID"/
cp -r . $HOME/qnli/quant_nli/results/"$MODEL_NAME"/"$MODEL_SIZE"/"$SLURM_JOBID"/
cd "$HOME"/qnli/quant_nli/jobs/finetuning/
cp slurm_output_"$SLURM_JOBID".out $HOME/qnli/quant_nli/results/"$MODEL_NAME"/"$MODEL_SIZE"/"$SLURM_JOBID"/
cd "$HOME"/qnli/quant_nli/src/finetuning/
cp training_configuration.py $HOME/qnli/quant_nli/results/"$MODEL_NAME"/"$MODEL_SIZE"/"$SLURM_JOBID"/