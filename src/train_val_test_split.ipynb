{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "root_path = os.path.dirname(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXTRACT SAMPLES FROM LILA (they use Task 7 from NumGLUE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "lila_path = os.path.join(root_path, \"data\", \"lila\", \"all\", \"NumGLUE_Type_7_crowdsourced.json\")\n",
    "\n",
    "with open(lila_path, 'r') as f:\n",
    "    lila_equate = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['Source', 'Categories', 'Instances', 'Metadata'])"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_equate.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "lila_equate = lila_equate[\"Instances\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "6325"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lila_equate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['Input', 'Output Program', 'Output Answer', 'split'])"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_equate[0].keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "' \"statement 1\": In a deck of less than 72 cards , how many ways are there to select 13 Spade and 13 heart cards without repetition ?, \"statement 2\" :In a deck of 52 cards , how many ways are there to select 13 Spade and 13 heart cards without repetition ?, \"options: \" Entailment or contradiction or neutral?'"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_equate[0][\"Input\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "\"RajeshHaveQuestionsS1 = 41 \\nRajeshHaveQuestionsS2 = 31\\nif RajeshHaveQuestionsS1 is None or RajeshHaveQuestionsS2 is None:\\n   print('neutral')\\nelif RajeshHaveQuestionsS1==RajeshHaveQuestionsS2:\\n      print('Entailment')\\nelif RajeshHaveQuestionsS1!=RajeshHaveQuestionsS2:\\n     print('contradiction')\""
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_equate[1][\"Output Program\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "train, val, test = [], [], []\n",
    "for instance in lila_equate:\n",
    "    new_instance = dict()\n",
    "    inputs = instance[\"Input\"]\n",
    "    premise_hypothesis = inputs.split(\", \\\"statement 2\\\" :\")\n",
    "    try:\n",
    "        premise, hypothesis = premise_hypothesis[0], premise_hypothesis[1]\n",
    "        premise = premise.split(\"\\\"statement 1\\\": \")[-1]\n",
    "        hypothesis = hypothesis.split(\", \\\"options: \\\"\")[0]\n",
    "        new_instance.update({\"premise\": premise,\n",
    "                             \"hypothesis\": hypothesis,\n",
    "                             \"lila_label\": instance[\"Output Answer\"][0],\n",
    "                             \"lila_script\": instance[\"Output Program\"][0]})\n",
    "        split = instance[\"split\"]\n",
    "        if split == \"train\":\n",
    "            train.append(new_instance)\n",
    "        elif split == \"dev\":\n",
    "            val.append(new_instance)\n",
    "        else:\n",
    "            test.append(new_instance)\n",
    "    except IndexError:\n",
    "        print(f\"ERROR extracting inputs:\\n{inputs}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4302 806 1217\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str):\n",
    "    return re.sub(r'\\s+', ' ', text.lower().replace(\"\\n\", \"\")).strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  In a deck of less than 72 cards , how many way...   \n1  If out of 41 questions solved by Rajesh 37 que...   \n2  Alice drives at a constant speed of 30 km per ...   \n3               Mary is 22 years younger than Albert   \n4  Assuming that Karen drives at an average speed...   \n\n                                          hypothesis     lila_label  \\\n0  In a deck of 52 cards , how many ways are ther...        neutral   \n1  If out of 31 questions solved by Rajesh 37 que...  contradiction   \n2  Alice drives at a constant speed of 20 km per ...  contradiction   \n3               Mary is 72 years younger than Albert  contradiction   \n4  Assuming that Karen drives at an average speed...        neutral   \n\n                                         lila_script  \n0  DeckOfCardStatement1= None \\nDeckOfCardStateme...  \n1  RajeshHaveQuestionsS1 = 41 \\nRajeshHaveQuestio...  \n2  DrivesSpeedS1 = 30\\nDrivesSpeedS2 = 20\\nif Dri...  \n3  AgeS1 = 22\\nAgeS2 = 72\\nif AgeS1 is None or Ag...  \n4  \\nDriveS2 = 60\\nDriveS1 = None\\nif DriveS1 is ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lila_label</th>\n      <th>lila_script</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In a deck of less than 72 cards , how many way...</td>\n      <td>In a deck of 52 cards , how many ways are ther...</td>\n      <td>neutral</td>\n      <td>DeckOfCardStatement1= None \\nDeckOfCardStateme...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>If out of 41 questions solved by Rajesh 37 que...</td>\n      <td>If out of 31 questions solved by Rajesh 37 que...</td>\n      <td>contradiction</td>\n      <td>RajeshHaveQuestionsS1 = 41 \\nRajeshHaveQuestio...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alice drives at a constant speed of 30 km per ...</td>\n      <td>Alice drives at a constant speed of 20 km per ...</td>\n      <td>contradiction</td>\n      <td>DrivesSpeedS1 = 30\\nDrivesSpeedS2 = 20\\nif Dri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mary is 22 years younger than Albert</td>\n      <td>Mary is 72 years younger than Albert</td>\n      <td>contradiction</td>\n      <td>AgeS1 = 22\\nAgeS2 = 72\\nif AgeS1 is None or Ag...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Assuming that Karen drives at an average speed...</td>\n      <td>Assuming that Karen drives at an average speed...</td>\n      <td>neutral</td>\n      <td>\\nDriveS2 = 60\\nDriveS1 = None\\nif DriveS1 is ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_train = pd.DataFrame(train)\n",
    "lila_test = pd.DataFrame(test)\n",
    "lila_val = pd.DataFrame(val)\n",
    "lila_val.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "lila_val[\"lila_label\"] = lila_val[\"lila_label\"].apply(lambda label: label.lower())\n",
    "lila_val[\"lila_label\"] = lila_val[\"lila_label\"].apply(lambda label: label.lower())\n",
    "lila_val[\"lila_label\"] = lila_val[\"lila_label\"].apply(lambda label: label.lower())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "for col in [\"premise\", \"hypothesis\"]:\n",
    "    lila_train[f\"clean_{col}\"] = lila_train[col].apply(lambda text: clean_text(text))\n",
    "    lila_test[f\"clean_{col}\"] = lila_test[col].apply(lambda text: clean_text(text))\n",
    "    lila_val[f\"clean_{col}\"] = lila_val[col].apply(lambda text: clean_text(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "lila_train = lila_train[~lila_train.duplicated(subset=['clean_premise', 'clean_hypothesis'])]\n",
    "lila_test = lila_test[~lila_test.duplicated(subset=['clean_premise', 'clean_hypothesis'])]\n",
    "lila_val = lila_val[~lila_val.duplicated(subset=['clean_premise', 'clean_hypothesis'])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4302 1217 806\n"
     ]
    }
   ],
   "source": [
    "print(lila_train.shape[0], lila_test.shape[0], lila_val.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save train-val-test splits from LILA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['lila_label', 'lila_script', 'clean_premise', 'clean_hypothesis'], dtype='object')"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_train.drop([\"premise\", \"hypothesis\"], axis=1, inplace=True)\n",
    "lila_test.drop([\"premise\", \"hypothesis\"], axis=1, inplace=True)\n",
    "lila_val.drop([\"premise\", \"hypothesis\"], axis=1, inplace=True)\n",
    "lila_test.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "lila_train.to_csv(os.path.join(root_path, \"data\", \"lila\", \"lila_train.csv\"), index=False)\n",
    "lila_test.to_csv(os.path.join(root_path, \"data\", \"lila\", \"lila_test.csv\"), index=False)\n",
    "lila_val.to_csv(os.path.join(root_path, \"data\", \"lila\", \"lila_val.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check for duplicates accross the entire LILA data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "lila_all = []\n",
    "lila_all.extend(train)\n",
    "lila_all.extend(test)\n",
    "lila_all.extend(val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "6325"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_all_df = pd.DataFrame(lila_all)\n",
    "lila_all_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_all_df[lila_all_df.duplicated(subset=[\"clean_premise\", \"clean_hypothesis\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_all_df[lila_all_df.duplicated(subset=[\"clean_premise\", \"clean_hypothesis\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge LILA samples with EQUATE samples per EQUATE dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "(4302, 4)"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "def extract_completion(full_script: str):\n",
    "    \"\"\"\n",
    "    Extract only script from the script .py files (remove comments containing the premise, hypothesis and EQUATE label.\n",
    "    :param full_script: the script as saved in a .py file in `data/generated/[dataset]/gpt4`\n",
    "    :return: the script without the removed parts or np.nan for a missing script\n",
    "    \"\"\"\n",
    "    if full_script is None or full_script == \"-\" or pd.isna(full_script):\n",
    "        return np.nan\n",
    "    try:\n",
    "        lines = full_script.split(\"\\n\")\n",
    "    except AttributeError:\n",
    "        print(\"#############\", \"\\n\", full_script)\n",
    "    idx = 0\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.startswith(\"# Golden Label:\"):\n",
    "            break\n",
    "    return \"\\n\".join(lines[idx+3:-1])  # skip label and 2 blank lines\n",
    "\n",
    "def add_script_to_df(df: pd.DataFrame, dataset:str):\n",
    "    labels_df = pd.read_csv(os.path.join(root_path, \"data\", \"generated\", dataset, \"gpt4\", \"results_overview.csv\"))\n",
    "    print(labels_df[labels_df[\"py_file_content\"]==\"-\"].shape[0])\n",
    "    print(labels_df[labels_df[\"py_file_content\"]==\"-\"][\"sample_index\"].unique())\n",
    "    labels_df[\"completion\"] = labels_df[\"py_file_content\"].apply(lambda full_script: extract_completion(full_script))\n",
    "    return pd.merge(df, labels_df, on=\"sample_index\", how=\"left\").drop([\"llm_answer\", \"py_file_content\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######NewsNLI#######\n",
      "Total instances: 963\n",
      "0\n",
      "[]\n",
      "Index(['sample_index', 'generated_label', 'error_message', 'golden_label',\n",
      "       'premise', 'hypothesis', 'clean_premise', 'clean_hypothesis',\n",
      "       'completion'],\n",
      "      dtype='object')\n",
      "963\n",
      "Rest (train/val): 963\n",
      "#######RTE_Quant#######\n",
      "Total instances: 165\n",
      "0\n",
      "[]\n",
      "Index(['sample_index', 'generated_label', 'error_message', 'golden_label',\n",
      "       'premise', 'hypothesis', 'clean_premise', 'clean_hypothesis',\n",
      "       'completion'],\n",
      "      dtype='object')\n",
      "165\n",
      "Rest (train/val): 165\n",
      "#######RedditNLI#######\n",
      "Total instances: 247\n",
      "0\n",
      "[]\n",
      "Index(['sample_index', 'generated_label', 'error_message', 'golden_label',\n",
      "       'premise', 'hypothesis', 'clean_premise', 'clean_hypothesis',\n",
      "       'completion'],\n",
      "      dtype='object')\n",
      "247\n",
      "Rest (train/val): 247\n",
      "#######StressTest#######\n",
      "Total instances: 6938\n",
      "11\n",
      "[5699. 5700. 5701. 5702. 5703. 5704. 5705. 5706. 5707. 5959. 5960.]\n",
      "Index(['sample_index', 'generated_label', 'error_message', 'golden_label',\n",
      "       'premise', 'hypothesis', 'clean_premise', 'clean_hypothesis',\n",
      "       'completion'],\n",
      "      dtype='object')\n",
      "6938\n",
      "Test: 1215\n",
      "Test: lila_label\n",
      "contradiction    465\n",
      "Entailment       382\n",
      "neutral          368\n",
      "Name: count, dtype: int64\n",
      "Available for FT: 3689\n",
      "Rest (train/val): 5723\n",
      "#######AWPNLI#######\n",
      "Total instances: 722\n",
      "0\n",
      "[]\n",
      "Index(['sample_index', 'generated_label', 'error_message', 'golden_label',\n",
      "       'premise', 'hypothesis', 'clean_premise', 'clean_hypothesis',\n",
      "       'completion'],\n",
      "      dtype='object')\n",
      "722\n",
      "Rest (train/val): 722\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"NewsNLI\", \"RTE_Quant\", \"RedditNLI\", \"StressTest\", \"AWPNLI\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"#######{dataset}#######\")\n",
    "    equate_df = pd.read_csv(os.path.join(root_path, \"data\", \"equate_labelled\", f\"cleaned_{dataset}_gpt4.csv\"))\n",
    "    print(f\"Total instances: {equate_df.shape[0]}\")\n",
    "    equate_df = add_script_to_df(equate_df, dataset)\n",
    "    print(equate_df.columns)\n",
    "    print(equate_df.shape[0])\n",
    "    equate_lila_train = pd.merge(equate_df, lila_train, on=[\"clean_premise\", \"clean_hypothesis\"], how=\"inner\")\n",
    "    equate_lila_test = pd.merge(equate_df, lila_test, on=[\"clean_premise\", \"clean_hypothesis\"], how=\"inner\")\n",
    "    if equate_lila_test.shape[0] > 0:\n",
    "        print(f\"Test: {equate_lila_test.shape[0]}\")\n",
    "        print(f\"Test: {equate_lila_test['lila_label'].value_counts()}\")\n",
    "        rest_equate = equate_df[~equate_df[\"sample_index\"].isin(equate_lila_test[\"sample_index\"].unique())]\n",
    "        print(f\"Available for FT: {rest_equate[rest_equate['golden_label'] == rest_equate['generated_label']].shape[0]}\")\n",
    "    else:\n",
    "        rest_equate = equate_df\n",
    "    print(f\"Rest (train/val): {rest_equate.shape[0]}\")\n",
    "    output_path = os.path.join(root_path, \"data\", \"lila-equate\", dataset)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    if dataset == \"StressTest\":\n",
    "        rest_equate.to_csv(os.path.join(output_path, \"train_val.csv\"), index=False)\n",
    "    else:\n",
    "        rest_equate.to_csv(os.path.join(output_path, \"all.csv\"), index=False)\n",
    "    # we need the train samples for code quality comparison\n",
    "    if equate_lila_train.shape[0] > 0:\n",
    "        equate_lila_train.to_csv(os.path.join(output_path, \"cc_train.csv\"), index=False)\n",
    "    if equate_lila_test.shape[0] > 0:\n",
    "        equate_lila_test.to_csv(os.path.join(output_path, \"test.csv\"), index=False)\n",
    "        equate_lila_test.drop([\"lila_label\", \"lila_script\"], axis=1, inplace=True)\n",
    "        equate_lila_test.to_csv(os.path.join(root_path, 'data', \"finetuning\", \"StressTest\", \"test.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SPLITS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "{'StressTest': 1215, 'AWPNLI': 180, 'RTE_Quant': 41, 'RedditNLI': 62, 'NewsNLI': 242}\n",
      "{'StressTest': 0.7, 'AWPNLI': 0.1, 'RTE_Quant': 0.02, 'RedditNLI': 0.04, 'NewsNLI': 0.14}\n",
      "StressTest FT data: 3689\n",
      "Train: 3135; Val: 554\n",
      "AWPNLI FT data: 516\n",
      "Train: 438; Val: 78\n",
      "RTE_Quant FT data: 97\n",
      "Train: 82; Val: 15\n",
      "RedditNLI FT data: 135\n",
      "Train: 114; Val: 21\n",
      "NewsNLI FT data: 548\n",
      "Train: 465; Val: 83\n",
      "Total: 4985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "total_datasets = {\"StressTest\": 6938, \"AWPNLI\": 722, \"RTE_Quant\": 166, \"RedditNLI\": 250, \"NewsNLI\": 968}\n",
    "test_pctg = 0.25\n",
    "test_sizes = {\"StressTest\": 1215, \"AWPNLI\": int(test_pctg*722), \"RTE_Quant\": int(test_pctg*166), \"RedditNLI\": int(test_pctg*250), \"NewsNLI\": int(test_pctg*968)}\n",
    "total_test_set_size = np.sum([set_size for _, set_size in test_sizes.items()])\n",
    "print(total_test_set_size)\n",
    "print(test_sizes)\n",
    "test_pctgs = {ds: round(subset/total_test_set_size, 2) for ds, subset in test_sizes.items()}\n",
    "print(test_pctgs)\n",
    "total_ft = 0\n",
    "for dataset, test_size in test_sizes.items():\n",
    "    if dataset != \"StressTest\":\n",
    "        df = pd.read_csv(os.path.join(root_path, \"data\", \"lila-equate\", dataset, \"all.csv\"))\n",
    "        train_val, test = train_test_split(df, test_size=test_size, stratify=df[\"golden_label\"])\n",
    "        ft = train_val[train_val['golden_label']==train_val['generated_label']]\n",
    "        total_ft += ft.shape[0]\n",
    "        train, val = train_test_split(ft, test_size=0.15, stratify=ft[\"golden_label\"])\n",
    "        print(f\"{dataset} FT data: {ft.shape[0]}\")\n",
    "        print(f\"Train: {train.shape[0]}; Val: {val.shape[0]}\")\n",
    "        os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", dataset), exist_ok=True)\n",
    "        test.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"test.csv\"), index=False)\n",
    "        train.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"train.csv\"), index=False)\n",
    "        val.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"val.csv\"), index=False)\n",
    "    else:\n",
    "        os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", dataset), exist_ok=True)\n",
    "        df = pd.read_csv(os.path.join(root_path, \"data\", \"lila-equate\", dataset, \"train_val.csv\"))\n",
    "        ft_data = df[df['golden_label'] == df['generated_label']]\n",
    "        print(f\"{dataset} FT data: {ft_data.shape[0]}\")\n",
    "        train, val = train_test_split(ft_data, test_size=0.15, stratify=ft_data[\"golden_label\"])\n",
    "        print(f\"Train: {train.shape[0]}; Val: {val.shape[0]}\")\n",
    "        train.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"train.csv\"), index=False)\n",
    "        val.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"val.csv\"), index=False)\n",
    "        total_ft += ft_data.shape[0]\n",
    "print(f\"Total: {total_ft}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate prompt and completion features, which will form the dataset for fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN set\n",
      "Samples with no generated script: 3, ([3256 4683 3594])\n",
      "Creating train file.\n",
      "TEST set\n",
      "Samples with no generated script: 2, ([6454 5929])\n",
      "Creating test file.\n",
      "VAL set\n",
      "Samples with no generated script: 3, ([6573 1563 1122])\n",
      "Creating val file.\n"
     ]
    }
   ],
   "source": [
    "from prompts import format_prompt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dataset = \"RTE_Quant\"   # can also use `dataset` variable set above\n",
    "for dataset in [\"StressTest\", \"NewsNLI\", \"RTE_Quant\", \"RedditNLI\", \"AWPNLI\"]:\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        print(f\"{split.upper()} set\")\n",
    "        df = pd.read_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, f\"{split}.csv\"))\n",
    "        missing_scripts = df[df[\"completion\"].isna()][\"sample_index\"]\n",
    "        print(f'Samples with no generated script: {missing_scripts.shape[0]}, ({missing_scripts.unique()})')\n",
    "        print(f\"Creating {split} file.\")\n",
    "        df.dropna(subset=[\"completion\"], inplace=True)\n",
    "        df[\"prompt\"] = df.apply(lambda row: format_prompt(dataset.lower().replace(\"_\", \"\"), {\"premise\": row[\"premise\"], \"hypothesis\": row[\"hypothesis\"]}), axis=1)\n",
    "        df[\"completion\"] = df[\"completion\"].apply(lambda completion: f\"```python\\n{completion}```\")\n",
    "        os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"completion\"), exist_ok=True)\n",
    "        if split != \"test\":\n",
    "            df[[\"completion\", \"prompt\"]].to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"completion\", f\"{split}.csv\"), index=False)\n",
    "        else:\n",
    "            df[[\"sample_index\", \"completion\", \"prompt\"]].to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"completion\", f\"{split}.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "\"### Instruction:\\nYou need to reason about weather a hypothesis entails or contradicts a premise, by generating Python scripts. The scripts should classify the relation between the hypothesis and premise based on the quantitative and textual information mentioned in them. All the quantities and textual details in the hypothesis should be entailed by the information in the premise. First, manually extract all the individual quantities from both of the inputs, as valid numbers. Use the variable name to describe what the quantity measures, based on the context. Then, define a Python function that takes the extracted quantities as arguments. Within the function, use these quantities to perform computations based on the context of the premise and hypothesis. Finally, compare the resulting variables to determine the relationship. If the comparison indicates entailment, return True; for contradiction return False. Remember to include brief comments in the script to explain each step of the reasoning process. To illustrate, consider the following examples:\\nSTART_EXAMPLE\\nPremise: Yesterday I learned 35 verbs and 5 nouns in the morning and 10 verbs in the evening.\\nHypothesis: I learned 5 nouns and less than fifty verbs yesterday.\\nAnswer:\\n```python\\nverbs_morning_premise = 35\\nverbs_evening_premise = 10\\nnouns_premise = 5\\nmax_verbs_hypothesis = 50 \\nnouns_hypothesis = 5\\n\\ndef entailment_or_contradiction(verbs_morning_premise, verbs_evening_premise, nouns_premise, max_verbs_hypothesis, nouns_hypothesis):\\n    # the hypothesis talks about the number of learned nouns and verbs, which are also referenced in the premise\\n    # find the total number of verbs learned from the premise \\n    total_verbs_premise = verbs_morning_premise + verbs_evening_premise\\n    # check if the total verbs form the hypothesis is more than 'verbs_evening_premise' and if the number of nouns is equal between the premise and hypothesis\\n    return max_verbs_hypothesis > total_verbs_premise and nouns_premise == nouns_hypothesis\\n\\nprint(entailment_or_contradiction(verbs_morning_premise, verbs_evening_premise, nouns_premise, max_verbs_hypothesis, nouns_hypothesis))\\n```\\nEND_EXAMPLE\\n\\nSTART_EXAMPLE\\nPremise: She bought 10 crayons and received 5 more from her desk mate.\\nHypothesis: She has 10 crayons in total.\\nAnswer:\\n```python\\nbought_crayons_premise = 10\\nreceived_crayons_premise = 5\\ntotal_crayons_hypothesis = 12\\n\\ndef entailment_or_contradiction(bought_crayons_premise, received_crayons_premise, total_crayons_hypothesis):\\n    # the entity in the hypothesis can be computed from the entities in the premise\\n    total_crayons_premise = bought_crayons_premise + received_crayons_premise\\n    # check if 'total_crayons_hypothesis' entails the quantity deduced from the premise, so if they are equal\\n    return total_crayons_premise == total_crayons_hypothesis:\\n\\nprint(entailment_or_contradiction(bought_crayons_premise, received_crayons_premise, total_crayons_hypothesis))\\n```\\nEND_EXAMPLE\\n### Input:\\nPremise: Jason picked 46.0 pears , Keith picked 47.0 pears , and Mike picked 12.0 pears from the pear tree .\\nHypothesis: 101.0 pears were picked in total\""
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"completion\", \"prompt\"]].head(1)[\"prompt\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# df[\"text\"] = df.apply(lambda row: f'{row[\"prompt\"]}\\n### Response:\\n```python\\n{row[\"completion\"]}```', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# df[\"text\"].to_csv(os.path.join(root_path, \"data\", \"finetuning\", \"AWPNLI\", \"text\", \"val_text.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "\"### Instruction:\\nYou need to reason about weather a hypothesis entails or contradicts a premise, by generating Python scripts. The scripts should classify the relation between the hypothesis and premise based on the quantitative and textual information mentioned in them. All the quantities and textual details in the hypothesis should be entailed by the information in the premise. First, manually extract all the individual quantities from both of the inputs, as valid numbers. Use the variable name to describe what the quantity measures, based on the context. Then, define a Python function that takes the extracted quantities as arguments. Within the function, use these quantities to perform computations based on the context of the premise and hypothesis. Finally, compare the resulting variables to determine the relationship. If the comparison indicates entailment, return True; for contradiction return False. Remember to include brief comments in the script to explain each step of the reasoning process. To illustrate, consider the following examples:\\nSTART_EXAMPLE\\nPremise: Yesterday I learned 35 verbs and 5 nouns in the morning and 10 verbs in the evening.\\nHypothesis: I learned 5 nouns and less than fifty verbs yesterday.\\nAnswer:\\n```python\\nverbs_morning_premise = 35\\nverbs_evening_premise = 10\\nnouns_premise = 5\\nmax_verbs_hypothesis = 50 \\nnouns_hypothesis = 5\\n\\ndef entailment_or_contradiction(verbs_morning_premise, verbs_evening_premise, nouns_premise, max_verbs_hypothesis, nouns_hypothesis):\\n    # the hypothesis talks about the number of learned nouns and verbs, which are also referenced in the premise\\n    # find the total number of verbs learned from the premise \\n    total_verbs_premise = verbs_morning_premise + verbs_evening_premise\\n    # check if the total verbs form the hypothesis is more than 'verbs_evening_premise' and if the number of nouns is equal between the premise and hypothesis\\n    return max_verbs_hypothesis > total_verbs_premise and nouns_premise == nouns_hypothesis\\n\\nprint(entailment_or_contradiction(verbs_morning_premise, verbs_evening_premise, nouns_premise, max_verbs_hypothesis, nouns_hypothesis))\\n```\\nEND_EXAMPLE\\n\\nSTART_EXAMPLE\\nPremise: She bought 10 crayons and received 5 more from her desk mate.\\nHypothesis: She has 10 crayons in total.\\nAnswer:\\n```python\\nbought_crayons_premise = 10\\nreceived_crayons_premise = 5\\ntotal_crayons_hypothesis = 12\\n\\ndef entailment_or_contradiction(bought_crayons_premise, received_crayons_premise, total_crayons_hypothesis):\\n    # the entity in the hypothesis can be computed from the entities in the premise\\n    total_crayons_premise = bought_crayons_premise + received_crayons_premise\\n    # check if 'total_crayons_hypothesis' entails the quantity deduced from the premise, so if they are equal\\n    return total_crayons_premise == total_crayons_hypothesis:\\n\\nprint(entailment_or_contradiction(bought_crayons_premise, received_crayons_premise, total_crayons_hypothesis))\\n```\\nEND_EXAMPLE\\n### Input:\\nPremise: Sally had 760.0 quarters in her bank  and she spent 418.0 of her quarters \\nHypothesis: She has 342.0 quarters now\\n### Response:\\n```python\\nquarters_beginning_premise = 760.0\\nquarters_spent_premise = 418.0\\nquarters_remaining_hypothesis = 342.0\\n\\ndef entailment_or_contradiction(quarters_beginning_premise, quarters_spent_premise, quarters_remaining_hypothesis):\\n    # the hypothesis talks about the remaining quarters, which can be computed from the premise\\n    quarters_remaining_premise = quarters_beginning_premise - quarters_spent_premise\\n    # check if 'quarters_remaining_hypothesis' entails the quantity deduced from the premise, so if they are equal\\n    return quarters_remaining_premise == quarters_remaining_hypothesis\\n\\nprint(entailment_or_contradiction(quarters_beginning_premise, quarters_spent_premise, quarters_remaining_hypothesis))\\n```\""
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"text\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## COMBINE ALL DATASETS INTO ONE SET FOR TRAIN/VAL/TEST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 4231\n",
      "test set size: 1738\n",
      "val set size: 748\n"
     ]
    }
   ],
   "source": [
    "for split in [\"train\", \"test\", \"val\"]:\n",
    "    all_df = pd.DataFrame()\n",
    "    for dataset in [\"StressTest\", \"AWPNLI\", \"NewsNLI\", \"RedditNLI\", \"RTE_Quant\"]:\n",
    "        samples = pd.read_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"completion\", f\"{split}.csv\"))\n",
    "        if split == \"test\":\n",
    "            samples[\"source\"] = dataset.lower().replace(\"_\", \"\")\n",
    "        all_df = pd.concat([all_df, samples], ignore_index=True)\n",
    "    print(f\"{split} set size: {all_df.shape[0]}\")\n",
    "    os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", \"completion\"), exist_ok=True)\n",
    "    all_df = all_df.sample(frac=1).reset_index(drop=True)  # shuffle data\n",
    "    all_df.to_csv(os.path.join(root_path, \"data\", \"finetuning\", \"completion\", f\"{split}_all.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
