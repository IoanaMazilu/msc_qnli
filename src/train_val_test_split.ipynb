{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ioanamazilu/PycharmProjects/quant_nli/data/lila/all/NumGLUE_Type_7_crowdsourced.json\n"
     ]
    }
   ],
   "source": [
    "root_path = os.path.dirname(os.getcwd())\n",
    "lila_path = os.path.join(root_path, \"data\", \"lila\", \"all\", \"NumGLUE_Type_7_crowdsourced.json\")\n",
    "print(lila_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SECTION 1: EXTRACT SAMPLES FROM LILA (they use Task 7 from NumGLUE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with open(lila_path, 'r') as f:\n",
    "    lila_equate = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['Source', 'Categories', 'Instances', 'Metadata'])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_equate.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "lila_equate = lila_equate[\"Instances\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "6325"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lila_equate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['Input', 'Output Program', 'Output Answer', 'split'])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_equate[0].keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "' \"statement 1\": In a deck of less than 72 cards , how many ways are there to select 13 Spade and 13 heart cards without repetition ?, \"statement 2\" :In a deck of 52 cards , how many ways are there to select 13 Spade and 13 heart cards without repetition ?, \"options: \" Entailment or contradiction or neutral?'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_equate[0][\"Input\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "\"RajeshHaveQuestionsS1 = 41 \\nRajeshHaveQuestionsS2 = 31\\nif RajeshHaveQuestionsS1 is None or RajeshHaveQuestionsS2 is None:\\n   print('neutral')\\nelif RajeshHaveQuestionsS1==RajeshHaveQuestionsS2:\\n      print('Entailment')\\nelif RajeshHaveQuestionsS1!=RajeshHaveQuestionsS2:\\n     print('contradiction')\""
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_equate[1][\"Output Program\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train, val, test = [], [], []\n",
    "for instance in lila_equate:\n",
    "    new_instance = dict()\n",
    "    inputs = instance[\"Input\"]\n",
    "    premise_hypothesis = inputs.split(\", \\\"statement 2\\\" :\")\n",
    "    try:\n",
    "        premise, hypothesis = premise_hypothesis[0], premise_hypothesis[1]\n",
    "        premise = premise.split(\"\\\"statement 1\\\": \")[-1]\n",
    "        hypothesis = hypothesis.split(\", \\\"options: \\\"\")[0]\n",
    "        new_instance.update({\"premise\": premise,\n",
    "                             \"hypothesis\": hypothesis,\n",
    "                             \"lila_label\": instance[\"Output Answer\"][0],\n",
    "                             \"lila_script\": instance[\"Output Program\"][0]})\n",
    "        split = instance[\"split\"]\n",
    "        if split == \"train\":\n",
    "            train.append(new_instance)\n",
    "        elif split == \"dev\":\n",
    "            val.append(new_instance)\n",
    "        else:\n",
    "            test.append(new_instance)\n",
    "    except IndexError:\n",
    "        print(f\"ERROR extracting inputs:\\n{inputs}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4302 806 1217\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str):\n",
    "    return re.sub(r'\\s+', ' ', text.lower().replace(\"\\n\", \"\")).strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  In a deck of less than 72 cards , how many way...   \n1  If out of 41 questions solved by Rajesh 37 que...   \n2  Alice drives at a constant speed of 30 km per ...   \n3               Mary is 22 years younger than Albert   \n4  Assuming that Karen drives at an average speed...   \n\n                                          hypothesis     lila_label  \\\n0  In a deck of 52 cards , how many ways are ther...        neutral   \n1  If out of 31 questions solved by Rajesh 37 que...  contradiction   \n2  Alice drives at a constant speed of 20 km per ...  contradiction   \n3               Mary is 72 years younger than Albert  contradiction   \n4  Assuming that Karen drives at an average speed...        neutral   \n\n                                         lila_script  \n0  DeckOfCardStatement1= None \\nDeckOfCardStateme...  \n1  RajeshHaveQuestionsS1 = 41 \\nRajeshHaveQuestio...  \n2  DrivesSpeedS1 = 30\\nDrivesSpeedS2 = 20\\nif Dri...  \n3  AgeS1 = 22\\nAgeS2 = 72\\nif AgeS1 is None or Ag...  \n4  \\nDriveS2 = 60\\nDriveS1 = None\\nif DriveS1 is ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lila_label</th>\n      <th>lila_script</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In a deck of less than 72 cards , how many way...</td>\n      <td>In a deck of 52 cards , how many ways are ther...</td>\n      <td>neutral</td>\n      <td>DeckOfCardStatement1= None \\nDeckOfCardStateme...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>If out of 41 questions solved by Rajesh 37 que...</td>\n      <td>If out of 31 questions solved by Rajesh 37 que...</td>\n      <td>contradiction</td>\n      <td>RajeshHaveQuestionsS1 = 41 \\nRajeshHaveQuestio...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alice drives at a constant speed of 30 km per ...</td>\n      <td>Alice drives at a constant speed of 20 km per ...</td>\n      <td>contradiction</td>\n      <td>DrivesSpeedS1 = 30\\nDrivesSpeedS2 = 20\\nif Dri...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mary is 22 years younger than Albert</td>\n      <td>Mary is 72 years younger than Albert</td>\n      <td>contradiction</td>\n      <td>AgeS1 = 22\\nAgeS2 = 72\\nif AgeS1 is None or Ag...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Assuming that Karen drives at an average speed...</td>\n      <td>Assuming that Karen drives at an average speed...</td>\n      <td>neutral</td>\n      <td>\\nDriveS2 = 60\\nDriveS1 = None\\nif DriveS1 is ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_train = pd.DataFrame(train)\n",
    "lila_test = pd.DataFrame(test)\n",
    "lila_val = pd.DataFrame(val)\n",
    "lila_val.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "lila_val[\"lila_label\"] = lila_val[\"lila_label\"].apply(lambda label: label.lower())\n",
    "lila_val[\"lila_label\"] = lila_val[\"lila_label\"].apply(lambda label: label.lower())\n",
    "lila_val[\"lila_label\"] = lila_val[\"lila_label\"].apply(lambda label: label.lower())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "for col in [\"premise\", \"hypothesis\"]:\n",
    "    lila_train[f\"clean_{col}\"] = lila_train[col].apply(lambda text: clean_text(text))\n",
    "    lila_test[f\"clean_{col}\"] = lila_test[col].apply(lambda text: clean_text(text))\n",
    "    lila_val[f\"clean_{col}\"] = lila_val[col].apply(lambda text: clean_text(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "lila_train = lila_train[~lila_train.duplicated(subset=['clean_premise', 'clean_hypothesis'])]\n",
    "lila_test = lila_test[~lila_test.duplicated(subset=['clean_premise', 'clean_hypothesis'])]\n",
    "lila_val = lila_val[~lila_val.duplicated(subset=['clean_premise', 'clean_hypothesis'])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4302 1217 806\n"
     ]
    }
   ],
   "source": [
    "print(lila_train.shape[0], lila_test.shape[0], lila_val.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We find no duplicates in LILA, at the train/val/test set levels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check for duplicates across the entire LILA data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "lila_all = []\n",
    "lila_all.extend(train)\n",
    "lila_all.extend(test)\n",
    "lila_all.extend(val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "6325"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_all_df = pd.DataFrame(lila_all)\n",
    "lila_all_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "for col in [\"premise\", \"hypothesis\"]:\n",
    "    lila_all_df[f\"clean_{col}\"] = lila_all_df[col].apply(lambda text: clean_text(text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lila_all_df[lila_all_df.duplicated(subset=[\"clean_premise\", \"clean_hypothesis\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also no duplicates at the whole LILA dataset level (i.e. a sample in both the train and test set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save train-val-test splits from LILA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "columns = [\"premise\", \"hypothesis\", \"lila_label\", \"lila_script\"]\n",
    "lila_train[columns].to_csv(os.path.join(root_path, \"data\", \"lila\", \"lila_train.csv\"), index=False)\n",
    "lila_test[columns].to_csv(os.path.join(root_path, \"data\", \"lila\", \"lila_test.csv\"), index=False)\n",
    "lila_val[columns].to_csv(os.path.join(root_path, \"data\", \"lila\", \"lila_val.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SECTION 2: Merge LILA samples with EQUATE samples per EQUATE dataset\n",
    "\n",
    "!!! Before this section, run the 03_cleaning.ipynb for the LILA dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read cleaned LILA datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "lila_train = pd.read_csv(os.path.join(root_path, \"data\", \"lila\", \"03_cleaned\", \"lila_train.csv\"))\n",
    "lila_test= pd.read_csv(os.path.join(root_path, \"data\", \"lila\", \"03_cleaned\", \"lila_test.csv\"))\n",
    "lila_val= pd.read_csv(os.path.join(root_path, \"data\", \"lila\", \"03_cleaned\", \"lila_val.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def add_script_to_df(df: pd.DataFrame, dataset:str):\n",
    "    sample_indices, scripts = [], []\n",
    "    scripts_path = os.path.join(root_path, \"data\", \"code_quality\", \"gpt4\", dataset, \"phase2\")\n",
    "    for scriptfile in os.listdir(scripts_path):\n",
    "        index = int(scriptfile.split(\"_\")[1].split(\".\")[0])\n",
    "        sample_indices.append(index)\n",
    "        with open(os.path.join(scripts_path, scriptfile), 'r') as f:\n",
    "            script = \"\".join(f.readlines())\n",
    "            scripts.append(script)\n",
    "    assert len(sample_indices) == len(scripts)\n",
    "    print(df.shape[0], len(sample_indices))\n",
    "    return pd.merge(df, pd.DataFrame({\"sample_index\": sample_indices, \"completion\": scripts}), on=\"sample_index\", how=\"left\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######StressTest#######\n",
      "Total instances: 6945\n",
      "6945 6947\n",
      "Test: 1217\n",
      "395\n",
      "Test: golden_label\n",
      "contradiction    466\n",
      "entailment       382\n",
      "neutral          369\n",
      "Name: count, dtype: int64\n",
      "Available for train-val split: 5728\n",
      "CC: 3836\n",
      "#######RTE_Quant#######\n",
      "Total instances: 162\n",
      "162 162\n",
      "Test: 0\n",
      "0\n",
      "Available for train-val split: 162\n",
      "CC: 137\n",
      "#######RedditNLI#######\n",
      "Total instances: 247\n",
      "247 247\n",
      "Test: 0\n",
      "0\n",
      "Available for train-val split: 247\n",
      "CC: 175\n",
      "#######NewsNLI#######\n",
      "Total instances: 958\n",
      "958 958\n",
      "Test: 0\n",
      "0\n",
      "Available for train-val split: 958\n",
      "CC: 696\n",
      "#######AWPNLI#######\n",
      "Total instances: 719\n",
      "719 722\n",
      "Test: 0\n",
      "0\n",
      "Available for train-val split: 719\n",
      "CC: 692\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"StressTest\", \"RTE_Quant\", \"RedditNLI\", \"NewsNLI\", \"AWPNLI\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"#######{dataset}#######\")\n",
    "    equate_df = pd.read_csv(os.path.join(root_path, \"data\", \"equate_labelled\", \"processed\", f\"{dataset}.csv\"))\n",
    "    print(f\"Total instances: {equate_df.shape[0]}\")\n",
    "    equate_df = add_script_to_df(equate_df, dataset)\n",
    "    equate_lila_test = pd.merge(equate_df, lila_test, on=[\"premise\", \"hypothesis\"], how=\"inner\")\n",
    "    print(f\"Test: {equate_lila_test.shape[0]}\")\n",
    "    print(equate_lila_test[equate_lila_test[\"golden_label\"] != equate_lila_test[\"reference_label\"]].shape[0])\n",
    "    if equate_lila_test.shape[0] > 0:\n",
    "        print(f\"Test: {equate_lila_test['golden_label'].value_counts()}\")\n",
    "        rest_equate = equate_df[~equate_df[\"sample_index\"].isin(equate_lila_test[\"sample_index\"].unique())]\n",
    "        ft_data = rest_equate  # [rest_equate['golden_label'] == rest_equate['reference_label']]\n",
    "    else:\n",
    "        ft_data = equate_df  # [equate_df['golden_label'] == equate_df['reference_label']]\n",
    "    print(f\"Available for train-val split: {ft_data.shape[0]}\")\n",
    "    print(f\"CC: {ft_data[ft_data['golden_label'] == ft_data['reference_label']].shape[0]}\")\n",
    "    output_path = os.path.join(root_path, \"data\", \"lila-equate\", dataset)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", dataset), exist_ok=True)\n",
    "    if dataset == \"StressTest\":\n",
    "        ft_data.to_csv(os.path.join(output_path, \"train_val.csv\"), index=False)\n",
    "    else:\n",
    "        ft_data.to_csv(os.path.join(output_path, \"all.csv\"), index=False)\n",
    "    # # we need the train samples for code quality comparison\n",
    "    # if equate_lila_train.shape[0] > 0:\n",
    "    #     equate_lila_train.to_csv(os.path.join(output_path, \"cc_train.csv\"), index=False)\n",
    "    if equate_lila_test.shape[0] > 0:\n",
    "        equate_lila_test.to_csv(os.path.join(output_path, \"test.csv\"), index=False)\n",
    "        equate_lila_test.drop([\"lila_label\", \"lila_script\"], axis=1, inplace=True)\n",
    "        equate_lila_test.to_csv(os.path.join(root_path, 'data', \"finetuning\", \"StressTest\", \"test.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SPLITS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1734\n",
      "{'StressTest': 1215, 'AWPNLI': 179, 'RTE_Quant': 40, 'RedditNLI': 61, 'NewsNLI': 239}\n",
      "{'StressTest': 0.7, 'AWPNLI': 0.1, 'RTE_Quant': 0.02, 'RedditNLI': 0.04, 'NewsNLI': 0.14}\n",
      "StressTest FT data: 3836\n",
      "Train: 3260; Val: 576\n",
      "AWPNLI FT data: 520\n",
      "Train: 442; Val: 78\n",
      "RTE_Quant FT data: 101\n",
      "Train: 85; Val: 16\n",
      "RedditNLI FT data: 132\n",
      "Train: 112; Val: 20\n",
      "NewsNLI FT data: 522\n",
      "Train: 443; Val: 79\n",
      "Total: 5111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "total_datasets = {\"StressTest\": 5728, \"AWPNLI\": 719, \"RTE_Quant\": 162, \"RedditNLI\": 247, \"NewsNLI\": 959}\n",
    "test_pctg = 0.25\n",
    "test_sizes = {\"StressTest\": 1215, \"AWPNLI\": int(test_pctg*719), \"RTE_Quant\": int(test_pctg*162), \"RedditNLI\": int(test_pctg*247), \"NewsNLI\": int(test_pctg*959)}\n",
    "total_test_set_size = np.sum([set_size for _, set_size in test_sizes.items()])\n",
    "print(total_test_set_size)\n",
    "print(test_sizes)\n",
    "test_pctgs = {ds: round(subset/total_test_set_size, 2) for ds, subset in test_sizes.items()}\n",
    "print(test_pctgs)\n",
    "total_ft = 0\n",
    "for dataset, test_size in test_sizes.items():\n",
    "    if dataset != \"StressTest\":\n",
    "        df = pd.read_csv(os.path.join(root_path, \"data\", \"lila-equate\", dataset, \"all.csv\"))\n",
    "        train_val, test = train_test_split(df, test_size=test_size, stratify=df[\"golden_label\"])\n",
    "        ft = train_val[train_val['golden_label']==train_val['reference_label']]\n",
    "        total_ft += ft.shape[0]\n",
    "        train, val = train_test_split(ft, test_size=0.15, stratify=ft[\"golden_label\"])\n",
    "        print(f\"{dataset} FT data: {ft.shape[0]}\")\n",
    "        print(f\"Train: {train.shape[0]}; Val: {val.shape[0]}\")\n",
    "        os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", dataset), exist_ok=True)\n",
    "        test.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"test.csv\"), index=False)\n",
    "        train.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"train.csv\"), index=False)\n",
    "        val.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"val.csv\"), index=False)\n",
    "    else:\n",
    "        os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", dataset), exist_ok=True)\n",
    "        df = pd.read_csv(os.path.join(root_path, \"data\", \"lila-equate\", dataset, \"train_val.csv\"))\n",
    "        ft_data = df[df['golden_label'] == df['reference_label']]\n",
    "        print(f\"{dataset} FT data: {ft_data.shape[0]}\")\n",
    "        train, val = train_test_split(ft_data, test_size=0.15, stratify=ft_data[\"golden_label\"])\n",
    "        print(f\"Train: {train.shape[0]}; Val: {val.shape[0]}\")\n",
    "        train.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"train.csv\"), index=False)\n",
    "        val.to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"val.csv\"), index=False)\n",
    "        total_ft += ft_data.shape[0]\n",
    "print(f\"Total: {total_ft}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate prompt and completion features, which will form the dataset for fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating train file.\n",
      "TEST set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating test file.\n",
      "VAL set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating val file.\n",
      "TRAIN set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating train file.\n",
      "TEST set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating test file.\n",
      "VAL set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating val file.\n",
      "TRAIN set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating train file.\n",
      "TEST set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating test file.\n",
      "VAL set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating val file.\n",
      "TRAIN set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating train file.\n",
      "TEST set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating test file.\n",
      "VAL set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating val file.\n",
      "TRAIN set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating train file.\n",
      "TEST set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating test file.\n",
      "VAL set\n",
      "Samples with no generated script: 0, ([])\n",
      "Creating val file.\n"
     ]
    }
   ],
   "source": [
    "from prompts import format_prompt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dataset = \"RTE_Quant\"   # can also use `dataset` variable set above\n",
    "for dataset in [\"StressTest\", \"NewsNLI\", \"RTE_Quant\", \"RedditNLI\", \"AWPNLI\"]:\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        print(f\"{split.upper()} set\")\n",
    "        df = pd.read_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, f\"{split}.csv\"))\n",
    "        missing_scripts = df[df[\"completion\"].isna()][\"sample_index\"]\n",
    "        print(f'Samples with no generated script: {missing_scripts.shape[0]}, ({missing_scripts.unique()})')\n",
    "        print(f\"Creating {split} file.\")\n",
    "        df.dropna(subset=[\"completion\"], inplace=True)\n",
    "        df[\"prompt\"] = df.apply(lambda row: format_prompt(dataset.lower().replace(\"_\", \"\"), {\"premise\": row[\"premise\"], \"hypothesis\": row[\"hypothesis\"]}), axis=1)\n",
    "        df[\"completion\"] = df[\"completion\"].apply(lambda completion: f\"```python\\n{completion}```\")\n",
    "        os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"completion\"), exist_ok=True)\n",
    "        if split != \"test\":\n",
    "            df[[\"completion\", \"prompt\"]].to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"completion\", f\"{split}.csv\"), index=False)\n",
    "        else:\n",
    "            df[[\"sample_index\", \"completion\", \"prompt\"]].to_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"completion\", f\"{split}.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'### Instruction:\\n\\nYou must write Python code starting from 2 input sentences, based on these rules:\\n- first you define variables with representative names for the numerical entities in both inputs (one variable per entity, per sentence);\\n- extract all quantities as valid numbers (integers or floats). do not ignore any quantity or numerical information;\\n- next, use brief comments to explain what comparison you do between the defined variables (do not use their values in the comments). Any comparison you do should be do through code as well.;\\n- refrain from concluding in the comments the entailment/contradiction or neutral relation;\\n- use the variables to perform calculations if necessary and finally compare them accordingly to infer one of the following: \\n    - \"entailment\": the hypothesis can be fully and explicitly entailed from the premise\\n    - \"contradiction\": at least one aspect in the hypothesis contradicts the premise\\n- use the correct comparison operators (i.e., if we know \"A is more than B\", then we can either check if A>B for entailment or if B<=A for contradiction).\\n- sometimes the textual information indicates neutrality and no comparison is needed.\\n\\nOnly answer with the script. To illustrate, consider the following examples:\\n\\nSTART_EXAMPLE\\nPossible labels: entailment, contradiction\\nPremise: Yesterday I learned 35 verbs and 5 nouns in the morning and 10 verbs in the evening.\\nHypothesis: I learned 5 nouns and less than fifty verbs yesterday.\\nAnswer:```python\\nverbs_morning_premise = 35\\nverbs_evening_premise = 10\\nverbs_hypothesis = 50 \\nnouns_premise = 5\\nnouns_hypothesis = 5\\n\\n# the hypothesis talks about the number of learned nouns and verbs, which are also referenced in the premise\\n# find the total number of verbs learned from the premise \\ntotal_verbs_premise = verbs_morning_premise + verbs_evening_premise\\nif max_verbs_hypothesis >= total_verbs_premise:\\n    # check if the total verbs from the hypothesis contradict the estimate of more than \\'verbs_evening_premise\\'\\n    label = \"contradiction\"\\nelif nouns_hypothesis != nouns_premise:\\n    # check if the number of nouns from the hypothesis contradicts the number of nouns in the premise\\n    label = \"contradiction\"\\nelse:\\n    # if the hypothesis values and estimates do not contradict the premise values, we can infer entailment\\n    label = \"entailment\"\\n    \\nprint(label)\\n```\\nEND_EXAMPLE\\n\\nSTART_EXAMPLE\\nPossible labels: entailment, contradiction\\nPremise: She bought 10 crayons and received 5 more from her desk mate.\\nHypothesis: She has 10 crayons in total.\\nAnswer:```python\\nbought_crayons_premise = 10\\nreceived_crayons_premise = 5\\ntotal_crayons_hypothesis = 12\\n\\n# the hypothesis refers to the number of crayons, which are also mentioned in the premise\\n# compute the total number of crayons in the premise\\ntotal_crayons_premise = bought_crayons_premise + received_crayons_premise\\nif total_crayons_hypothesis != total_crayons_premise:\\n    # check if the number of crayons in the hypothesis contradicts the number of crayons from the premise\\n    label = \"contradiction\"\\nelse:\\n    # if the hypothesis values and estimates do not contradict the premise values, we can infer entailment\\n    label = \"entailment\"    \\n\\nprint(label)\\n```\\nEND_EXAMPLE\\n\\n### Input:\\n\\nPossible labels: entailment, contradiction\\nPremise: There are 390.0 students at a school, and each classroom holds 30.0 students.\\nHypothesis: 13.0 classrooms are needed at the school.'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"completion\", \"prompt\"]].head(1)[\"prompt\"][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## COMBINE ALL DATASETS INTO ONE SET FOR TRAIN/VAL/TEST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 4342\n",
      "test set size: 1736\n",
      "val set size: 769\n"
     ]
    }
   ],
   "source": [
    "for split in [\"train\", \"test\", \"val\"]:\n",
    "    all_df = pd.DataFrame()\n",
    "    for dataset in [\"StressTest\", \"AWPNLI\", \"NewsNLI\", \"RedditNLI\", \"RTE_Quant\"]:\n",
    "        samples = pd.read_csv(os.path.join(root_path, \"data\", \"finetuning\", dataset, \"completion\", f\"{split}.csv\"))\n",
    "        if split == \"test\":\n",
    "            samples[\"source\"] = dataset.lower().replace(\"_\", \"\")\n",
    "        all_df = pd.concat([all_df, samples], ignore_index=True)\n",
    "    print(f\"{split} set size: {all_df.shape[0]}\")\n",
    "    os.makedirs(os.path.join(root_path, \"data\", \"finetuning\", \"completion\"), exist_ok=True)\n",
    "    all_df = all_df.sample(frac=1).reset_index(drop=True)  # shuffle data\n",
    "    all_df.to_csv(os.path.join(root_path, \"data\", \"finetuning\", \"completion\", f\"{split}_all.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
