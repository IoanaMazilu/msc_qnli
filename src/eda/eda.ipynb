{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Personal Information\n",
    "Name: **Ioana Mazilu**\n",
    "\n",
    "StudentID: **14642484**\n",
    "\n",
    "Email: [**ioana.mazilu@student.uva.nl**](ioana.mazilu@student.uva.nl)\n",
    "\n",
    "Submitted on: **18.03.2024**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Context\n",
    "**The aim of my research project is to test the ability of base and fine-tuned language models (small - 7B/13B and large 1.76T parameters) to generate Python scripts that can perform calculations and comparisons and return the correct classification label for a pair of premise and hypothesis. This task is known as Quantitative Natural Language Inference (QNLI), and it is derived from the NLI task, but focuses only on sentences with quantitative information.**\n",
    "\n",
    "**The dataset I will be using is called EQUATE, which is a benchmark dataset for QNLI, introduced by Ravichander et al. [1]. It consists of 5 sub-datasets, with various characteristics. Three of the datasets are of natural-language source (NewsNLI - from news articles, RedditNLI - from financial headlines on Reddit, and RTE_Quant - from a dataset for numerical reasoning). The other 2 datasets are of synthetic nature (AWPNLI - derived from math word problems, and StressTest, derived from algebra word problems). The datasets consist of a premise, hypothesis and label, which is one of entailment, contradiction or neutral. There are other features as well in the datasets, created from processing of the premise/hypothesis features. However, for my project only the former-named columns are relevant. For the EDA, we will also take a look at some of the other columns.**\n",
    "\n",
    "The code related to this project is stored at https://github.com/IoanaMazilu/msc_qnli\n",
    "\n",
    "**References**\n",
    "\n",
    "[1] References\n",
    "Ravichander, A., Naik, A., Ros√©, C. P., & Hovy, E. H. (2019). EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference. CoRR, abs/1901.03735. Retrieved from http://arxiv.org/abs/1901.03735"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data description\n",
    "\n",
    "**The EDA reveals that the 5 datasets contain different combinations of labels. For instance, StressTest and RedditNLI have samples from all 3 categories, while the other datasets have samples from the entailment category and the second label is either neutral or contradiction. The RedditNLI dataset is highly imbalanced.\n",
    "The frequency of each label in each dataset is as follows:**\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        RedditNLI: entailment (57.6%), neutral (34%), contradiction (7.6%)\n",
    "    </li>\n",
    "    <li>\n",
    "        NewsNLI: entailment (50.7%), neutral (49.3%)\n",
    "    </li>\n",
    "    <li>\n",
    "        RTE_Quant: entailment (42.2%), neutral (57.8%)\n",
    "    </li>\n",
    "    <li>\n",
    "        AWPNLI: entailment (50%), contradiction (50%)\n",
    "    </li>\n",
    "    <li>\n",
    "        StressTest: entailment (33.3%), neutral (33.3%), contradiction (33.3%)\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "**At the level of EQUATE, there is also an imbalance, as StressTest has over 7K samples, while all other datasets have less than 1K samples, and RTE_Quant even less than 200:**\n",
    "\n",
    "<ul>\n",
    "    <li>RedditNLI: 2.58%</li>\n",
    "    <li>NewsNLI: 9.98%</li>\n",
    "    <li>RTE_Quant: 1.71%</li>\n",
    "    <li>AWPNLI: 7.44%</li>\n",
    "    <li>StressTest: 78.29%</li>\n",
    "</ul>\n",
    "\n",
    "**In terms of studying the quality of the data and if there are samples which need to be discarded (since correcting them is not an option in our case), we look at duplicates and sentences that do not contain quantitative information (which is a requirement for the QNLI task). We inspect the duplicates, and find that StressTest has the most duplicates, namely 649, or 8.54% of all the samples, while all other datasets have at most 5 duplicates. We also identify samples where either the premise or hypothesis potentially do not contain quantitative information. Manual inspection of these samples (as they are few) reveals which samples can be kept and which must be discarded. One interesting insight is about the RTE_Quant dataset, for which annotator labels are provided. In more than half of instances, there is a disagreement between the annotators regarding the correct label. This could be an indicator of the higher difficulty of samples in this set, and it is worth keeping in mind during the data generation and evaluation parts.**\n",
    "\n",
    "**By looking at the length of the premise and hypothesis, we observe that some datasets have almost equal-length premises and hypotheses. We assume this is because the focus is on a direct comparison of the quantities in the inputs and/or identifying if the hypothesis is not related to the premise (which is usually the neutral class). Conversely, for the sets where the hypothesis is much shorter than the premise, we observe that the former is usually a shorter, rephrased version of the latter (2 or 3 times shorter). We also find that AWPNLI involves calculations using the quantities in the premise, obtaining a final value which must be compared to the one in the hypothesis.**\n",
    "\n",
    "**Using word-clouds to inspect the most frequent unigrams at both the premise and hypothesis level also reveals some interesting insights about the topics covered in the datasets and what types of quantities can be found inside them. For instance, AWPNLI (derived from math word problems) contains a lot of simple nouns (i.e., orange, apple, dimes, books etc.) and verbs indicating either addition or subtraction (picked, left, bought, needed). This also suggests that samples in this dataset will require calculations before a comparison can be made to infer the QNLI label.**\n",
    "\n",
    "**Finally, we analyze the amount of samples in each set that contain textual quantifiers. We create a (non-exhaustive) list of common quantifiers. We find that the synthetic datasets (math-based) and the NewsNLI set contain the most samples with quantifiers. While AWPNLI contains only 4 unique quantifiers, the other 2 datasets (StressTest and NewsNLI) contain a more diverse set of quantifiers.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# go back 2 directories from the cwd\n",
    "root_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_directory_path = os.path.join(root_path, \"data\", \"equate\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    ''' Reads a jsonl file\n",
    "    :param filename: file to be read\n",
    "    :return: list of NLI samples\n",
    "    '''\n",
    "    print(f\"###############\\nData file: {filename.split('/')[-1]}\")\n",
    "    samples = []\n",
    "    with jsonlines.open(os.path.join(data_directory_path, filename)) as reader:\n",
    "        for obj in reader:\n",
    "            samples.append(obj)\n",
    "    assert len(samples) > 0\n",
    "    labels = set([sample['gold_label'] for sample in samples]) # unique labels in the dataset\n",
    "    samples_df = pd.DataFrame(samples)\n",
    "    samples_df[\"sample_index\"] = samples_df.index\n",
    "    samples_df = samples_df.rename(columns={\"sentence1\": \"premise\", \"sentence2\": \"hypothesis\", \"gold_label\": \"label\", \"sentence1_pos\": \"premise_pos\", \"sentence2_pos\": \"hypothesis_pos\"})\n",
    "    print(f\"Dataset features: {samples_df.columns}\")\n",
    "    return samples_df, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "Data file: AWPNLI.jsonl\n",
      "Dataset features: Index(['sentence2_tokens', 'sentence1_dep_parse', 'sentence1_binary_parse',\n",
      "       'sentence2_binary_parse', 'sentence1_syntax_parse', 'premise',\n",
      "       'hypothesis', 'sentence2_syntax_parse', 'hypothesis_pos',\n",
      "       'sentence2_dep_parse', 'label', 'premise_pos', 'sentence1_tokens',\n",
      "       'sample_index'],\n",
      "      dtype='object')\n",
      "###############\n",
      "Data file: NewsNLI.jsonl\n",
      "Dataset features: Index(['sentence2_tokens', 'annotator_labels', 'sentence1_tokens',\n",
      "       'sentence1_dep_parse', 'sentence2_syntax_parse', 'Phenomena',\n",
      "       'sentence1_binary_parse', 'Hard', 'sentence2_parse',\n",
      "       'sentence2_binary_parse', 'sentence1_syntax_parse', 'premise',\n",
      "       'hypothesis', 'sentence1_parse', 'genre', 'hypothesis_pos',\n",
      "       'sentence2_dep_parse', 'label', 'premise_pos', 'PairID',\n",
      "       'sample_index'],\n",
      "      dtype='object')\n",
      "###############\n",
      "Data file: RedditNLI.jsonl\n",
      "Dataset features: Index(['sentence2_tokens', 'sentence1_tokens', 'sentence1_dep_parse',\n",
      "       'sentence2_syntax_parse', 'sentence1_binary_parse', 'sentence2_parse',\n",
      "       'sentence2_binary_parse', 'sentence1_syntax_parse', 'premise',\n",
      "       'hypothesis', 'sentence1_parse', 'genre', 'hypothesis_pos',\n",
      "       'sentence2_dep_parse', 'label', 'premise_pos', 'PairID',\n",
      "       'sample_index'],\n",
      "      dtype='object')\n",
      "###############\n",
      "Data file: RTE_Quant.jsonl\n",
      "Dataset features: Index(['sentence2_tokens', 'annotator_labels', 'sentence1_dep_parse',\n",
      "       'sentence1_binary_parse', 'sentence2_binary_parse',\n",
      "       'sentence1_syntax_parse', 'premise', 'hypothesis',\n",
      "       'sentence2_syntax_parse', 'genre', 'hypothesis_pos',\n",
      "       'sentence2_dep_parse', 'label', 'premise_pos', 'sentence1_tokens',\n",
      "       'sample_index'],\n",
      "      dtype='object')\n",
      "###############\n",
      "Data file: StressTest.jsonl\n",
      "Dataset features: Index(['sentence2_tokens', 'sentence1_dep_parse', 'sentence1_binary_parse',\n",
      "       'sentence2_binary_parse', 'sentence1_syntax_parse', 'premise',\n",
      "       'hypothesis', 'sentence2_syntax_parse', 'hypothesis_pos',\n",
      "       'sentence2_dep_parse', 'label', 'premise_pos', 'sentence1_tokens',\n",
      "       'sample_index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"AWPNLI.jsonl\", \"NewsNLI.jsonl\", \"RedditNLI.jsonl\", \"RTE_Quant.jsonl\", \"StressTest.jsonl\"]\n",
    "datasets = [os.path.join(data_directory_path, dataset) for dataset in datasets]\n",
    "\n",
    "awp, awp_labels = read_data(datasets[0])\n",
    "news, news_labels = read_data(datasets[1])\n",
    "reddit, reddit_labels = read_data(datasets[2])\n",
    "rte, rte_labels = read_data(datasets[3])\n",
    "stress, stress_labels = read_data(datasets[4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### We notice some of the datasets have unique features (i.e. RTE_Quant has a 'genre' feature, NewsNLI has a 'genre', 'Hard', 'Phenomena' and 'annotator_labels'. We will inspect these features later to find out what they represent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation of the columns, relevant for the EDA:\n",
    "<ul>\n",
    "<li>premise: the premise</li>\n",
    "<li>hypothesis: the hypothesis</li>\n",
    "<li>label: the NLI classification label (entailment/neutral/contradiction)</li>\n",
    "<li>premise_pos, hypothesis_pos: For each sentence in the premise/hypothesis, a list is extracted of the role of each word in the sentence. Example: For the sentence \"15.0 pizzas were served today\", the following list of word roles is extracted: [[\"CD\", \"NNS\", \"VBD\", \"VBN\", \"NN\"]]. We observe that \"CD\" represents quantities.</li>\n",
    "\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "                               sentence2_tokens  \\\n0                [[sam, has, 16.0, dimes, now]]   \n1                [[sam, has, 17.0, dimes, now]]   \n2         [[15.0, pizzas, were, served, today]]   \n3         [[17.0, pizzas, were, served, today]]   \n4  [[5.0, pencils, are, now, there, in, total]]   \n\n                                 sentence1_dep_parse  \\\n0  [[{'dep': 'ROOT', 'dependent': 11, 'governorGl...   \n1  [[{'dep': 'ROOT', 'dependent': 11, 'governorGl...   \n2  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...   \n3  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...   \n4  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...   \n\n                              sentence1_binary_parse  \\\n0  Sam had 9.0 dimes in his bank  and his dad gav...   \n1  Sam had 9.0 dimes in his bank  and his dad gav...   \n2  A restaurant served 9.0 pizzas during lunch an...   \n3  A restaurant served 9.0 pizzas during lunch an...   \n4  There are 2.0 pencils in the drawer  and Tim p...   \n\n                sentence2_binary_parse  \\\n0               Sam has 16.0 dimes now   \n1               Sam has 17.0 dimes now   \n2        15.0 pizzas were served today   \n3        17.0 pizzas were served today   \n4  5.0 pencils are now there in total    \n\n                              sentence1_syntax_parse  \\\n0  [(ROOT\\n  (NP\\n    (S\\n      (S\\n        (NP (...   \n1  [(ROOT\\n  (NP\\n    (S\\n      (S\\n        (NP (...   \n2  [(ROOT\\n  (S\\n    (NP (DT a) (NN restaurant))\\...   \n3  [(ROOT\\n  (S\\n    (NP (DT a) (NN restaurant))\\...   \n4  [(ROOT\\n  (S\\n    (NP (EX there))\\n    (VP (VB...   \n\n                                             premise  \\\n0  Sam had 9.0 dimes in his bank  and his dad gav...   \n1  Sam had 9.0 dimes in his bank  and his dad gav...   \n2  A restaurant served 9.0 pizzas during lunch an...   \n3  A restaurant served 9.0 pizzas during lunch an...   \n4  There are 2.0 pencils in the drawer  and Tim p...   \n\n                            hypothesis  \\\n0               Sam has 16.0 dimes now   \n1               Sam has 17.0 dimes now   \n2        15.0 pizzas were served today   \n3        17.0 pizzas were served today   \n4  5.0 pencils are now there in total    \n\n                              sentence2_syntax_parse  \\\n0  [(ROOT\\n  (S\\n    (NP (NN sam))\\n    (VP (VBZ ...   \n1  [(ROOT\\n  (S\\n    (NP (NN sam))\\n    (VP (VBZ ...   \n2  [(ROOT\\n  (S\\n    (NP (CD 15.0) (NNS pizzas))\\...   \n3  [(ROOT\\n  (S\\n    (NP (CD 17.0) (NNS pizzas))\\...   \n4  [(ROOT\\n  (S\\n    (NP (CD 5.0) (NNS pencils))\\...   \n\n                     hypothesis_pos  \\\n0          [[NN, VBZ, CD, NNS, RB]]   \n1          [[NN, VBZ, CD, NNS, RB]]   \n2         [[CD, NNS, VBD, VBN, NN]]   \n3         [[CD, NNS, VBD, VBN, NN]]   \n4  [[CD, NNS, VBP, RB, RB, IN, NN]]   \n\n                                 sentence2_dep_parse          label  \\\n0  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...     entailment   \n1  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...  contradiction   \n2  [[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...     entailment   \n3  [[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...  contradiction   \n4  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...     entailment   \n\n                                         premise_pos  \\\n0  [[NN, VBD, CD, NNS, IN, PRP$, NN, CC, PRP$, NN...   \n1  [[NN, VBD, CD, NNS, IN, PRP$, NN, CC, PRP$, NN...   \n2  [[DT, NN, VBD, CD, NNS, IN, NN, CC, CD, IN, NN...   \n3  [[DT, NN, VBD, CD, NNS, IN, NN, CC, CD, IN, NN...   \n4  [[EX, VBP, CD, NNS, IN, DT, NN, CC, NN, VBD, C...   \n\n                                    sentence1_tokens  sample_index  \n0  [[sam, had, 9.0, dimes, in, his, bank, and, hi...             0  \n1  [[sam, had, 9.0, dimes, in, his, bank, and, hi...             1  \n2  [[a, restaurant, served, 9.0, pizzas, during, ...             2  \n3  [[a, restaurant, served, 9.0, pizzas, during, ...             3  \n4  [[there, are, 2.0, pencils, in, the, drawer, a...             4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence2_tokens</th>\n      <th>sentence1_dep_parse</th>\n      <th>sentence1_binary_parse</th>\n      <th>sentence2_binary_parse</th>\n      <th>sentence1_syntax_parse</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sentence2_syntax_parse</th>\n      <th>hypothesis_pos</th>\n      <th>sentence2_dep_parse</th>\n      <th>label</th>\n      <th>premise_pos</th>\n      <th>sentence1_tokens</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[sam, has, 16.0, dimes, now]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 11, 'governorGl...</td>\n      <td>Sam had 9.0 dimes in his bank  and his dad gav...</td>\n      <td>Sam has 16.0 dimes now</td>\n      <td>[(ROOT\\n  (NP\\n    (S\\n      (S\\n        (NP (...</td>\n      <td>Sam had 9.0 dimes in his bank  and his dad gav...</td>\n      <td>Sam has 16.0 dimes now</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN sam))\\n    (VP (VBZ ...</td>\n      <td>[[NN, VBZ, CD, NNS, RB]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[NN, VBD, CD, NNS, IN, PRP$, NN, CC, PRP$, NN...</td>\n      <td>[[sam, had, 9.0, dimes, in, his, bank, and, hi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[sam, has, 17.0, dimes, now]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 11, 'governorGl...</td>\n      <td>Sam had 9.0 dimes in his bank  and his dad gav...</td>\n      <td>Sam has 17.0 dimes now</td>\n      <td>[(ROOT\\n  (NP\\n    (S\\n      (S\\n        (NP (...</td>\n      <td>Sam had 9.0 dimes in his bank  and his dad gav...</td>\n      <td>Sam has 17.0 dimes now</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN sam))\\n    (VP (VBZ ...</td>\n      <td>[[NN, VBZ, CD, NNS, RB]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>contradiction</td>\n      <td>[[NN, VBD, CD, NNS, IN, PRP$, NN, CC, PRP$, NN...</td>\n      <td>[[sam, had, 9.0, dimes, in, his, bank, and, hi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[15.0, pizzas, were, served, today]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>15.0 pizzas were served today</td>\n      <td>[(ROOT\\n  (S\\n    (NP (DT a) (NN restaurant))\\...</td>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>15.0 pizzas were served today</td>\n      <td>[(ROOT\\n  (S\\n    (NP (CD 15.0) (NNS pizzas))\\...</td>\n      <td>[[CD, NNS, VBD, VBN, NN]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[DT, NN, VBD, CD, NNS, IN, NN, CC, CD, IN, NN...</td>\n      <td>[[a, restaurant, served, 9.0, pizzas, during, ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[17.0, pizzas, were, served, today]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>17.0 pizzas were served today</td>\n      <td>[(ROOT\\n  (S\\n    (NP (DT a) (NN restaurant))\\...</td>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>17.0 pizzas were served today</td>\n      <td>[(ROOT\\n  (S\\n    (NP (CD 17.0) (NNS pizzas))\\...</td>\n      <td>[[CD, NNS, VBD, VBN, NN]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...</td>\n      <td>contradiction</td>\n      <td>[[DT, NN, VBD, CD, NNS, IN, NN, CC, CD, IN, NN...</td>\n      <td>[[a, restaurant, served, 9.0, pizzas, during, ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[5.0, pencils, are, now, there, in, total]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>There are 2.0 pencils in the drawer  and Tim p...</td>\n      <td>5.0 pencils are now there in total</td>\n      <td>[(ROOT\\n  (S\\n    (NP (EX there))\\n    (VP (VB...</td>\n      <td>There are 2.0 pencils in the drawer  and Tim p...</td>\n      <td>5.0 pencils are now there in total</td>\n      <td>[(ROOT\\n  (S\\n    (NP (CD 5.0) (NNS pencils))\\...</td>\n      <td>[[CD, NNS, VBP, RB, RB, IN, NN]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[EX, VBP, CD, NNS, IN, DT, NN, CC, NN, VBD, C...</td>\n      <td>[[there, are, 2.0, pencils, in, the, drawer, a...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awp.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of sample counts per sub-dataset in EQUATE and per language type (natural/synthetic)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### How many samples are in each dataset?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722 968 250 166 7596\n"
     ]
    }
   ],
   "source": [
    "print(len(awp), len(news), len(reddit), len(rte), len(stress))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### What fraction of the EQUATE benchmark each sub-dataset is?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.44 9.98 2.58 1.71 78.29\n"
     ]
    }
   ],
   "source": [
    "equate_size = len(awp) + len(news) + len(reddit) + len(rte) + len(stress)\n",
    "print(round(len(awp)*100/equate_size, 2), round(len(news)*100/equate_size, 2), round(len(reddit)*100/equate_size, 2), round(len(rte)*100/equate_size, 2), round(len(stress)*100/equate_size, 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "            Label  Label proportion     Dataset\n8   contradiction          0.500000      AWPNLI\n7      entailment          0.500000      AWPNLI\n11  contradiction          0.333333  StressTest\n9      entailment          0.333333  StressTest\n10        neutral          0.333333  StressTest\n4   contradiction          0.076000   RedditNLI\n2      entailment          0.584000   RedditNLI\n3         neutral          0.340000   RedditNLI\n1      entailment          0.421687   RTE_Quant\n0         neutral          0.578313   RTE_Quant\n5      entailment          0.507231     NewsNLI\n6         neutral          0.492769     NewsNLI",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>Label proportion</th>\n      <th>Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>contradiction</td>\n      <td>0.500000</td>\n      <td>AWPNLI</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>entailment</td>\n      <td>0.500000</td>\n      <td>AWPNLI</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>contradiction</td>\n      <td>0.333333</td>\n      <td>StressTest</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>entailment</td>\n      <td>0.333333</td>\n      <td>StressTest</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>neutral</td>\n      <td>0.333333</td>\n      <td>StressTest</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>contradiction</td>\n      <td>0.076000</td>\n      <td>RedditNLI</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>entailment</td>\n      <td>0.584000</td>\n      <td>RedditNLI</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neutral</td>\n      <td>0.340000</td>\n      <td>RedditNLI</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>entailment</td>\n      <td>0.421687</td>\n      <td>RTE_Quant</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>0.578313</td>\n      <td>RTE_Quant</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>entailment</td>\n      <td>0.507231</td>\n      <td>NewsNLI</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>neutral</td>\n      <td>0.492769</td>\n      <td>NewsNLI</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equate_label_distribution = pd.DataFrame()\n",
    "for dataset, df in zip([\"RTE_Quant\", \"RedditNLI\", \"NewsNLI\", \"AWPNLI\", \"StressTest\"], [rte, reddit, news, awp, stress]):\n",
    "    label_distribution = df[\"label\"].value_counts(normalize=True)\n",
    "    label_distribution = label_distribution.reset_index()\n",
    "    label_distribution[\"Dataset\"] = dataset\n",
    "    equate_label_distribution = pd.concat([equate_label_distribution, label_distribution], ignore_index=True)\n",
    "\n",
    "equate_label_distribution = equate_label_distribution.sort_values(by=\"label\").rename(columns={\"proportion\": \"Label proportion\", \"label\": \"Label\"})\n",
    "\n",
    "# sort df by dataset name, first synthetic sets, then real ones\n",
    "custom_order = ['AWPNLI', 'StressTest', 'RedditNLI', 'RTE_Quant', 'NewsNLI']\n",
    "\n",
    "# Create a temporary sorting key based on your custom order\n",
    "equate_label_distribution['sorting_key'] = equate_label_distribution['Dataset'].map(lambda x: custom_order.index(x))\n",
    "\n",
    "# Sort the DataFrame by the temporary sorting key\n",
    "equate_label_distribution = equate_label_distribution.sort_values('sorting_key').drop('sorting_key', axis=1)\n",
    "equate_label_distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Label=contradiction<br>Dataset=%{x}<br>Label proportion=%{text}<extra></extra>",
         "legendgroup": "contradiction",
         "marker": {
          "color": "lightgray",
          "pattern": {
           "shape": ""
          }
         },
         "name": "contradiction",
         "offsetgroup": "contradiction",
         "orientation": "v",
         "showlegend": true,
         "text": [
          0.5,
          0.3333333333333333,
          0.076
         ],
         "textposition": "inside",
         "x": [
          "AWPNLI",
          "StressTest",
          "RedditNLI"
         ],
         "xaxis": "x",
         "y": [
          0.5,
          0.3333333333333333,
          0.076
         ],
         "yaxis": "y",
         "type": "bar",
         "texttemplate": "%{text:.2%}"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Label=entailment<br>Dataset=%{x}<br>Label proportion=%{text}<extra></extra>",
         "legendgroup": "entailment",
         "marker": {
          "color": "darkgray",
          "pattern": {
           "shape": ""
          }
         },
         "name": "entailment",
         "offsetgroup": "entailment",
         "orientation": "v",
         "showlegend": true,
         "text": [
          0.5,
          0.3333333333333333,
          0.584,
          0.42168674698795183,
          0.5072314049586777
         ],
         "textposition": "inside",
         "x": [
          "AWPNLI",
          "StressTest",
          "RedditNLI",
          "RTE_Quant",
          "NewsNLI"
         ],
         "xaxis": "x",
         "y": [
          0.5,
          0.3333333333333333,
          0.584,
          0.42168674698795183,
          0.5072314049586777
         ],
         "yaxis": "y",
         "type": "bar",
         "texttemplate": "%{text:.2%}"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Label=neutral<br>Dataset=%{x}<br>Label proportion=%{text}<extra></extra>",
         "legendgroup": "neutral",
         "marker": {
          "color": "black",
          "pattern": {
           "shape": ""
          }
         },
         "name": "neutral",
         "offsetgroup": "neutral",
         "orientation": "v",
         "showlegend": true,
         "text": [
          0.3333333333333333,
          0.34,
          0.5783132530120482,
          0.4927685950413223
         ],
         "textposition": "inside",
         "x": [
          "StressTest",
          "RedditNLI",
          "RTE_Quant",
          "NewsNLI"
         ],
         "xaxis": "x",
         "y": [
          0.3333333333333333,
          0.34,
          0.5783132530120482,
          0.4927685950413223
         ],
         "yaxis": "y",
         "type": "bar",
         "texttemplate": "%{text:.2%}"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmapgl": [
           {
            "type": "heatmapgl",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "bgcolor": "rgb(17,17,17)",
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "rgb(17,17,17)",
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "subunitcolor": "#506784",
           "showland": true,
           "showlakes": true,
           "lakecolor": "rgb(17,17,17)"
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "borderwidth": 1,
           "bordercolor": "rgb(17,17,17)",
           "tickwidth": 0
          },
          "mapbox": {
           "style": "dark"
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0.0,
          1.0
         ],
         "title": {
          "text": "Dataset",
          "font": {
           "color": "black"
          }
         },
         "tickfont": {
          "color": "black"
         },
         "showline": false,
         "gridcolor": "rgba(0,0,0,0)",
         "tickvals": [
          "AWPNLI",
          "StressTest",
          "RedditNLI",
          "RTE_Quant",
          "NewsNLI"
         ],
         "ticktext": [
          "<span style=\"color:red\">AWPNLI</span>",
          "<span style=\"color:red\">StressTest</span>",
          "<span style=\"color:blue\">RedditNLI</span>",
          "<span style=\"color:blue\">RTE_Quant</span>",
          "<span style=\"color:blue\">NewsNLI</span>"
         ],
         "tickmode": "array"
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.0,
          1.0
         ],
         "title": {
          "text": "Label proportion",
          "font": {
           "color": "black"
          }
         },
         "tickfont": {
          "color": "white"
         },
         "showline": false,
         "gridcolor": "rgba(0,0,0,0)"
        },
        "legend": {
         "title": {
          "text": "Label"
         },
         "tracegroupgap": 0
        },
        "title": {
         "text": "Label distribution per dataset",
         "font": {
          "color": "black"
         }
        },
        "barmode": "relative",
        "font": {
         "color": "black"
        },
        "plot_bgcolor": "rgba(255, 255, 255, 1)",
        "paper_bgcolor": "rgba(255, 255, 255, 1)",
        "annotations": [
         {
          "showarrow": false,
          "text": "Samples: 166",
          "x": "RTE_Quant",
          "y": 1,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "Samples: 722",
          "x": "AWPNLI",
          "y": 1,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "Samples: 250",
          "x": "RedditNLI",
          "y": 1,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "Samples: 968",
          "x": "NewsNLI",
          "y": 1,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "Samples: 7596",
          "x": "StressTest",
          "y": 1,
          "yshift": 10
         }
        ]
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      },
      "text/html": "<div>                            <div id=\"c573424b-e58b-441f-a0cf-2395ebe3f1f2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c573424b-e58b-441f-a0cf-2395ebe3f1f2\")) {                    Plotly.newPlot(                        \"c573424b-e58b-441f-a0cf-2395ebe3f1f2\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Label=contradiction\\u003cbr\\u003eDataset=%{x}\\u003cbr\\u003eLabel proportion=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"contradiction\",\"marker\":{\"color\":\"lightgray\",\"pattern\":{\"shape\":\"\"}},\"name\":\"contradiction\",\"offsetgroup\":\"contradiction\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[0.5,0.3333333333333333,0.076],\"textposition\":\"inside\",\"x\":[\"AWPNLI\",\"StressTest\",\"RedditNLI\"],\"xaxis\":\"x\",\"y\":[0.5,0.3333333333333333,0.076],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{text:.2%}\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Label=entailment\\u003cbr\\u003eDataset=%{x}\\u003cbr\\u003eLabel proportion=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"entailment\",\"marker\":{\"color\":\"darkgray\",\"pattern\":{\"shape\":\"\"}},\"name\":\"entailment\",\"offsetgroup\":\"entailment\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[0.5,0.3333333333333333,0.584,0.42168674698795183,0.5072314049586777],\"textposition\":\"inside\",\"x\":[\"AWPNLI\",\"StressTest\",\"RedditNLI\",\"RTE_Quant\",\"NewsNLI\"],\"xaxis\":\"x\",\"y\":[0.5,0.3333333333333333,0.584,0.42168674698795183,0.5072314049586777],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{text:.2%}\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Label=neutral\\u003cbr\\u003eDataset=%{x}\\u003cbr\\u003eLabel proportion=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"neutral\",\"marker\":{\"color\":\"black\",\"pattern\":{\"shape\":\"\"}},\"name\":\"neutral\",\"offsetgroup\":\"neutral\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[0.3333333333333333,0.34,0.5783132530120482,0.4927685950413223],\"textposition\":\"inside\",\"x\":[\"StressTest\",\"RedditNLI\",\"RTE_Quant\",\"NewsNLI\"],\"xaxis\":\"x\",\"y\":[0.3333333333333333,0.34,0.5783132530120482,0.4927685950413223],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{text:.2%}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"bgcolor\":\"rgb(17,17,17)\",\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"rgb(17,17,17)\",\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"subunitcolor\":\"#506784\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"rgb(17,17,17)\"},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"borderwidth\":1,\"bordercolor\":\"rgb(17,17,17)\",\"tickwidth\":0},\"mapbox\":{\"style\":\"dark\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Dataset\",\"font\":{\"color\":\"black\"}},\"tickfont\":{\"color\":\"black\"},\"showline\":false,\"gridcolor\":\"rgba(0,0,0,0)\",\"tickvals\":[\"AWPNLI\",\"StressTest\",\"RedditNLI\",\"RTE_Quant\",\"NewsNLI\"],\"ticktext\":[\"\\u003cspan style=\\\"color:red\\\"\\u003eAWPNLI\\u003c\\u002fspan\\u003e\",\"\\u003cspan style=\\\"color:red\\\"\\u003eStressTest\\u003c\\u002fspan\\u003e\",\"\\u003cspan style=\\\"color:blue\\\"\\u003eRedditNLI\\u003c\\u002fspan\\u003e\",\"\\u003cspan style=\\\"color:blue\\\"\\u003eRTE_Quant\\u003c\\u002fspan\\u003e\",\"\\u003cspan style=\\\"color:blue\\\"\\u003eNewsNLI\\u003c\\u002fspan\\u003e\"],\"tickmode\":\"array\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Label proportion\",\"font\":{\"color\":\"black\"}},\"tickfont\":{\"color\":\"white\"},\"showline\":false,\"gridcolor\":\"rgba(0,0,0,0)\"},\"legend\":{\"title\":{\"text\":\"Label\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Label distribution per dataset\",\"font\":{\"color\":\"black\"}},\"barmode\":\"relative\",\"font\":{\"color\":\"black\"},\"plot_bgcolor\":\"rgba(255, 255, 255, 1)\",\"paper_bgcolor\":\"rgba(255, 255, 255, 1)\",\"annotations\":[{\"showarrow\":false,\"text\":\"Samples: 166\",\"x\":\"RTE_Quant\",\"y\":1,\"yshift\":10},{\"showarrow\":false,\"text\":\"Samples: 722\",\"x\":\"AWPNLI\",\"y\":1,\"yshift\":10},{\"showarrow\":false,\"text\":\"Samples: 250\",\"x\":\"RedditNLI\",\"y\":1,\"yshift\":10},{\"showarrow\":false,\"text\":\"Samples: 968\",\"x\":\"NewsNLI\",\"y\":1,\"yshift\":10},{\"showarrow\":false,\"text\":\"Samples: 7596\",\"x\":\"StressTest\",\"y\":1,\"yshift\":10}]},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('c573424b-e58b-441f-a0cf-2395ebe3f1f2');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Custom color mapping for labels in shades of gray\n",
    "color_discrete_map = {\n",
    "    \"neutral\": \"black\",\n",
    "    \"entailment\": \"darkgray\",\n",
    "    \"contradiction\": \"lightgray\"\n",
    "}\n",
    "\n",
    "# Creating the bar chart\n",
    "fig = px.bar(equate_label_distribution, x=\"Dataset\", y=\"Label proportion\", color=\"Label\",\n",
    "             title=\"Label distribution per dataset\", color_discrete_map=color_discrete_map, text=\"Label proportion\")\n",
    "# Format text on bars to show as percentages\n",
    "fig.update_traces(texttemplate='%{text:.2%}', textposition='inside')\n",
    "# Remove the background behind the bars\n",
    "fig.update_layout({\n",
    "    'plot_bgcolor': 'rgba(255, 255, 255, 1)',\n",
    "    'paper_bgcolor': 'rgba(255, 255, 255, 1)',\n",
    "    'title_font_color': 'black',  # Set title text color to black\n",
    "    'font_color': 'black',  # Set global font color to black (affects ticks and legend)\n",
    "    # 'barmode': 'group'\n",
    "})\n",
    "\n",
    "# Remove grid lines and axis lines\n",
    "fig.update_xaxes(showline=False, gridcolor='rgba(0,0,0,0)', tickfont=dict(color='black'), title_font=dict(color='black'))\n",
    "fig.update_yaxes(showline=False, gridcolor='rgba(0,0,0,0)', tickfont=dict(color='white'), title_font=dict(color='black'))\n",
    "\n",
    "for dataset, sample_count in zip([\"RTE_Quant\", \"AWPNLI\", \"RedditNLI\", \"NewsNLI\", \"StressTest\"], [166, 722, 250, 968, 7596]):\n",
    "    fig.add_annotation(\n",
    "            x=dataset,\n",
    "            y=1,  # Position at the top of the bar; may need adjustment.\n",
    "            text=f\"Samples: {sample_count}\",\n",
    "            showarrow=False,\n",
    "            yshift=10  # Adjust this value to move the annotation up or down\n",
    "        )\n",
    "\n",
    "# Update dataset names/tick colors\n",
    "colors = ['red'] * 2 + ['blue'] * 3  # The first two red, the rest blue.\n",
    "fig.update_xaxes(\n",
    "    tickvals=equate_label_distribution['Dataset'].unique(),\n",
    "    ticktext=[f'<span style=\"color:{color}\">{text}</span>' for color, text in zip(colors, equate_label_distribution['Dataset'].unique())],\n",
    "    tickmode='array'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# os.makedirs(\"images\", exist_ok=True)\n",
    "fig.write_image(\"images/equate_label_distribution.png\")\n",
    "fig.write_image(\"images/equate_label_distribution.pdf\")\n",
    "fig.write_image(\"images/equate_label_distribution.svg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The datasets are of 2 types:\n",
    "- based on natural, every-day language, scraped from sources like Reddit (RedditNLI), news articles (NewsNLI) and a dataset of quantitative problems (RTE_Quant);\n",
    "- based on synthetic language, created from Math World Problems (MWPs) (StressTest, AWPNLI).\n",
    "Let's inspect how many samples of each category we have"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural-language samples: 1384 (14.27% of all samples in EQUATE)\n",
      "Synthetic-language samples: 8318 (85.73% of all samples in EQUATE)\n"
     ]
    }
   ],
   "source": [
    "natural_language_samples = news.shape[0] + reddit.shape[0] + rte.shape[0]\n",
    "synthetic_language_samples = stress.shape[0] + awp.shape[0]\n",
    "total_samples = natural_language_samples + synthetic_language_samples\n",
    "print(f\"Natural-language samples: {natural_language_samples} ({round((natural_language_samples/total_samples)*100, 2)}% of all samples in EQUATE)\")\n",
    "print(f\"Synthetic-language samples: {synthetic_language_samples} ({round((synthetic_language_samples/total_samples)*100, 2)}% of all samples in EQUATE)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### We notice a significant imbalance at the EQUATE dataset level between natural language samples and synthetic language samples, with a large ratio of the total samples being of synthetic nature, and more specifically from the StressTest dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of the sentences which form the premises and hypotheses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UTIL FUNCTIONS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def clean_text(df: pd.DataFrame, column_name: str):\n",
    "    df[column_name] = df[column_name].apply(lambda str_value: re.sub(r'\\s+', ' ', str_value.replace(\"\\n\", \"\")).strip())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def count_words_in_string(df: pd.DataFrame, column_name: str):\n",
    "    df[f\"{column_name}_word_cnt\"] = df[column_name].apply(lambda str_value: len(str_value.split(\" \")))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def count_chars_in_string(df: pd.DataFrame, column_name: str):\n",
    "    df[f\"{column_name}_char_cnt\"] = df[column_name].apply(lambda str_value: len(str_value))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "def quantities_in_sentence(df: pd.DataFrame, column_name: str):\n",
    "    df[f\"{column_name}_quantities_cnt\"] = df[f\"{column_name}_pos\"].apply(lambda entities: np.sum([1 for i in range(len(entities)) for entity in entities[i] if entity == \"CD\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def unique_annotator_labels(df: pd.DataFrame):\n",
    "    df[\"annotator_unique_labels\"] = df[\"annotator_labels\"].apply(lambda labels_array: len(set(labels_array)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def sentence_insights(dataset_df):\n",
    "    clean_text(dataset_df, \"premise\")\n",
    "    clean_text(dataset_df, \"hypothesis\")\n",
    "    count_words_in_string(dataset_df, \"premise\")\n",
    "    count_words_in_string(dataset_df, \"hypothesis\")\n",
    "    count_chars_in_string(dataset_df, \"premise\")\n",
    "    count_chars_in_string(dataset_df, \"hypothesis\")\n",
    "    quantities_in_sentence(dataset_df, \"premise\")\n",
    "    quantities_in_sentence(dataset_df, \"hypothesis\")\n",
    "    if \"annotator_labels\" in dataset_df.columns:\n",
    "        unique_annotator_labels(dataset_df)\n",
    "    else:\n",
    "        print(\"There is no data on the annotator labels for this dataset.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### AWPNLI dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "                               sentence2_tokens  \\\n0                [[sam, has, 16.0, dimes, now]]   \n1                [[sam, has, 17.0, dimes, now]]   \n2         [[15.0, pizzas, were, served, today]]   \n3         [[17.0, pizzas, were, served, today]]   \n4  [[5.0, pencils, are, now, there, in, total]]   \n\n                                 sentence1_dep_parse  \\\n0  [[{'dep': 'ROOT', 'dependent': 11, 'governorGl...   \n1  [[{'dep': 'ROOT', 'dependent': 11, 'governorGl...   \n2  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...   \n3  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...   \n4  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...   \n\n                              sentence1_binary_parse  \\\n0  Sam had 9.0 dimes in his bank  and his dad gav...   \n1  Sam had 9.0 dimes in his bank  and his dad gav...   \n2  A restaurant served 9.0 pizzas during lunch an...   \n3  A restaurant served 9.0 pizzas during lunch an...   \n4  There are 2.0 pencils in the drawer  and Tim p...   \n\n                sentence2_binary_parse  \\\n0               Sam has 16.0 dimes now   \n1               Sam has 17.0 dimes now   \n2        15.0 pizzas were served today   \n3        17.0 pizzas were served today   \n4  5.0 pencils are now there in total    \n\n                              sentence1_syntax_parse  \\\n0  [(ROOT\\n  (NP\\n    (S\\n      (S\\n        (NP (...   \n1  [(ROOT\\n  (NP\\n    (S\\n      (S\\n        (NP (...   \n2  [(ROOT\\n  (S\\n    (NP (DT a) (NN restaurant))\\...   \n3  [(ROOT\\n  (S\\n    (NP (DT a) (NN restaurant))\\...   \n4  [(ROOT\\n  (S\\n    (NP (EX there))\\n    (VP (VB...   \n\n                                             premise  \\\n0  Sam had 9.0 dimes in his bank  and his dad gav...   \n1  Sam had 9.0 dimes in his bank  and his dad gav...   \n2  A restaurant served 9.0 pizzas during lunch an...   \n3  A restaurant served 9.0 pizzas during lunch an...   \n4  There are 2.0 pencils in the drawer  and Tim p...   \n\n                            hypothesis  \\\n0               Sam has 16.0 dimes now   \n1               Sam has 17.0 dimes now   \n2        15.0 pizzas were served today   \n3        17.0 pizzas were served today   \n4  5.0 pencils are now there in total    \n\n                              sentence2_syntax_parse  \\\n0  [(ROOT\\n  (S\\n    (NP (NN sam))\\n    (VP (VBZ ...   \n1  [(ROOT\\n  (S\\n    (NP (NN sam))\\n    (VP (VBZ ...   \n2  [(ROOT\\n  (S\\n    (NP (CD 15.0) (NNS pizzas))\\...   \n3  [(ROOT\\n  (S\\n    (NP (CD 17.0) (NNS pizzas))\\...   \n4  [(ROOT\\n  (S\\n    (NP (CD 5.0) (NNS pencils))\\...   \n\n                     hypothesis_pos  \\\n0          [[NN, VBZ, CD, NNS, RB]]   \n1          [[NN, VBZ, CD, NNS, RB]]   \n2         [[CD, NNS, VBD, VBN, NN]]   \n3         [[CD, NNS, VBD, VBN, NN]]   \n4  [[CD, NNS, VBP, RB, RB, IN, NN]]   \n\n                                 sentence2_dep_parse          label  \\\n0  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...     entailment   \n1  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...  contradiction   \n2  [[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...     entailment   \n3  [[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...  contradiction   \n4  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...     entailment   \n\n                                         premise_pos  \\\n0  [[NN, VBD, CD, NNS, IN, PRP$, NN, CC, PRP$, NN...   \n1  [[NN, VBD, CD, NNS, IN, PRP$, NN, CC, PRP$, NN...   \n2  [[DT, NN, VBD, CD, NNS, IN, NN, CC, CD, IN, NN...   \n3  [[DT, NN, VBD, CD, NNS, IN, NN, CC, CD, IN, NN...   \n4  [[EX, VBP, CD, NNS, IN, DT, NN, CC, NN, VBD, C...   \n\n                                    sentence1_tokens  sample_index  \n0  [[sam, had, 9.0, dimes, in, his, bank, and, hi...             0  \n1  [[sam, had, 9.0, dimes, in, his, bank, and, hi...             1  \n2  [[a, restaurant, served, 9.0, pizzas, during, ...             2  \n3  [[a, restaurant, served, 9.0, pizzas, during, ...             3  \n4  [[there, are, 2.0, pencils, in, the, drawer, a...             4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence2_tokens</th>\n      <th>sentence1_dep_parse</th>\n      <th>sentence1_binary_parse</th>\n      <th>sentence2_binary_parse</th>\n      <th>sentence1_syntax_parse</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sentence2_syntax_parse</th>\n      <th>hypothesis_pos</th>\n      <th>sentence2_dep_parse</th>\n      <th>label</th>\n      <th>premise_pos</th>\n      <th>sentence1_tokens</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[sam, has, 16.0, dimes, now]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 11, 'governorGl...</td>\n      <td>Sam had 9.0 dimes in his bank  and his dad gav...</td>\n      <td>Sam has 16.0 dimes now</td>\n      <td>[(ROOT\\n  (NP\\n    (S\\n      (S\\n        (NP (...</td>\n      <td>Sam had 9.0 dimes in his bank  and his dad gav...</td>\n      <td>Sam has 16.0 dimes now</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN sam))\\n    (VP (VBZ ...</td>\n      <td>[[NN, VBZ, CD, NNS, RB]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[NN, VBD, CD, NNS, IN, PRP$, NN, CC, PRP$, NN...</td>\n      <td>[[sam, had, 9.0, dimes, in, his, bank, and, hi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[sam, has, 17.0, dimes, now]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 11, 'governorGl...</td>\n      <td>Sam had 9.0 dimes in his bank  and his dad gav...</td>\n      <td>Sam has 17.0 dimes now</td>\n      <td>[(ROOT\\n  (NP\\n    (S\\n      (S\\n        (NP (...</td>\n      <td>Sam had 9.0 dimes in his bank  and his dad gav...</td>\n      <td>Sam has 17.0 dimes now</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN sam))\\n    (VP (VBZ ...</td>\n      <td>[[NN, VBZ, CD, NNS, RB]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>contradiction</td>\n      <td>[[NN, VBD, CD, NNS, IN, PRP$, NN, CC, PRP$, NN...</td>\n      <td>[[sam, had, 9.0, dimes, in, his, bank, and, hi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[15.0, pizzas, were, served, today]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>15.0 pizzas were served today</td>\n      <td>[(ROOT\\n  (S\\n    (NP (DT a) (NN restaurant))\\...</td>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>15.0 pizzas were served today</td>\n      <td>[(ROOT\\n  (S\\n    (NP (CD 15.0) (NNS pizzas))\\...</td>\n      <td>[[CD, NNS, VBD, VBN, NN]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[DT, NN, VBD, CD, NNS, IN, NN, CC, CD, IN, NN...</td>\n      <td>[[a, restaurant, served, 9.0, pizzas, during, ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[17.0, pizzas, were, served, today]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>17.0 pizzas were served today</td>\n      <td>[(ROOT\\n  (S\\n    (NP (DT a) (NN restaurant))\\...</td>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>17.0 pizzas were served today</td>\n      <td>[(ROOT\\n  (S\\n    (NP (CD 17.0) (NNS pizzas))\\...</td>\n      <td>[[CD, NNS, VBD, VBN, NN]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...</td>\n      <td>contradiction</td>\n      <td>[[DT, NN, VBD, CD, NNS, IN, NN, CC, CD, IN, NN...</td>\n      <td>[[a, restaurant, served, 9.0, pizzas, during, ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[5.0, pencils, are, now, there, in, total]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>There are 2.0 pencils in the drawer  and Tim p...</td>\n      <td>5.0 pencils are now there in total</td>\n      <td>[(ROOT\\n  (S\\n    (NP (EX there))\\n    (VP (VB...</td>\n      <td>There are 2.0 pencils in the drawer  and Tim p...</td>\n      <td>5.0 pencils are now there in total</td>\n      <td>[(ROOT\\n  (S\\n    (NP (CD 5.0) (NNS pencils))\\...</td>\n      <td>[[CD, NNS, VBP, RB, RB, IN, NN]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[EX, VBP, CD, NNS, IN, DT, NN, CC, NN, VBD, C...</td>\n      <td>[[there, are, 2.0, pencils, in, the, drawer, a...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awp.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "label\nentailment       0.5\ncontradiction    0.5\nName: proportion, dtype: float64"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awp['label'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of samples across the 2 labels is balanced"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no data on the annotator labels for this dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  Sam had 9.0 dimes in his bank and his dad gave...   \n1  Sam had 9.0 dimes in his bank and his dad gave...   \n2  A restaurant served 9.0 pizzas during lunch an...   \n3  A restaurant served 9.0 pizzas during lunch an...   \n4  There are 2.0 pencils in the drawer and Tim pl...   \n\n                           hypothesis          label  premise_word_cnt  \\\n0              Sam has 16.0 dimes now     entailment                14   \n1              Sam has 17.0 dimes now  contradiction                14   \n2       15.0 pizzas were served today     entailment                13   \n3       17.0 pizzas were served today  contradiction                13   \n4  5.0 pencils are now there in total     entailment                15   \n\n   hypothesis_word_cnt  premise_char_cnt  hypothesis_char_cnt  \n0                    5                60                   22  \n1                    5                60                   22  \n2                    5                73                   29  \n3                    5                73                   29  \n4                    7                76                   34  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sam had 9.0 dimes in his bank and his dad gave...</td>\n      <td>Sam has 16.0 dimes now</td>\n      <td>entailment</td>\n      <td>14</td>\n      <td>5</td>\n      <td>60</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sam had 9.0 dimes in his bank and his dad gave...</td>\n      <td>Sam has 17.0 dimes now</td>\n      <td>contradiction</td>\n      <td>14</td>\n      <td>5</td>\n      <td>60</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>15.0 pizzas were served today</td>\n      <td>entailment</td>\n      <td>13</td>\n      <td>5</td>\n      <td>73</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A restaurant served 9.0 pizzas during lunch an...</td>\n      <td>17.0 pizzas were served today</td>\n      <td>contradiction</td>\n      <td>13</td>\n      <td>5</td>\n      <td>73</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>There are 2.0 pencils in the drawer and Tim pl...</td>\n      <td>5.0 pencils are now there in total</td>\n      <td>entailment</td>\n      <td>15</td>\n      <td>7</td>\n      <td>76</td>\n      <td>34</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_insights(awp)\n",
    "\n",
    "awp[[\"premise\", \"hypothesis\", \"label\", \"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\nmean         16.236842             6.450139         84.925208   \nstd           5.694548             1.919570         32.652246   \n\n      hypothesis_char_cnt  \nmean            33.279778  \nstd             10.538450  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>16.236842</td>\n      <td>6.450139</td>\n      <td>84.925208</td>\n      <td>33.279778</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>5.694548</td>\n      <td>1.919570</td>\n      <td>32.652246</td>\n      <td>10.538450</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awp[[\"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].aggregate([\"mean\", \"std\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice that the premises are larger than the hypotheses, on average, by at least 2 times. By looking at some examples of premise and hypothesis pairs, we notice that for this dataset, the hypothesis is a summary of the premise, with respect to the quantities, while the premise is longer as it presents more quantities. We can deduce that for the AWPNLI, there will always be a calculation needed between the quantities in the premise, before a comparison can be made to infer the label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check for duplicates in the dataset, at a premise-hypothesis pair level. Do these duplicates have the same label? If not, which is the pair with the correct label?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awp[awp.duplicated(subset=[\"premise\", \"hypothesis\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the premise and hypothesis to lowercase, to ensure we do a case-insensitive check for duplicates as well\n",
    "awp[\"premise_lower\"] = awp[\"premise\"].str.lower()\n",
    "awp[\"hypothesis_lower\"] = awp[\"hypothesis\"].str.lower()\n",
    "\n",
    "awp[awp.duplicated(subset=[\"premise_lower\", \"hypothesis_lower\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's inspect the frequency of quantities in the dataset premises and hypotheses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_quantities_cnt  hypothesis_quantities_cnt\nmean                2.234072                   1.034626\nstd                 0.561652                   0.229980\nmin                 1.000000                   0.000000\nmax                 4.000000                   2.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_quantities_cnt</th>\n      <th>hypothesis_quantities_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>2.234072</td>\n      <td>1.034626</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.561652</td>\n      <td>0.229980</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.000000</td>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awp[[\"premise_quantities_cnt\", \"hypothesis_quantities_cnt\"]].aggregate([\"mean\", \"std\", \"min\", \"max\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               premise  \\\n52   Each of farmer Cunningham 's 6048.0 lambs is e...   \n53   Each of farmer Cunningham 's 6048.0 lambs is e...   \n54   A treasure hunter discovered a buried treasure...   \n55   A treasure hunter discovered a buried treasure...   \n188  Randy has 78.0 blocks and he uses 19.0 blocks ...   \n532  There was 698.0 children taking a test and 105...   \n533  There was 698.0 children taking a test and 105...   \n\n                                         hypothesis  \\\n52   5855.0 of Farmer Cunningham 's lambs are black   \n53   5854.0 of Farmer Cunningham 's lambs are black   \n54                   5110.0 of the gems were rubies   \n55                   5108.0 of the gems were rubies   \n188                            59.0 blocks are left   \n532              593.0 children had to sit it again   \n533              591.0 children had to sit it again   \n\n                            hypothesis_pos  \n52   [[NN, IN, NN, NN, POS, NNS, VBP, JJ]]  \n53   [[NN, IN, NN, NN, POS, NNS, VBP, JJ]]  \n54           [[NN, IN, DT, NNS, VBD, NNS]]  \n55           [[NN, IN, DT, NNS, VBD, NNS]]  \n188                  [[NN, NNS, VBP, VBN]]  \n532      [[JJ, NNS, VBD, TO, VB, PRP, RB]]  \n533      [[JJ, NNS, VBD, TO, VB, PRP, RB]]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>hypothesis_pos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>52</th>\n      <td>Each of farmer Cunningham 's 6048.0 lambs is e...</td>\n      <td>5855.0 of Farmer Cunningham 's lambs are black</td>\n      <td>[[NN, IN, NN, NN, POS, NNS, VBP, JJ]]</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>Each of farmer Cunningham 's 6048.0 lambs is e...</td>\n      <td>5854.0 of Farmer Cunningham 's lambs are black</td>\n      <td>[[NN, IN, NN, NN, POS, NNS, VBP, JJ]]</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>A treasure hunter discovered a buried treasure...</td>\n      <td>5110.0 of the gems were rubies</td>\n      <td>[[NN, IN, DT, NNS, VBD, NNS]]</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>A treasure hunter discovered a buried treasure...</td>\n      <td>5108.0 of the gems were rubies</td>\n      <td>[[NN, IN, DT, NNS, VBD, NNS]]</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>Randy has 78.0 blocks and he uses 19.0 blocks ...</td>\n      <td>59.0 blocks are left</td>\n      <td>[[NN, NNS, VBP, VBN]]</td>\n    </tr>\n    <tr>\n      <th>532</th>\n      <td>There was 698.0 children taking a test and 105...</td>\n      <td>593.0 children had to sit it again</td>\n      <td>[[JJ, NNS, VBD, TO, VB, PRP, RB]]</td>\n    </tr>\n    <tr>\n      <th>533</th>\n      <td>There was 698.0 children taking a test and 105...</td>\n      <td>591.0 children had to sit it again</td>\n      <td>[[JJ, NNS, VBD, TO, VB, PRP, RB]]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awp[awp[\"hypothesis_quantities_cnt\"] == 0][[\"premise\", \"hypothesis\", \"hypothesis_pos\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "awp_no_quantities = awp[(awp[\"hypothesis_quantities_cnt\"] == 0) | (awp[\"premise_quantities_cnt\"] == 0)][[\"premise\", \"hypothesis\", \"sample_index\"]]\n",
    "awp_no_quantities.to_excel(\"AWPNLI_no_quantities.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that sometimes the role of words is not extracted properly. These sentences seem to have 1 quantity, so the hypotheses have between 1 and 2 quantities, while the premises have between 1 and 4 quantities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check if the dataset contains features which are not in all datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['sentence2_tokens', 'sentence1_dep_parse', 'sentence1_binary_parse',\n       'sentence2_binary_parse', 'sentence1_syntax_parse', 'premise',\n       'hypothesis', 'sentence2_syntax_parse', 'hypothesis_pos',\n       'sentence2_dep_parse', 'label', 'premise_pos', 'sentence1_tokens',\n       'sample_index', 'premise_word_cnt', 'hypothesis_word_cnt',\n       'premise_char_cnt', 'hypothesis_char_cnt', 'premise_quantities_cnt',\n       'hypothesis_quantities_cnt', 'premise_lower', 'hypothesis_lower'],\n      dtype='object')"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awp.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are no extra features in this dataset to analyze."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### NewsNLI dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "                                    sentence2_tokens  \\\n0  [[joey, lepore, says, he, took, photos, of, on...   \n1  [[darren, sharper, has, been, charged, in, two...   \n2  [[weldon, says, she, 's, a, single, mom, of, t...   \n3  [[the, crash, took, the, lives, of, 79, people...   \n4  [[rip, currents, kill, four, in, alabama, ,, c...   \n\n                                    annotator_labels  \\\n0  [entailment, entailment, entailment, neutral, ...   \n1  [neutral, entailment, entailment, entailment, ...   \n2  [neutral, neutral, entailment, entailment, ent...   \n3  [neutral, entailment, entailment, entailment, ...   \n4  [neutral, entailment, neutral, entailment, ent...   \n\n                                    sentence1_tokens  \\\n0  [[lepore, said, he, was, moved, to, photograph...   \n1  [[sharper, ,, 38, ,, faces, rape, charges, in,...   \n2  [[i, am, the, single, mother, of, three, sons,...   \n3  [[in, addition, to, 79, fatalities, ,, some, 1...   \n4  [[treacherous, currents, took, at, least, four...   \n\n                                 sentence1_dep_parse  \\\n0  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...   \n1  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...   \n2  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...   \n3  [[{'dep': 'ROOT', 'dependent': 11, 'governorGl...   \n4  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...   \n\n                              sentence2_syntax_parse  \\\n0  [(ROOT\\n  (S\\n    (NP (NN joey) (NN lepore))\\n...   \n1  [(ROOT\\n  (S\\n    (NP\\n      (NP (NN darren))\\...   \n2  [(ROOT\\n  (S\\n    (NP (NN weldon))\\n    (VP (V...   \n3  [(ROOT\\n  (S\\n    (NP (DT the) (NN crash))\\n  ...   \n4  [(ROOT\\n  (S\\n    (NP (NN rip) (NNS currents))...   \n\n                          Phenomena  \\\n0                                []   \n1  [Implicit quantity,  Arithmetic]   \n2                                []   \n3                                []   \n4                                []   \n\n                              sentence1_binary_parse Hard  \\\n0  ( ( ( Lepore ) ) ( ( said ) ( ( ( ( he ) ) ( (...  Yes   \n1  ( ( ( ( ( Sharper ) ) (, , ) ( ( 38 ) ) (, , )...  Yes   \n2  ( ( ( ( I ) ) ( ( am ) ( ( ( the ) ( single ) ...  Yes   \n3  ( ( ( In ) ( ( ( addition ) ) ( ( to ) ( ( 79 ...  Yes   \n4  ( ( ( Treacherous ) ( currents ) ) ( ( ( took ...  Yes   \n\n                                     sentence2_parse  \\\n0  (S (NP (NNP Joey) (NNP Lepore)) (VP (VBZ says)...   \n1  (S (NP (NNP Darren) (NNP Sharper)) (VP (VBZ ha...   \n2  (S (NP (NNP Weldon)) (VP (VBZ says) (SBAR (S (...   \n3  (S (NP (DT The) (NN crash)) (VP (VP (VBD took)...   \n4  (S (NP (NNP Rip) (NNS currents)) (VP (VBP kill...   \n\n                              sentence2_binary_parse  ...  \\\n0  ( ( ( Joey ) ( Lepore ) ) ( ( says ) ( ( ( ( h...  ...   \n1  ( ( ( Darren ) ( Sharper ) ) ( ( has ) ( ( bee...  ...   \n2  ( ( ( Weldon ) ) ( ( says ) ( ( ( ( she ) ) ( ...  ...   \n3  ( ( ( The ) ( crash ) ) ( ( ( took ) ( ( ( the...  ...   \n4  ( ( ( Rip ) ( currents ) ) ( ( kill ) ( ( four...  ...   \n\n                                             premise  \\\n0  Lepore said he was moved to photograph the slu...   \n1  Sharper , 38 , faces rape charges in Arizona a...   \n2  I am the single mother of three sons -- grown ...   \n3  In addition to 79 fatalities , some 170 passen...   \n4  Treacherous currents took at least four lives ...   \n\n                                          hypothesis  \\\n0  Joey Lepore says he took photos of one guard s...   \n1  Darren Sharper has been charged in two states ...   \n2  Weldon says she 's a single mom of three flour...   \n3  The crash took the lives of 79 people and inju...   \n4  Rip currents kill four in Alabama , close beac...   \n\n                                     sentence1_parse genre  \\\n0  (S (NP (NNP Lepore)) (VP (VBD said) (SBAR (S (...  News   \n1  (S (S (NP (NP (JJR Sharper)) (, ,) (NP (CD 38)...  News   \n2  (S (S (NP (PRP I)) (VP (VBP am) (NP (NP (DT th...  News   \n3  (S (PP (IN In) (NP (NP (NN addition)) (PP (TO ...  News   \n4  (S (NP (JJ Treacherous) (NNS currents)) (VP (V...  News   \n\n                                      hypothesis_pos  \\\n0  [[NN, NN, VBZ, PRP, VBD, NNS, IN, CD, NN, VBG,...   \n1  [[NN, JJR, VBZ, VBN, VBN, IN, CD, NNS, IN, DT,...   \n2  [[NN, VBZ, PRP, VBZ, DT, JJ, NN, IN, CD, VBG, ...   \n3  [[DT, NN, VBD, DT, NNS, IN, CD, NNS, CC, VBN, ...   \n4   [[NN, NNS, VBP, CD, IN, NN, ,, JJ, NNS, IN, NN]]   \n\n                                 sentence2_dep_parse       label  \\\n0  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...  entailment   \n1  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...  entailment   \n2  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...  entailment   \n3  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...  entailment   \n4  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...  entailment   \n\n                                         premise_pos PairID  sample_index  \n0  [[NN, VBD, PRP, VBD, VBN, TO, NN, DT, JJ, NNS,...     27             0  \n1  [[JJR, ,, CD, ,, VBZ, NN, NNS, IN, NN, CC, NN,...   2670             1  \n2  [[LS, VBP, DT, JJ, NN, IN, CD, NNS, :, :, VBN,...   1401             2  \n3  [[IN, NN, TO, CD, NNS, ,, DT, CD, NNS, VBD, VB...    974             3  \n4  [[JJ, NNS, VBD, IN, JJS, CD, NNS, IN, DT, NN, ...   2464             4  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence2_tokens</th>\n      <th>annotator_labels</th>\n      <th>sentence1_tokens</th>\n      <th>sentence1_dep_parse</th>\n      <th>sentence2_syntax_parse</th>\n      <th>Phenomena</th>\n      <th>sentence1_binary_parse</th>\n      <th>Hard</th>\n      <th>sentence2_parse</th>\n      <th>sentence2_binary_parse</th>\n      <th>...</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sentence1_parse</th>\n      <th>genre</th>\n      <th>hypothesis_pos</th>\n      <th>sentence2_dep_parse</th>\n      <th>label</th>\n      <th>premise_pos</th>\n      <th>PairID</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[joey, lepore, says, he, took, photos, of, on...</td>\n      <td>[entailment, entailment, entailment, neutral, ...</td>\n      <td>[[lepore, said, he, was, moved, to, photograph...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN joey) (NN lepore))\\n...</td>\n      <td>[]</td>\n      <td>( ( ( Lepore ) ) ( ( said ) ( ( ( ( he ) ) ( (...</td>\n      <td>Yes</td>\n      <td>(S (NP (NNP Joey) (NNP Lepore)) (VP (VBZ says)...</td>\n      <td>( ( ( Joey ) ( Lepore ) ) ( ( says ) ( ( ( ( h...</td>\n      <td>...</td>\n      <td>Lepore said he was moved to photograph the slu...</td>\n      <td>Joey Lepore says he took photos of one guard s...</td>\n      <td>(S (NP (NNP Lepore)) (VP (VBD said) (SBAR (S (...</td>\n      <td>News</td>\n      <td>[[NN, NN, VBZ, PRP, VBD, NNS, IN, CD, NN, VBG,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[NN, VBD, PRP, VBD, VBN, TO, NN, DT, JJ, NNS,...</td>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[darren, sharper, has, been, charged, in, two...</td>\n      <td>[neutral, entailment, entailment, entailment, ...</td>\n      <td>[[sharper, ,, 38, ,, faces, rape, charges, in,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>[(ROOT\\n  (S\\n    (NP\\n      (NP (NN darren))\\...</td>\n      <td>[Implicit quantity,  Arithmetic]</td>\n      <td>( ( ( ( ( Sharper ) ) (, , ) ( ( 38 ) ) (, , )...</td>\n      <td>Yes</td>\n      <td>(S (NP (NNP Darren) (NNP Sharper)) (VP (VBZ ha...</td>\n      <td>( ( ( Darren ) ( Sharper ) ) ( ( has ) ( ( bee...</td>\n      <td>...</td>\n      <td>Sharper , 38 , faces rape charges in Arizona a...</td>\n      <td>Darren Sharper has been charged in two states ...</td>\n      <td>(S (S (NP (NP (JJR Sharper)) (, ,) (NP (CD 38)...</td>\n      <td>News</td>\n      <td>[[NN, JJR, VBZ, VBN, VBN, IN, CD, NNS, IN, DT,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[JJR, ,, CD, ,, VBZ, NN, NNS, IN, NN, CC, NN,...</td>\n      <td>2670</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[weldon, says, she, 's, a, single, mom, of, t...</td>\n      <td>[neutral, neutral, entailment, entailment, ent...</td>\n      <td>[[i, am, the, single, mother, of, three, sons,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN weldon))\\n    (VP (V...</td>\n      <td>[]</td>\n      <td>( ( ( ( I ) ) ( ( am ) ( ( ( the ) ( single ) ...</td>\n      <td>Yes</td>\n      <td>(S (NP (NNP Weldon)) (VP (VBZ says) (SBAR (S (...</td>\n      <td>( ( ( Weldon ) ) ( ( says ) ( ( ( ( she ) ) ( ...</td>\n      <td>...</td>\n      <td>I am the single mother of three sons -- grown ...</td>\n      <td>Weldon says she 's a single mom of three flour...</td>\n      <td>(S (S (NP (PRP I)) (VP (VBP am) (NP (NP (DT th...</td>\n      <td>News</td>\n      <td>[[NN, VBZ, PRP, VBZ, DT, JJ, NN, IN, CD, VBG, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[LS, VBP, DT, JJ, NN, IN, CD, NNS, :, :, VBN,...</td>\n      <td>1401</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[the, crash, took, the, lives, of, 79, people...</td>\n      <td>[neutral, entailment, entailment, entailment, ...</td>\n      <td>[[in, addition, to, 79, fatalities, ,, some, 1...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 11, 'governorGl...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (DT the) (NN crash))\\n  ...</td>\n      <td>[]</td>\n      <td>( ( ( In ) ( ( ( addition ) ) ( ( to ) ( ( 79 ...</td>\n      <td>Yes</td>\n      <td>(S (NP (DT The) (NN crash)) (VP (VP (VBD took)...</td>\n      <td>( ( ( The ) ( crash ) ) ( ( ( took ) ( ( ( the...</td>\n      <td>...</td>\n      <td>In addition to 79 fatalities , some 170 passen...</td>\n      <td>The crash took the lives of 79 people and inju...</td>\n      <td>(S (PP (IN In) (NP (NP (NN addition)) (PP (TO ...</td>\n      <td>News</td>\n      <td>[[DT, NN, VBD, DT, NNS, IN, CD, NNS, CC, VBN, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[IN, NN, TO, CD, NNS, ,, DT, CD, NNS, VBD, VB...</td>\n      <td>974</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[rip, currents, kill, four, in, alabama, ,, c...</td>\n      <td>[neutral, entailment, neutral, entailment, ent...</td>\n      <td>[[treacherous, currents, took, at, least, four...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN rip) (NNS currents))...</td>\n      <td>[]</td>\n      <td>( ( ( Treacherous ) ( currents ) ) ( ( ( took ...</td>\n      <td>Yes</td>\n      <td>(S (NP (NNP Rip) (NNS currents)) (VP (VBP kill...</td>\n      <td>( ( ( Rip ) ( currents ) ) ( ( kill ) ( ( four...</td>\n      <td>...</td>\n      <td>Treacherous currents took at least four lives ...</td>\n      <td>Rip currents kill four in Alabama , close beac...</td>\n      <td>(S (NP (JJ Treacherous) (NNS currents)) (VP (V...</td>\n      <td>News</td>\n      <td>[[NN, NNS, VBP, CD, IN, NN, ,, JJ, NNS, IN, NN]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[JJ, NNS, VBD, IN, JJS, CD, NNS, IN, DT, NN, ...</td>\n      <td>2464</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 21 columns</p>\n</div>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "label\nentailment    0.507231\nneutral       0.492769\nName: proportion, dtype: float64"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['label'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observe a balanced split between the 2 labels of this dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  Lepore said he was moved to photograph the slu...   \n1  Sharper , 38 , faces rape charges in Arizona a...   \n2  I am the single mother of three sons -- grown ...   \n3  In addition to 79 fatalities , some 170 passen...   \n4  Treacherous currents took at least four lives ...   \n\n                                          hypothesis       label  \\\n0  Joey Lepore says he took photos of one guard s...  entailment   \n1  Darren Sharper has been charged in two states ...  entailment   \n2  Weldon says she 's a single mom of three flour...  entailment   \n3  The crash took the lives of 79 people and inju...  entailment   \n4  Rip currents kill four in Alabama , close beac...  entailment   \n\n   premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\n0                20                   14               123   \n1                22                   13               114   \n2                17                   16                78   \n3                12                   12                65   \n4                23                   11               128   \n\n   hypothesis_char_cnt  \n0                   73  \n1                   76  \n2                   85  \n3                   58  \n4                   60  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lepore said he was moved to photograph the slu...</td>\n      <td>Joey Lepore says he took photos of one guard s...</td>\n      <td>entailment</td>\n      <td>20</td>\n      <td>14</td>\n      <td>123</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sharper , 38 , faces rape charges in Arizona a...</td>\n      <td>Darren Sharper has been charged in two states ...</td>\n      <td>entailment</td>\n      <td>22</td>\n      <td>13</td>\n      <td>114</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I am the single mother of three sons -- grown ...</td>\n      <td>Weldon says she 's a single mom of three flour...</td>\n      <td>entailment</td>\n      <td>17</td>\n      <td>16</td>\n      <td>78</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In addition to 79 fatalities , some 170 passen...</td>\n      <td>The crash took the lives of 79 people and inju...</td>\n      <td>entailment</td>\n      <td>12</td>\n      <td>12</td>\n      <td>65</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Treacherous currents took at least four lives ...</td>\n      <td>Rip currents kill four in Alabama , close beac...</td>\n      <td>entailment</td>\n      <td>23</td>\n      <td>11</td>\n      <td>128</td>\n      <td>60</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_insights(news)\n",
    "\n",
    "news[[\"premise\", \"hypothesis\", \"label\", \"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\nmean         22.330579            11.984504        120.600207   \nstd           8.180754             2.999615         46.449696   \n\n      hypothesis_char_cnt  \nmean            66.581612  \nstd             15.740119  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>22.330579</td>\n      <td>11.984504</td>\n      <td>120.600207</td>\n      <td>66.581612</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.180754</td>\n      <td>2.999615</td>\n      <td>46.449696</td>\n      <td>15.740119</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[[\"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].aggregate([\"mean\", \"std\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We observe that the premises are on average twice as long as the hypothesis, with respect to the number of words and characters. By inspecting some of the premise-hypothesis pairs, we notice that the hypothesis is usually a shorter, rephrased version of the premise (similar to a summary of a long sentence). However, in contrast to the AWPNLI dataset, there are not necessarily calculations that need to be done in either of the 2 sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check for duplicates in the dataset, at a premise-hypothesis pair level. Do these duplicates have the same label? If not, which is the pair with the correct label?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news.duplicated(subset=[\"premise\", \"hypothesis\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news.duplicated(subset=[\"premise\", \"hypothesis\", \"label\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like the duplicates have the same label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the premise and hypothesis to lowercase, to ensure we do a case-insensitive check for duplicates as well\n",
    "news[\"premise_lower\"] = news[\"premise\"].str.lower()\n",
    "news[\"hypothesis_lower\"] = news[\"hypothesis\"].str.lower()\n",
    "\n",
    "news[news.duplicated(subset=[\"premise_lower\", \"hypothesis_lower\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               premise  \\\n41   Mycoskie had already started four other busine...   \n67   Cobb declined two requests from CNN to respond...   \n151  In fact , Wernick had only seen one zombie fil...   \n300  There were no reports of serious injuries , bu...   \n416  42 percent of homeless children are younger th...   \n\n                                            hypothesis       label  \n41   Blake Mycoskie had launched four other start-u...  entailment  \n67   Cobb declined two requests to speak with CNN f...  entailment  \n151  One of film 's writers had seen just one zombi...  entailment  \n300  At least 8 reported arrested , but no reports ...  entailment  \n416  Study says 42 percent of homeless children are...  entailment  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>41</th>\n      <td>Mycoskie had already started four other busine...</td>\n      <td>Blake Mycoskie had launched four other start-u...</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>Cobb declined two requests from CNN to respond...</td>\n      <td>Cobb declined two requests to speak with CNN f...</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>In fact , Wernick had only seen one zombie fil...</td>\n      <td>One of film 's writers had seen just one zombi...</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>There were no reports of serious injuries , bu...</td>\n      <td>At least 8 reported arrested , but no reports ...</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>416</th>\n      <td>42 percent of homeless children are younger th...</td>\n      <td>Study says 42 percent of homeless children are...</td>\n      <td>entailment</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news.duplicated(subset=[\"premise\", \"hypothesis\"])][[\"premise\", \"hypothesis\", \"label\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's inspect the frequencies of quantities in the premises and hypotheses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_quantities_cnt  hypothesis_quantities_cnt\nmean                1.622934                   1.380165\nstd                 1.033748                   0.736196\nmin                 0.000000                   0.000000\nmax                12.000000                   8.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_quantities_cnt</th>\n      <th>hypothesis_quantities_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>1.622934</td>\n      <td>1.380165</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.033748</td>\n      <td>0.736196</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>12.000000</td>\n      <td>8.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[[\"premise_quantities_cnt\", \"hypothesis_quantities_cnt\"]].aggregate([\"mean\", \"std\", \"min\", \"max\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               premise  \\\n94   Shaffer : Just to be clear , I was offered the...   \n127  That inmate and the county worker were undergo...   \n178  But terrarium gardens and other tiny plant pro...   \n317  Jiang has became a celebrity , followed by loc...   \n364  '' Jeremy Lin is a marketing dream come true ,...   \n507  ( CNN ) -- Tony Gwynn , a Hall of Fame outfiel...   \n513  After they complete their sentence , the pair ...   \n530  In the latest attack , a parked motorcycle bom...   \n569  ( CNET.com ) -- The HP Pavilion Media Center T...   \n592  The latest trend is theaters offering '' luxur...   \n671  ( CNN ) -- Indonesian police are searching for...   \n697  While the cardinal-electors are locked in the ...   \n778  Hundreds of thousands took to the streets in B...   \n802  The contest rules spelled out that NASA reserv...   \n810  Tamer was at the same demonstration Hamza atte...   \n875  Bobby Jindal declared a statewide state of eme...   \n886  The incident started after South Korean comman...   \n887  The second explosion took place at a crowded b...   \n894  The British island group of Tristan da Cunha s...   \n900  Five service members hurt , building damaged i...   \n904  With the Eastern Cape being so key to the coun...   \n909  Stone has come up with a name for the new stat...   \n911  With a win in Dubai , Stenson would become the...   \n919  I 'd rather be na ve , heartfelt and hopeful t...   \n951  Kapoor and Eroshevich were each also charged w...   \n960  Armed officers were confronted by a pack of do...   \n967  Sponseller graduated from The Citadel and is a...   \n\n                                            hypothesis  sample_index  \n94   Shaffer was offered chance to play Jerry Seinf...            94  \n127  Two of the injured were undergoing emergency s...           127  \n178  Terrariums and other small plant projects are ...           178  \n317  Newspaper headline hails her as '' China 's Mo...           317  \n364  Lin is a '' marketing dream come true , '' one...           364  \n507  Gwynn died at 54 after a long battle with sali...           507  \n513  The two Britons will be deported after they co...           513  \n530  Motorcycle bomb kills six in Sunni neighborhoo...           530  \n569  The HP Pavilion Media Center TV m8120n retails...           569  \n592  Premium screening rooms offer cocktails , wine...           592  \n671  More than 200 inmates escaped from Indonesian ...           671  \n697  115 cardinal-electors are gathered in the Sist...           697  \n778  At the height of the war , 46,000 British troo...           778  \n802        NASA reserves right to pick name for Node 3           802  \n810  Tamer Mohammed al Sharey , 15 , disappeared at...           810  \n875  Florida governor declares a state of emergency...           875  \n886  South Korea has seized the ship and its nine s...           886  \n887  The earlier explosion injured 12 people at a N...           887  \n894  The islands of Tristan da Cunha sit 1,750 mile...           894  \n900  He said the attack included rockets , small ar...           900  \n904  The Eastern Cape provides 51 % of South Africa...           904  \n909  He has drawn plans for 13 counties to form the...           909  \n911  Henrik Stenson shoots an eight-under-par 64 to...           911  \n919  '' Call me na ve , '' Vedder said in website post           919  \n951  All three charged with giving '' a controlled ...           951  \n960  Four dogs were shot by armed police officers a...           960  \n967  Tom Sponseller , 61 , is head of the South Car...           967  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>94</th>\n      <td>Shaffer : Just to be clear , I was offered the...</td>\n      <td>Shaffer was offered chance to play Jerry Seinf...</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>That inmate and the county worker were undergo...</td>\n      <td>Two of the injured were undergoing emergency s...</td>\n      <td>127</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>But terrarium gardens and other tiny plant pro...</td>\n      <td>Terrariums and other small plant projects are ...</td>\n      <td>178</td>\n    </tr>\n    <tr>\n      <th>317</th>\n      <td>Jiang has became a celebrity , followed by loc...</td>\n      <td>Newspaper headline hails her as '' China 's Mo...</td>\n      <td>317</td>\n    </tr>\n    <tr>\n      <th>364</th>\n      <td>'' Jeremy Lin is a marketing dream come true ,...</td>\n      <td>Lin is a '' marketing dream come true , '' one...</td>\n      <td>364</td>\n    </tr>\n    <tr>\n      <th>507</th>\n      <td>( CNN ) -- Tony Gwynn , a Hall of Fame outfiel...</td>\n      <td>Gwynn died at 54 after a long battle with sali...</td>\n      <td>507</td>\n    </tr>\n    <tr>\n      <th>513</th>\n      <td>After they complete their sentence , the pair ...</td>\n      <td>The two Britons will be deported after they co...</td>\n      <td>513</td>\n    </tr>\n    <tr>\n      <th>530</th>\n      <td>In the latest attack , a parked motorcycle bom...</td>\n      <td>Motorcycle bomb kills six in Sunni neighborhoo...</td>\n      <td>530</td>\n    </tr>\n    <tr>\n      <th>569</th>\n      <td>( CNET.com ) -- The HP Pavilion Media Center T...</td>\n      <td>The HP Pavilion Media Center TV m8120n retails...</td>\n      <td>569</td>\n    </tr>\n    <tr>\n      <th>592</th>\n      <td>The latest trend is theaters offering '' luxur...</td>\n      <td>Premium screening rooms offer cocktails , wine...</td>\n      <td>592</td>\n    </tr>\n    <tr>\n      <th>671</th>\n      <td>( CNN ) -- Indonesian police are searching for...</td>\n      <td>More than 200 inmates escaped from Indonesian ...</td>\n      <td>671</td>\n    </tr>\n    <tr>\n      <th>697</th>\n      <td>While the cardinal-electors are locked in the ...</td>\n      <td>115 cardinal-electors are gathered in the Sist...</td>\n      <td>697</td>\n    </tr>\n    <tr>\n      <th>778</th>\n      <td>Hundreds of thousands took to the streets in B...</td>\n      <td>At the height of the war , 46,000 British troo...</td>\n      <td>778</td>\n    </tr>\n    <tr>\n      <th>802</th>\n      <td>The contest rules spelled out that NASA reserv...</td>\n      <td>NASA reserves right to pick name for Node 3</td>\n      <td>802</td>\n    </tr>\n    <tr>\n      <th>810</th>\n      <td>Tamer was at the same demonstration Hamza atte...</td>\n      <td>Tamer Mohammed al Sharey , 15 , disappeared at...</td>\n      <td>810</td>\n    </tr>\n    <tr>\n      <th>875</th>\n      <td>Bobby Jindal declared a statewide state of eme...</td>\n      <td>Florida governor declares a state of emergency...</td>\n      <td>875</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>The incident started after South Korean comman...</td>\n      <td>South Korea has seized the ship and its nine s...</td>\n      <td>886</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>The second explosion took place at a crowded b...</td>\n      <td>The earlier explosion injured 12 people at a N...</td>\n      <td>887</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>The British island group of Tristan da Cunha s...</td>\n      <td>The islands of Tristan da Cunha sit 1,750 mile...</td>\n      <td>894</td>\n    </tr>\n    <tr>\n      <th>900</th>\n      <td>Five service members hurt , building damaged i...</td>\n      <td>He said the attack included rockets , small ar...</td>\n      <td>900</td>\n    </tr>\n    <tr>\n      <th>904</th>\n      <td>With the Eastern Cape being so key to the coun...</td>\n      <td>The Eastern Cape provides 51 % of South Africa...</td>\n      <td>904</td>\n    </tr>\n    <tr>\n      <th>909</th>\n      <td>Stone has come up with a name for the new stat...</td>\n      <td>He has drawn plans for 13 counties to form the...</td>\n      <td>909</td>\n    </tr>\n    <tr>\n      <th>911</th>\n      <td>With a win in Dubai , Stenson would become the...</td>\n      <td>Henrik Stenson shoots an eight-under-par 64 to...</td>\n      <td>911</td>\n    </tr>\n    <tr>\n      <th>919</th>\n      <td>I 'd rather be na ve , heartfelt and hopeful t...</td>\n      <td>'' Call me na ve , '' Vedder said in website post</td>\n      <td>919</td>\n    </tr>\n    <tr>\n      <th>951</th>\n      <td>Kapoor and Eroshevich were each also charged w...</td>\n      <td>All three charged with giving '' a controlled ...</td>\n      <td>951</td>\n    </tr>\n    <tr>\n      <th>960</th>\n      <td>Armed officers were confronted by a pack of do...</td>\n      <td>Four dogs were shot by armed police officers a...</td>\n      <td>960</td>\n    </tr>\n    <tr>\n      <th>967</th>\n      <td>Sponseller graduated from The Citadel and is a...</td>\n      <td>Tom Sponseller , 61 , is head of the South Car...</td>\n      <td>967</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_no_quantities = news[(news[\"premise_quantities_cnt\"] == 0) | (news[\"hypothesis_quantities_cnt\"] == 0)][[\"premise\", \"hypothesis\", \"sample_index\"]]\n",
    "news_no_quantities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "news_no_quantities[[\"premise\", \"hypothesis\", \"sample_index\"]].to_excel(\"NewsNLI_no_quantities.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Manual inspection of these samples reveals that indeed there are no quantities in them, either in numerical or verbal format. However, if at least one of the premise or hypothesis does contain a quantity, the pair should not be discarded. These could represent \"neutral\" samples, where the label is not necessarily inferred on a quantitative basis, but on the lack of details in one sentence (usually the premise) to support the quantitative details in the other (usually the hypothesis)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               premise  \\\n178  But terrarium gardens and other tiny plant pro...   \n919  I 'd rather be na ve , heartfelt and hopeful t...   \n\n                                            hypothesis  sample_index  \n178  Terrariums and other small plant projects are ...           178  \n919  '' Call me na ve , '' Vedder said in website post           919  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>178</th>\n      <td>But terrarium gardens and other tiny plant pro...</td>\n      <td>Terrariums and other small plant projects are ...</td>\n      <td>178</td>\n    </tr>\n    <tr>\n      <th>919</th>\n      <td>I 'd rather be na ve , heartfelt and hopeful t...</td>\n      <td>'' Call me na ve , '' Vedder said in website post</td>\n      <td>919</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_no_quantities = news[(news[\"premise_quantities_cnt\"] == 0) & (news[\"hypothesis_quantities_cnt\"] == 0)][[\"premise\", \"hypothesis\", \"sample_index\"]]\n",
    "pairs_no_quantities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These 2 samples could be dropped as they do not contain any quantitative information so they are not part of the type of sentences QNLI focuses on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's inspect the data on the annotator labels - for how many samples were there disagreements between annotators?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "annotator_unique_labels\n2    0.646694\n1    0.351240\n3    0.002066\nName: proportion, dtype: float64"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[\"annotator_unique_labels\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like in almost 65% of cases, there was a disagreement between the annotators. This can also indicate a higher complexity of the sentences in this dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "annotator_unique_labels  label     \n1                        neutral       0.523529\n                         entailment    0.476471\n2                        entailment    0.523962\n                         neutral       0.476038\n3                        entailment    0.500000\n                         neutral       0.500000\nName: proportion, dtype: float64"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.groupby(\"annotator_unique_labels\")[\"label\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It also looks like the disagreements were almost equally split between samples from both categories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check if the dataset contains features which are not in all datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['sentence2_tokens', 'annotator_labels', 'sentence1_tokens',\n       'sentence1_dep_parse', 'sentence2_syntax_parse', 'Phenomena',\n       'sentence1_binary_parse', 'Hard', 'sentence2_parse',\n       'sentence2_binary_parse', 'sentence1_syntax_parse', 'premise',\n       'hypothesis', 'sentence1_parse', 'genre', 'hypothesis_pos',\n       'sentence2_dep_parse', 'label', 'premise_pos', 'PairID', 'sample_index',\n       'premise_word_cnt', 'hypothesis_word_cnt', 'premise_char_cnt',\n       'hypothesis_char_cnt', 'premise_quantities_cnt',\n       'hypothesis_quantities_cnt', 'annotator_unique_labels', 'premise_lower',\n       'hypothesis_lower'],\n      dtype='object')"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "news[\"Phenomena\"] = news[\"Phenomena\"].fillna(\"[]\")\n",
    "news[\"Hard\"] = news[\"Hard\"].fillna(\"Unknown\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "Phenomena\n",
      "[]                                               461\n",
      "[]                                               403\n",
      "[Numeration]                                      32\n",
      "[QC]                                              11\n",
      "[Count]                                            8\n",
      "[Implicit quantity,  Arithmetic]                   6\n",
      "[Quantifiers]                                      5\n",
      "[Unit conversion]                                  4\n",
      "[Numeration,  Arithmetic]                          2\n",
      "[Arithmetic]                                       2\n",
      "[Named Set]                                        2\n",
      "[Ratios]                                           2\n",
      "[Named set resolution]                             2\n",
      "[SETS]                                             2\n",
      "[Numeration,  Unit conversion]                     2\n",
      "[Ordinality]                                       2\n",
      "[Quantity conversion]                              2\n",
      "[Quantity conversion,  Quantifiers]                2\n",
      "[Quantifiers,  Ranges]                             1\n",
      "[MULIPLE]                                          1\n",
      "[COUNT]                                            1\n",
      "[Sets]                                             1\n",
      "[Reasoning]                                        1\n",
      "[Quantifiers,  Numeration]                         1\n",
      "[Named set resolution,  Implicit quantity]         1\n",
      "[Quantifiers,  Quantity conversion]                1\n",
      "[Numeration,  Quantity conversion]                 1\n",
      "[Approximation]                                    1\n",
      "[Implicit quantity,  Numeration]                   1\n",
      "[Multi-hop reasoning,  Quantity conversion]        1\n",
      "[Arithmetic,  Numeration]                          1\n",
      "[Named set resolution,  Arithmetic]                1\n",
      "[Quantity conversion,  Approximation]              1\n",
      "[Implicit quantity,  Arithmetic,  Numeration]      1\n",
      "[Approximation,  Quantifiers]                      1\n",
      "[Quantifier,  Geography]                           1\n",
      "Name: count, dtype: int64\n",
      "###############\n",
      "Hard\n",
      "Yes    507\n",
      "No     461\n",
      "Name: count, dtype: int64\n",
      "###############\n",
      "genre\n",
      "News    968\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "extra_columns = [\"Phenomena\", \"Hard\", \"genre\"]\n",
    "for column in extra_columns:\n",
    "    print(f\"###############\")\n",
    "    print(news[column].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like the 'genre' column is not informative, it points to the source of the samples, namely news articles.\n",
    "\n",
    "The 'Hard' column is of Boolean nature, it may indicate if a certain example is harder to classify (more complex), but this is only an assumption. There is no information in the original paper about this column or its meaning.\n",
    "\n",
    "The 'Phenomena' column seems to assign categories to some of the samples, of quantitative phenomena. It would be interesting to analyze the results on the samples with Phenomena vs the samples without, to see if there is any discrepancy. The ratio of samples with phenomena is relatively low."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "Phenomena\n[]                                               403\n[Numeration]                                      32\n[QC]                                              11\n[Count]                                            8\n[Implicit quantity,  Arithmetic]                   6\n[Quantifiers]                                      5\n[Unit conversion]                                  4\n[SETS]                                             2\n[Ratios]                                           2\n[Named Set]                                        2\n[Numeration,  Arithmetic]                          2\n[Named set resolution]                             2\n[Arithmetic]                                       2\n[Ordinality]                                       2\n[Numeration,  Unit conversion]                     2\n[Quantity conversion,  Quantifiers]                2\n[Quantity conversion]                              2\n[Arithmetic,  Numeration]                          1\n[Implicit quantity,  Arithmetic,  Numeration]      1\n[Sets]                                             1\n[COUNT]                                            1\n[MULIPLE]                                          1\n[Reasoning]                                        1\n[Approximation,  Quantifiers]                      1\n[Quantifiers,  Ranges]                             1\n[Named set resolution,  Arithmetic]                1\n[Named set resolution,  Implicit quantity]         1\n[Quantifiers,  Quantity conversion]                1\n[Numeration,  Quantity conversion]                 1\n[Approximation]                                    1\n[Quantifiers,  Numeration]                         1\n[Quantity conversion,  Approximation]              1\n[Implicit quantity,  Numeration]                   1\n[Multi-hop reasoning,  Quantity conversion]        1\n[Quantifier,  Geography]                           1\nName: count, dtype: int64"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## are 'Hard' examples the ones with 'Phenomena'?\n",
    "\n",
    "news[news['Hard'] == 'Yes'][\"Phenomena\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "Phenomena\n[]    461\nName: count, dtype: int64"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news['Hard'] == 'No'][\"Phenomena\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All samples with phenomena are categorized as 'hard', but there are also samples with no-phenomena out in the same category. It remains unclear what the 'Hard' column could represent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reddit dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "                                    sentence2_tokens  \\\n0  [[sensex, and, nifty, up, ,, 2, sept, nifty, s...   \n1  [[at, davos, ,, wall, street, billionaire, ste...   \n2  [[sensex, down, 74.58, points, ,, nifty, futur...   \n3  [[at, davos, ,, wall, street, billionaire, mr,...   \n4  [[stocks, nifty, future, call, today, :, sense...   \n\n                                    sentence1_tokens  \\\n0  [[stocks, nifty, future, call, today, :, sense...   \n1  [[at, davos, ,, financial, billionaire, schwar...   \n2  [[sensex, nifty, up, ,, today, stocks, nifty, ...   \n3  [[at, davos, ,, financial, billionaire, schwar...   \n4  [[sensex, and, nifty, up, ,, 2, sept, nifty, s...   \n\n                                 sentence1_dep_parse  \\\n0  [[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...   \n1  [[{'dep': 'ROOT', 'dependent': 19, 'governorGl...   \n2  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...   \n3  [[{'dep': 'ROOT', 'dependent': 19, 'governorGl...   \n4  [[{'dep': 'ROOT', 'dependent': 1, 'governorGlo...   \n\n                              sentence2_syntax_parse  \\\n0  [(ROOT\\n  (NP\\n    (NP\\n      (NP (NN sensex))...   \n1  [(ROOT\\n  (S\\n    (PP (IN at)\\n      (NP (NNP ...   \n2  [(ROOT\\n  (S\\n    (NP\\n      (NP (NN sensex))\\...   \n3  [(ROOT\\n  (S\\n    (PP (IN at)\\n      (NP (NNP ...   \n4  [(ROOT\\n  (FRAG\\n    (NP-TMP\\n      (NP (NNS s...   \n\n                              sentence1_binary_parse  \\\n0  ( (NP-TMP ( ( stocks ) ) ( ( nifty ) ( future ...   \n1  ( ( ( At ) ( ( DAVOS ) ) ) (, , ) ( ( ( Financ...   \n2  ( ( ( ( ( SENSEX ) ( Nifty ) ) ( ( up ) ) ) (,...   \n3  ( ( ( At ) ( ( DAVOS ) ) ) (, , ) ( ( ( Financ...   \n4  ( ( ( ( ( Sensex ) ( and ) ( Nifty ) ) ( ( up ...   \n\n                                     sentence2_parse  \\\n0  (NP (NP (NP (NP (NNP Sensex) (CC and) (NNP Nif...   \n1  (S (PP (IN At) (NP (NNP Davos))) (, ,) (NP (NP...   \n2  (SINV (VP (VB Sensex) (PRT (RP down)) (NP (NP ...   \n3  (S (PP (IN At) (NP (NNP Davos))) (, ,) (NP (NP...   \n4  (FRAG (NP-TMP (NP (NNS stocks)) (NP (JJ nifty)...   \n\n                              sentence2_binary_parse  \\\n0  ( ( ( ( ( Sensex ) ( and ) ( Nifty ) ) ( ( up ...   \n1  ( ( ( At ) ( ( Davos ) ) ) (, , ) ( ( ( Wall )...   \n2  ( ( ( Sensex ) ( ( down ) ) ( ( ( 74.58 ) ( po...   \n3  ( ( ( At ) ( ( Davos ) ) ) (, , ) ( ( ( Wall )...   \n4  ( (NP-TMP ( ( stocks ) ) ( ( nifty ) ( future ...   \n\n                              sentence1_syntax_parse  \\\n0  [(ROOT\\n  (FRAG\\n    (NP-TMP\\n      (NP (NNS s...   \n1  [(ROOT\\n  (S\\n    (PP (IN at)\\n      (NP (NNP ...   \n2  [(ROOT\\n  (FRAG\\n    (NP\\n      (NP\\n        (...   \n3  [(ROOT\\n  (S\\n    (PP (IN at)\\n      (NP (NNP ...   \n4  [(ROOT\\n  (NP\\n    (NP\\n      (NP (NN sensex))...   \n\n                                             premise  \\\n0  stocks nifty future call today: Sensex Weak an...   \n1  At DAVOS, Financial Billionaire Schwartzman, w...   \n2  SENSEX Nifty up, Today stocks nifty future tra...   \n3  At DAVOS, Financial Billionaire Schwartzman, w...   \n4  Sensex and Nifty up, 2 sept Nifty stock market...   \n\n                                          hypothesis  \\\n0  Sensex and Nifty up, 2 sept Nifty stock market...   \n1  At Davos, Wall Street Billionaire Steven Schwa...   \n2  Sensex down 74.58 points, Nifty future tips, T...   \n3  At Davos, Wall Street Billionaire Mr Schwartzf...   \n4  stocks nifty future call today: Sensex Weak an...   \n\n                                     sentence1_parse          genre  \\\n0  (FRAG (NP-TMP (NP (NNS stocks)) (NP (JJ nifty)...  Economic News   \n1  (S (PP (IN At) (NP (NNP DAVOS))) (, ,) (NP (NP...  Economic News   \n2  (FRAG (NP (NP (NP (NNP SENSEX) (NNP Nifty)) (A...  Economic News   \n3  (S (PP (IN At) (NP (NNP DAVOS))) (, ,) (NP (NP...  Economic News   \n4  (NP (NP (NP (NP (NNP Sensex) (CC and) (NNP Nif...  Economic News   \n\n                                      hypothesis_pos  \\\n0  [[NN, CC, JJ, RB, ,, CD, JJ, JJ, NN, NN, NN, N...   \n1  [[IN, NNP, ,, NN, NN, NN, NN, NN, ,, WP, VBZ, ...   \n2  [[NN, IN, CD, NNS, ,, JJ, JJ, NNS, ,, NN, JJ, ...   \n3  [[IN, NNP, ,, NN, NN, NN, NN, NN, ,, WP, VBZ, ...   \n4  [[NNS, JJ, JJ, NN, NN, :, NN, JJ, CC, JJ, JJ, ...   \n\n                                 sentence2_dep_parse          label  \\\n0  [[{'dep': 'ROOT', 'dependent': 1, 'governorGlo...  contradiction   \n1  [[{'dep': 'ROOT', 'dependent': 21, 'governorGl...        neutral   \n2  [[{'dep': 'ROOT', 'dependent': 24, 'governorGl...  contradiction   \n3  [[{'dep': 'ROOT', 'dependent': 21, 'governorGl...     entailment   \n4  [[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...  contradiction   \n\n                                         premise_pos  PairID  sample_index  \n0  [[NNS, JJ, JJ, NN, NN, :, NN, JJ, CC, JJ, JJ, ...       1             0  \n1  [[IN, NNP, ,, JJ, NN, NN, ,, WP, VBD, NN, IN, ...       2             1  \n2  [[NN, JJ, RB, ,, NN, NNS, JJ, JJ, NN, NNS, CC,...       3             2  \n3  [[IN, NNP, ,, JJ, NN, NN, ,, WP, VBD, NN, IN, ...       4             3  \n4  [[NN, CC, JJ, RB, ,, CD, JJ, JJ, NN, NN, NN, N...       5             4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence2_tokens</th>\n      <th>sentence1_tokens</th>\n      <th>sentence1_dep_parse</th>\n      <th>sentence2_syntax_parse</th>\n      <th>sentence1_binary_parse</th>\n      <th>sentence2_parse</th>\n      <th>sentence2_binary_parse</th>\n      <th>sentence1_syntax_parse</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sentence1_parse</th>\n      <th>genre</th>\n      <th>hypothesis_pos</th>\n      <th>sentence2_dep_parse</th>\n      <th>label</th>\n      <th>premise_pos</th>\n      <th>PairID</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[sensex, and, nifty, up, ,, 2, sept, nifty, s...</td>\n      <td>[[stocks, nifty, future, call, today, :, sense...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...</td>\n      <td>[(ROOT\\n  (NP\\n    (NP\\n      (NP (NN sensex))...</td>\n      <td>( (NP-TMP ( ( stocks ) ) ( ( nifty ) ( future ...</td>\n      <td>(NP (NP (NP (NP (NNP Sensex) (CC and) (NNP Nif...</td>\n      <td>( ( ( ( ( Sensex ) ( and ) ( Nifty ) ) ( ( up ...</td>\n      <td>[(ROOT\\n  (FRAG\\n    (NP-TMP\\n      (NP (NNS s...</td>\n      <td>stocks nifty future call today: Sensex Weak an...</td>\n      <td>Sensex and Nifty up, 2 sept Nifty stock market...</td>\n      <td>(FRAG (NP-TMP (NP (NNS stocks)) (NP (JJ nifty)...</td>\n      <td>Economic News</td>\n      <td>[[NN, CC, JJ, RB, ,, CD, JJ, JJ, NN, NN, NN, N...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 1, 'governorGlo...</td>\n      <td>contradiction</td>\n      <td>[[NNS, JJ, JJ, NN, NN, :, NN, JJ, CC, JJ, JJ, ...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[at, davos, ,, wall, street, billionaire, ste...</td>\n      <td>[[at, davos, ,, financial, billionaire, schwar...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 19, 'governorGl...</td>\n      <td>[(ROOT\\n  (S\\n    (PP (IN at)\\n      (NP (NNP ...</td>\n      <td>( ( ( At ) ( ( DAVOS ) ) ) (, , ) ( ( ( Financ...</td>\n      <td>(S (PP (IN At) (NP (NNP Davos))) (, ,) (NP (NP...</td>\n      <td>( ( ( At ) ( ( Davos ) ) ) (, , ) ( ( ( Wall )...</td>\n      <td>[(ROOT\\n  (S\\n    (PP (IN at)\\n      (NP (NNP ...</td>\n      <td>At DAVOS, Financial Billionaire Schwartzman, w...</td>\n      <td>At Davos, Wall Street Billionaire Steven Schwa...</td>\n      <td>(S (PP (IN At) (NP (NNP DAVOS))) (, ,) (NP (NP...</td>\n      <td>Economic News</td>\n      <td>[[IN, NNP, ,, NN, NN, NN, NN, NN, ,, WP, VBZ, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 21, 'governorGl...</td>\n      <td>neutral</td>\n      <td>[[IN, NNP, ,, JJ, NN, NN, ,, WP, VBD, NN, IN, ...</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[sensex, down, 74.58, points, ,, nifty, futur...</td>\n      <td>[[sensex, nifty, up, ,, today, stocks, nifty, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>[(ROOT\\n  (S\\n    (NP\\n      (NP (NN sensex))\\...</td>\n      <td>( ( ( ( ( SENSEX ) ( Nifty ) ) ( ( up ) ) ) (,...</td>\n      <td>(SINV (VP (VB Sensex) (PRT (RP down)) (NP (NP ...</td>\n      <td>( ( ( Sensex ) ( ( down ) ) ( ( ( 74.58 ) ( po...</td>\n      <td>[(ROOT\\n  (FRAG\\n    (NP\\n      (NP\\n        (...</td>\n      <td>SENSEX Nifty up, Today stocks nifty future tra...</td>\n      <td>Sensex down 74.58 points, Nifty future tips, T...</td>\n      <td>(FRAG (NP (NP (NP (NNP SENSEX) (NNP Nifty)) (A...</td>\n      <td>Economic News</td>\n      <td>[[NN, IN, CD, NNS, ,, JJ, JJ, NNS, ,, NN, JJ, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 24, 'governorGl...</td>\n      <td>contradiction</td>\n      <td>[[NN, JJ, RB, ,, NN, NNS, JJ, JJ, NN, NNS, CC,...</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[at, davos, ,, wall, street, billionaire, mr,...</td>\n      <td>[[at, davos, ,, financial, billionaire, schwar...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 19, 'governorGl...</td>\n      <td>[(ROOT\\n  (S\\n    (PP (IN at)\\n      (NP (NNP ...</td>\n      <td>( ( ( At ) ( ( DAVOS ) ) ) (, , ) ( ( ( Financ...</td>\n      <td>(S (PP (IN At) (NP (NNP Davos))) (, ,) (NP (NP...</td>\n      <td>( ( ( At ) ( ( Davos ) ) ) (, , ) ( ( ( Wall )...</td>\n      <td>[(ROOT\\n  (S\\n    (PP (IN at)\\n      (NP (NNP ...</td>\n      <td>At DAVOS, Financial Billionaire Schwartzman, w...</td>\n      <td>At Davos, Wall Street Billionaire Mr Schwartzf...</td>\n      <td>(S (PP (IN At) (NP (NNP DAVOS))) (, ,) (NP (NP...</td>\n      <td>Economic News</td>\n      <td>[[IN, NNP, ,, NN, NN, NN, NN, NN, ,, WP, VBZ, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 21, 'governorGl...</td>\n      <td>entailment</td>\n      <td>[[IN, NNP, ,, JJ, NN, NN, ,, WP, VBD, NN, IN, ...</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[stocks, nifty, future, call, today, :, sense...</td>\n      <td>[[sensex, and, nifty, up, ,, 2, sept, nifty, s...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 1, 'governorGlo...</td>\n      <td>[(ROOT\\n  (FRAG\\n    (NP-TMP\\n      (NP (NNS s...</td>\n      <td>( ( ( ( ( Sensex ) ( and ) ( Nifty ) ) ( ( up ...</td>\n      <td>(FRAG (NP-TMP (NP (NNS stocks)) (NP (JJ nifty)...</td>\n      <td>( (NP-TMP ( ( stocks ) ) ( ( nifty ) ( future ...</td>\n      <td>[(ROOT\\n  (NP\\n    (NP\\n      (NP (NN sensex))...</td>\n      <td>Sensex and Nifty up, 2 sept Nifty stock market...</td>\n      <td>stocks nifty future call today: Sensex Weak an...</td>\n      <td>(NP (NP (NP (NP (NNP Sensex) (CC and) (NNP Nif...</td>\n      <td>Economic News</td>\n      <td>[[NNS, JJ, JJ, NN, NN, :, NN, JJ, CC, JJ, JJ, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...</td>\n      <td>contradiction</td>\n      <td>[[NN, CC, JJ, RB, ,, CD, JJ, JJ, NN, NN, NN, N...</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "label\nentailment       0.584\nneutral          0.340\ncontradiction    0.076\nName: proportion, dtype: float64"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit['label'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice an imbalance in this dataset between the 3 labels, with the contradiction label representing less than 10% of the samples (specifically 7.6%). The entailment label is represents the majority, followed by neutral."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no data on the annotator labels for this dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  stocks nifty future call today: Sensex Weak an...   \n1  At DAVOS, Financial Billionaire Schwartzman, w...   \n2  SENSEX Nifty up, Today stocks nifty future tra...   \n3  At DAVOS, Financial Billionaire Schwartzman, w...   \n4  Sensex and Nifty up, 2 sept Nifty stock market...   \n\n                                          hypothesis          label  \\\n0  Sensex and Nifty up, 2 sept Nifty stock market...  contradiction   \n1  At Davos, Wall Street Billionaire Steven Schwa...        neutral   \n2  Sensex down 74.58 points, Nifty future tips, T...  contradiction   \n3  At Davos, Wall Street Billionaire Mr Schwartzf...     entailment   \n4  stocks nifty future call today: Sensex Weak an...  contradiction   \n\n   premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\n0                28                   30               153   \n1                22                   24               146   \n2                27                   26               153   \n3                22                   24               146   \n4                30                   28               167   \n\n   hypothesis_char_cnt  \n0                  167  \n1                  160  \n2                  158  \n3                  154  \n4                  153  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>stocks nifty future call today: Sensex Weak an...</td>\n      <td>Sensex and Nifty up, 2 sept Nifty stock market...</td>\n      <td>contradiction</td>\n      <td>28</td>\n      <td>30</td>\n      <td>153</td>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>At DAVOS, Financial Billionaire Schwartzman, w...</td>\n      <td>At Davos, Wall Street Billionaire Steven Schwa...</td>\n      <td>neutral</td>\n      <td>22</td>\n      <td>24</td>\n      <td>146</td>\n      <td>160</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SENSEX Nifty up, Today stocks nifty future tra...</td>\n      <td>Sensex down 74.58 points, Nifty future tips, T...</td>\n      <td>contradiction</td>\n      <td>27</td>\n      <td>26</td>\n      <td>153</td>\n      <td>158</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>At DAVOS, Financial Billionaire Schwartzman, w...</td>\n      <td>At Davos, Wall Street Billionaire Mr Schwartzf...</td>\n      <td>entailment</td>\n      <td>22</td>\n      <td>24</td>\n      <td>146</td>\n      <td>154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sensex and Nifty up, 2 sept Nifty stock market...</td>\n      <td>stocks nifty future call today: Sensex Weak an...</td>\n      <td>contradiction</td>\n      <td>30</td>\n      <td>28</td>\n      <td>167</td>\n      <td>153</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_insights(reddit)\n",
    "\n",
    "reddit[[\"premise\", \"hypothesis\", \"label\", \"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\nmean         11.960000            11.460000         69.448000   \nstd           4.833617             5.144537         30.102757   \n\n      hypothesis_char_cnt  \nmean            65.660000  \nstd             31.356488  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>11.960000</td>\n      <td>11.460000</td>\n      <td>69.448000</td>\n      <td>65.660000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4.833617</td>\n      <td>5.144537</td>\n      <td>30.102757</td>\n      <td>31.356488</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[[\"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].aggregate([\"mean\", \"std\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice that the frequency of words and characters is very similar between the premises and hypotheses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check for duplicates in the dataset, at a premise-hypothesis pair level. Do these duplicates have the same label? If not, which is the pair with the correct label?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit.duplicated(subset=[\"premise\", \"hypothesis\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the duplicates have the same label?\n",
    "reddit[reddit.duplicated(subset=[\"premise\", \"hypothesis\", \"label\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the premise and hypothesis to lowercase, to ensure we do a case-insensitive check for duplicates as well\n",
    "reddit[\"premise_lower\"] = reddit[\"premise\"].str.lower()\n",
    "reddit[\"hypothesis_lower\"] = reddit[\"hypothesis\"].str.lower()\n",
    "\n",
    "reddit[reddit.duplicated(subset=[\"premise_lower\", \"hypothesis_lower\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               premise  \\\n54   U.S. economy added 161,000 jobs in October as ...   \n125  Wages Salaries jump by 3.1 percent; highest in...   \n127  U.S. economy off to slow start in 2017 under T...   \n\n                                            hypothesis  sample_index  \n54   U.S. Economy Grew by 161,000 Jobs in October; ...            54  \n125  Wages and salaries jump by 3.1%, highest level...           125  \n127  G.D.P. Report Shows U.S. Economy Off to Slow S...           127  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>54</th>\n      <td>U.S. economy added 161,000 jobs in October as ...</td>\n      <td>U.S. Economy Grew by 161,000 Jobs in October; ...</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>Wages Salaries jump by 3.1 percent; highest in...</td>\n      <td>Wages and salaries jump by 3.1%, highest level...</td>\n      <td>125</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>U.S. economy off to slow start in 2017 under T...</td>\n      <td>G.D.P. Report Shows U.S. Economy Off to Slow S...</td>\n      <td>127</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[reddit.duplicated(subset=[\"premise\", \"hypothesis\"])][[\"premise\", \"hypothesis\", \"sample_index\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These 3 samples should be discarded from the training/testing datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's inspect the frequency of quantities in the dataset premises and hypotheses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_quantities_cnt  hypothesis_quantities_cnt\nmean                1.632000                   1.552000\nstd                 0.811919                   0.811127\nmin                 0.000000                   0.000000\nmax                 4.000000                   6.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_quantities_cnt</th>\n      <th>hypothesis_quantities_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>1.632000</td>\n      <td>1.552000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.811919</td>\n      <td>0.811127</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.000000</td>\n      <td>6.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit[[\"premise_quantities_cnt\", \"hypothesis_quantities_cnt\"]].aggregate([\"mean\", \"std\", \"min\", \"max\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               premise  \\\n33   Based off 1st time unemployment claims, the Ju...   \n34   Based off of 1st unemployment reports the jobs...   \n163     Dow Closes Above 18K for First Time Since July   \n182  Dow closes above 18000 for first time in 9 months   \n208  Home ownership falls to lowest level since the...   \n\n                                            hypothesis  sample_index  \n33   Based off of 1st unemployment reports the jobs...            33  \n34   Based off 1st time unemployment claims, the Ju...            34  \n163  Dow closes above 18000 for first time in 9 months           163  \n182     Dow Closes Above 18K for First Time Since July           182  \n208        Home ownership hits lowest level since 1965           208  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>33</th>\n      <td>Based off 1st time unemployment claims, the Ju...</td>\n      <td>Based off of 1st unemployment reports the jobs...</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Based off of 1st unemployment reports the jobs...</td>\n      <td>Based off 1st time unemployment claims, the Ju...</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>163</th>\n      <td>Dow Closes Above 18K for First Time Since July</td>\n      <td>Dow closes above 18000 for first time in 9 months</td>\n      <td>163</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>Dow closes above 18000 for first time in 9 months</td>\n      <td>Dow Closes Above 18K for First Time Since July</td>\n      <td>182</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>Home ownership falls to lowest level since the...</td>\n      <td>Home ownership hits lowest level since 1965</td>\n      <td>208</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_no_quantities = reddit[(reddit[\"premise_quantities_cnt\"] == 0) | (reddit[\"hypothesis_quantities_cnt\"] == 0)][[\"premise\", \"hypothesis\", \"sample_index\"]]\n",
    "reddit_no_quantities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "reddit_no_quantities.to_excel(\"RedditNLI_no_quantities.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Manual inspection of these samples indicates that they actually contain quantities, so they should not be discarded from the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check if the dataset contains features which are not in all datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['sentence2_tokens', 'sentence1_tokens', 'sentence1_dep_parse',\n       'sentence2_syntax_parse', 'sentence1_binary_parse', 'sentence2_parse',\n       'sentence2_binary_parse', 'sentence1_syntax_parse', 'premise',\n       'hypothesis', 'sentence1_parse', 'genre', 'hypothesis_pos',\n       'sentence2_dep_parse', 'label', 'premise_pos', 'PairID', 'sample_index',\n       'premise_word_cnt', 'hypothesis_word_cnt', 'premise_char_cnt',\n       'hypothesis_char_cnt', 'premise_quantities_cnt',\n       'hypothesis_quantities_cnt', 'premise_lower', 'hypothesis_lower'],\n      dtype='object')"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "genre\nEconomic News    250\nName: count, dtype: int64"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only the 'genre' column is an extra column, let's inspect its values\n",
    "reddit['genre'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RTE dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "                                    sentence2_tokens  \\\n0             [[accardo, composed, 24, caprices, .]]   \n1     [[golinkin, has, written, eighteen, books, .]]   \n2  [[david, golinkin, is, the, author, of, dozen,...   \n3  [[reinsdorf, was, the, chairman, of, the, whit...   \n4  [[the, white, sox, have, won, 24, championship...   \n\n                                    annotator_labels  \\\n0      [neutral, neutral, neutral, neutral, neutral]   \n1      [neutral, neutral, neutral, neutral, neutral]   \n2      [neutral, neutral, neutral, neutral, neutral]   \n3  [entailment, entailment, entailment, entailmen...   \n4      [neutral, neutral, neutral, neutral, neutral]   \n\n                                 sentence1_dep_parse  \\\n0  [[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...   \n1  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...   \n2  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...   \n3  [[{'dep': 'ROOT', 'dependent': 16, 'governorGl...   \n4  [[{'dep': 'ROOT', 'dependent': 16, 'governorGl...   \n\n                              sentence1_binary_parse  \\\n0  In 1956 Accardo won the Geneva Competition and...   \n1  David Golinkin is the editor or author of eigh...   \n2  David Golinkin is single-handedly responsible ...   \n3  During Reinsdorf 's 24 seasons as chairman of ...   \n4  During Reinsdorf 's 24 seasons as chairman of ...   \n\n                              sentence2_binary_parse  \\\n0                     Accardo composed 24 Caprices .   \n1              Golinkin has written eighteen books .   \n2  David Golinkin is the author of dozen of respo...   \n3  Reinsdorf was the chairman of the White Sox fo...   \n4          The White Sox have won 24 championships .   \n\n                              sentence1_syntax_parse  \\\n0  [(ROOT\\n  (S\\n    (PP (IN in)\\n      (NP (CD 1...   \n1  [(ROOT\\n  (S\\n    (NP (NN david) (NN golinkin)...   \n2  [(ROOT\\n  (S\\n    (NP (NN david) (NN golinkin)...   \n3  [(ROOT\\n  (S\\n    (PP (IN during)\\n      (NP\\n...   \n4  [(ROOT\\n  (S\\n    (PP (IN during)\\n      (NP\\n...   \n\n                                             premise  \\\n0  In 1956 Accardo won the Geneva Competition and...   \n1  David Golinkin is the editor or author of eigh...   \n2  David Golinkin is single-handedly responsible ...   \n3  During Reinsdorf 's 24 seasons as chairman of ...   \n4  During Reinsdorf 's 24 seasons as chairman of ...   \n\n                                          hypothesis  \\\n0                     Accardo composed 24 Caprices .   \n1              Golinkin has written eighteen books .   \n2  David Golinkin is the author of dozen of respo...   \n3  Reinsdorf was the chairman of the White Sox fo...   \n4          The White Sox have won 24 championships .   \n\n                              sentence2_syntax_parse genre  \\\n0  [(ROOT\\n  (S\\n    (NP (NN accardo))\\n    (VP (...  news   \n1  [(ROOT\\n  (S\\n    (NP (NN golinkin))\\n    (VP ...  news   \n2  [(ROOT\\n  (S\\n    (NP (NN david) (NN golinkin)...  news   \n3  [(ROOT\\n  (S\\n    (NP (NN reinsdorf))\\n    (VP...  news   \n4  [(ROOT\\n  (S\\n    (NP (DT the) (JJ white) (NN ...  news   \n\n                                      hypothesis_pos  \\\n0                            [[NN, VBN, CD, NNS, .]]   \n1                       [[NN, VBZ, VBN, CD, NNS, .]]   \n2  [[NN, NN, VBZ, DT, NN, IN, NN, IN, NN, IN, DT,...   \n3  [[NN, VBD, DT, NN, IN, DT, JJ, NN, IN, CD, NNS...   \n4               [[DT, JJ, NN, VBP, VBN, CD, NNS, .]]   \n\n                                 sentence2_dep_parse       label  \\\n0  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...     neutral   \n1  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...     neutral   \n2  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...     neutral   \n3  [[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...  entailment   \n4  [[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...     neutral   \n\n                                         premise_pos  \\\n0  [[IN, CD, NN, VBD, DT, NN, NN, CC, IN, CD, VBD...   \n1  [[NN, NN, VBZ, DT, NN, CC, NN, IN, CD, NNS, ,,...   \n2  [[NN, NN, VBZ, RB, JJ, IN, VBG, CC, VBG, NNS, ...   \n3  [[IN, NN, POS, CD, NNS, IN, NN, IN, DT, JJ, NN...   \n4  [[IN, NN, POS, CD, NNS, IN, NN, IN, DT, JJ, NN...   \n\n                                    sentence1_tokens  sample_index  \n0  [[in, 1956, accardo, won, the, geneva, competi...             0  \n1  [[david, golinkin, is, the, editor, or, author...             1  \n2  [[david, golinkin, is, single-handedly, respon...             2  \n3  [[during, reinsdorf, 's, 24, seasons, as, chai...             3  \n4  [[during, reinsdorf, 's, 24, seasons, as, chai...             4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence2_tokens</th>\n      <th>annotator_labels</th>\n      <th>sentence1_dep_parse</th>\n      <th>sentence1_binary_parse</th>\n      <th>sentence2_binary_parse</th>\n      <th>sentence1_syntax_parse</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sentence2_syntax_parse</th>\n      <th>genre</th>\n      <th>hypothesis_pos</th>\n      <th>sentence2_dep_parse</th>\n      <th>label</th>\n      <th>premise_pos</th>\n      <th>sentence1_tokens</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[accardo, composed, 24, caprices, .]]</td>\n      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...</td>\n      <td>In 1956 Accardo won the Geneva Competition and...</td>\n      <td>Accardo composed 24 Caprices .</td>\n      <td>[(ROOT\\n  (S\\n    (PP (IN in)\\n      (NP (CD 1...</td>\n      <td>In 1956 Accardo won the Geneva Competition and...</td>\n      <td>Accardo composed 24 Caprices .</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN accardo))\\n    (VP (...</td>\n      <td>news</td>\n      <td>[[NN, VBN, CD, NNS, .]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>neutral</td>\n      <td>[[IN, CD, NN, VBD, DT, NN, NN, CC, IN, CD, VBD...</td>\n      <td>[[in, 1956, accardo, won, the, geneva, competi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[golinkin, has, written, eighteen, books, .]]</td>\n      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>David Golinkin is the editor or author of eigh...</td>\n      <td>Golinkin has written eighteen books .</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN david) (NN golinkin)...</td>\n      <td>David Golinkin is the editor or author of eigh...</td>\n      <td>Golinkin has written eighteen books .</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN golinkin))\\n    (VP ...</td>\n      <td>news</td>\n      <td>[[NN, VBZ, VBN, CD, NNS, .]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>neutral</td>\n      <td>[[NN, NN, VBZ, DT, NN, CC, NN, IN, CD, NNS, ,,...</td>\n      <td>[[david, golinkin, is, the, editor, or, author...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[david, golinkin, is, the, author, of, dozen,...</td>\n      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>David Golinkin is single-handedly responsible ...</td>\n      <td>David Golinkin is the author of dozen of respo...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN david) (NN golinkin)...</td>\n      <td>David Golinkin is single-handedly responsible ...</td>\n      <td>David Golinkin is the author of dozen of respo...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN david) (NN golinkin)...</td>\n      <td>news</td>\n      <td>[[NN, NN, VBZ, DT, NN, IN, NN, IN, NN, IN, DT,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>neutral</td>\n      <td>[[NN, NN, VBZ, RB, JJ, IN, VBG, CC, VBG, NNS, ...</td>\n      <td>[[david, golinkin, is, single-handedly, respon...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[reinsdorf, was, the, chairman, of, the, whit...</td>\n      <td>[entailment, entailment, entailment, entailmen...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 16, 'governorGl...</td>\n      <td>During Reinsdorf 's 24 seasons as chairman of ...</td>\n      <td>Reinsdorf was the chairman of the White Sox fo...</td>\n      <td>[(ROOT\\n  (S\\n    (PP (IN during)\\n      (NP\\n...</td>\n      <td>During Reinsdorf 's 24 seasons as chairman of ...</td>\n      <td>Reinsdorf was the chairman of the White Sox fo...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN reinsdorf))\\n    (VP...</td>\n      <td>news</td>\n      <td>[[NN, VBD, DT, NN, IN, DT, JJ, NN, IN, CD, NNS...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 4, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[IN, NN, POS, CD, NNS, IN, NN, IN, DT, JJ, NN...</td>\n      <td>[[during, reinsdorf, 's, 24, seasons, as, chai...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[the, white, sox, have, won, 24, championship...</td>\n      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 16, 'governorGl...</td>\n      <td>During Reinsdorf 's 24 seasons as chairman of ...</td>\n      <td>The White Sox have won 24 championships .</td>\n      <td>[(ROOT\\n  (S\\n    (PP (IN during)\\n      (NP\\n...</td>\n      <td>During Reinsdorf 's 24 seasons as chairman of ...</td>\n      <td>The White Sox have won 24 championships .</td>\n      <td>[(ROOT\\n  (S\\n    (NP (DT the) (JJ white) (NN ...</td>\n      <td>news</td>\n      <td>[[DT, JJ, NN, VBP, VBN, CD, NNS, .]]</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 5, 'governorGlo...</td>\n      <td>neutral</td>\n      <td>[[IN, NN, POS, CD, NNS, IN, NN, IN, DT, JJ, NN...</td>\n      <td>[[during, reinsdorf, 's, 24, seasons, as, chai...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "label\nneutral       0.578313\nentailment    0.421687\nName: proportion, dtype: float64"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte['label'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The RTE_Quant dataset is relatively balanced between the 2 labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  In 1956 Accardo won the Geneva Competition and...   \n1  David Golinkin is the editor or author of eigh...   \n2  David Golinkin is single-handedly responsible ...   \n3  During Reinsdorf 's 24 seasons as chairman of ...   \n4  During Reinsdorf 's 24 seasons as chairman of ...   \n\n                                          hypothesis       label  \\\n0                     Accardo composed 24 Caprices .     neutral   \n1              Golinkin has written eighteen books .     neutral   \n2  David Golinkin is the author of dozen of respo...     neutral   \n3  Reinsdorf was the chairman of the White Sox fo...  entailment   \n4          The White Sox have won 24 championships .     neutral   \n\n   premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\n0                52                    5               281   \n1                22                    6               113   \n2                37                   22               239   \n3                31                   12               176   \n4                31                    8               176   \n\n   hypothesis_char_cnt  \n0                   30  \n1                   37  \n2                  123  \n3                   60  \n4                   41  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In 1956 Accardo won the Geneva Competition and...</td>\n      <td>Accardo composed 24 Caprices .</td>\n      <td>neutral</td>\n      <td>52</td>\n      <td>5</td>\n      <td>281</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>David Golinkin is the editor or author of eigh...</td>\n      <td>Golinkin has written eighteen books .</td>\n      <td>neutral</td>\n      <td>22</td>\n      <td>6</td>\n      <td>113</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>David Golinkin is single-handedly responsible ...</td>\n      <td>David Golinkin is the author of dozen of respo...</td>\n      <td>neutral</td>\n      <td>37</td>\n      <td>22</td>\n      <td>239</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>During Reinsdorf 's 24 seasons as chairman of ...</td>\n      <td>Reinsdorf was the chairman of the White Sox fo...</td>\n      <td>entailment</td>\n      <td>31</td>\n      <td>12</td>\n      <td>176</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>During Reinsdorf 's 24 seasons as chairman of ...</td>\n      <td>The White Sox have won 24 championships .</td>\n      <td>neutral</td>\n      <td>31</td>\n      <td>8</td>\n      <td>176</td>\n      <td>41</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_insights(rte)\n",
    "\n",
    "rte[[\"premise\", \"hypothesis\", \"label\", \"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\nmean         32.138554            11.590361        176.753012   \nstd          12.420856             4.819245         69.784728   \n\n      hypothesis_char_cnt  \nmean            61.246988  \nstd             26.422542  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>32.138554</td>\n      <td>11.590361</td>\n      <td>176.753012</td>\n      <td>61.246988</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>12.420856</td>\n      <td>4.819245</td>\n      <td>69.784728</td>\n      <td>26.422542</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte[[\"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].aggregate([\"mean\", \"std\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice that the premises are usually much larger than the hypotheses, around 3 times larger, respectively. Similarly to the NewsNLI dataset, the hypothesis seems to be a shorter, rephrased version of the premise, so a kind of summary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check for duplicates in the dataset, at a premise-hypothesis pair level. Do these duplicates have the same label? If not, which is the pair with the correct label?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte[rte.duplicated(subset=[\"premise\", \"hypothesis\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are the duplicates still duplicates at a label level as well?\n",
    "rte[rte.duplicated(subset=[\"premise\", \"hypothesis\", \"label\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               premise  \\\n110  Phil Mickelson finished a triumphant week in h...   \n\n                                            hypothesis  sample_index  \n110  Mickelson won by five shots last week , the la...           110  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>110</th>\n      <td>Phil Mickelson finished a triumphant week in h...</td>\n      <td>Mickelson won by five shots last week , the la...</td>\n      <td>110</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte[rte.duplicated(subset=[\"premise\", \"hypothesis\"])][[\"premise\", \"hypothesis\", \"sample_index\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This sample should be discarded from the training/testing sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's inspect the frequency of quantities in the dataset premises and hypotheses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_quantities_cnt  hypothesis_quantities_cnt\nmean                1.987952                   1.253012\nstd                 1.190813                   0.727692\nmin                 0.000000                   0.000000\nmax                 6.000000                   5.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_quantities_cnt</th>\n      <th>hypothesis_quantities_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>1.987952</td>\n      <td>1.253012</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.190813</td>\n      <td>0.727692</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>6.000000</td>\n      <td>5.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte[[\"premise_quantities_cnt\", \"hypothesis_quantities_cnt\"]].aggregate([\"mean\", \"std\", \"min\", \"max\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               premise  \\\n2    David Golinkin is single-handedly responsible ...   \n6    Dr. Felix Soto Toro ( born 1967 in Guaynabo , ...   \n21   The 8,568-meter Mt . Kanchenjunga , the third ...   \n38   A federal judge sentenced an apparently stunne...   \n46   Prosecutions tended to be more aggressive and ...   \n47   Even though there is some evidence that suppor...   \n49   A Los Angeles federal court judge Monday impos...   \n69   Due to these effects , a person who has consum...   \n76   Monday , when the hearings begin , the Palesti...   \n88   Israeli security forces seized large amounts o...   \n92   Two car bombs explode near a police station ou...   \n107  Commandos stormed a school Friday in southern ...   \n127  GUS on Friday disposed of its remaining home s...   \n129  Last week , saw the fall of the Dutch right wi...   \n133  Of all the national park lands in the United S...   \n138  It is outstripped only by Denmark , the Nether...   \n141  A seven-member Tibetan mountaineering team con...   \n142  The 10-men team is expected to arrive at the f...   \n149  Police in Rio de Janeiro arrested five men and...   \n150  Stolen Warhol works recovered : Amsterdam poli...   \n156  More than 6,400 migratory birds and other anim...   \n\n                                            hypothesis  sample_index  \n2    David Golinkin is the author of dozen of respo...             2  \n6           Soto Toro invented a 3D measuring system .             6  \n21                  Kanchenjunga is 8586 meters high .            21  \n38               Milken was given a 10-year sentence .            38  \n46   Bilking a large number of people out of millio...            46  \n47   It is predicted that as of 1994 , a referendum...            47  \n49   A Los Angeles federal judge imposed a 15-year ...            49  \n69   Half of road-traffic deaths are caused by alco...            69  \n76   Israelis will demonstrate and a counter-demons...            76  \n88   The forces took millions of shekels in cash fr...            88  \n92   A pair of car bombs explode near government of...            92  \n107  The total number of hostages held in the schoo...           107  \n127                         Wehkamp cost % u20AC390m .           127  \n129  Three parties form a Dutch coalition government .           129  \n133                   The Everglades is 50-mile wide .           133  \n138  12 members of the European Union use the Euro ...           138  \n141                 Kanchenjunga is 8586 meters high .           141  \n142                 Kanchenjunga is 8586 meters high .           142  \n149  Millions of dollars of art were recovered , in...           149  \n150  Millions of dollars of art were recovered , in...           150  \n156  Animals have died by the thousands from drinki...           156  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>David Golinkin is single-handedly responsible ...</td>\n      <td>David Golinkin is the author of dozen of respo...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Dr. Felix Soto Toro ( born 1967 in Guaynabo , ...</td>\n      <td>Soto Toro invented a 3D measuring system .</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>The 8,568-meter Mt . Kanchenjunga , the third ...</td>\n      <td>Kanchenjunga is 8586 meters high .</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>A federal judge sentenced an apparently stunne...</td>\n      <td>Milken was given a 10-year sentence .</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Prosecutions tended to be more aggressive and ...</td>\n      <td>Bilking a large number of people out of millio...</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Even though there is some evidence that suppor...</td>\n      <td>It is predicted that as of 1994 , a referendum...</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>A Los Angeles federal court judge Monday impos...</td>\n      <td>A Los Angeles federal judge imposed a 15-year ...</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>Due to these effects , a person who has consum...</td>\n      <td>Half of road-traffic deaths are caused by alco...</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>Monday , when the hearings begin , the Palesti...</td>\n      <td>Israelis will demonstrate and a counter-demons...</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>Israeli security forces seized large amounts o...</td>\n      <td>The forces took millions of shekels in cash fr...</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>Two car bombs explode near a police station ou...</td>\n      <td>A pair of car bombs explode near government of...</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>Commandos stormed a school Friday in southern ...</td>\n      <td>The total number of hostages held in the schoo...</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>GUS on Friday disposed of its remaining home s...</td>\n      <td>Wehkamp cost % u20AC390m .</td>\n      <td>127</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>Last week , saw the fall of the Dutch right wi...</td>\n      <td>Three parties form a Dutch coalition government .</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>Of all the national park lands in the United S...</td>\n      <td>The Everglades is 50-mile wide .</td>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>It is outstripped only by Denmark , the Nether...</td>\n      <td>12 members of the European Union use the Euro ...</td>\n      <td>138</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>A seven-member Tibetan mountaineering team con...</td>\n      <td>Kanchenjunga is 8586 meters high .</td>\n      <td>141</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>The 10-men team is expected to arrive at the f...</td>\n      <td>Kanchenjunga is 8586 meters high .</td>\n      <td>142</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>Police in Rio de Janeiro arrested five men and...</td>\n      <td>Millions of dollars of art were recovered , in...</td>\n      <td>149</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>Stolen Warhol works recovered : Amsterdam poli...</td>\n      <td>Millions of dollars of art were recovered , in...</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>More than 6,400 migratory birds and other anim...</td>\n      <td>Animals have died by the thousands from drinki...</td>\n      <td>156</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte_no_quantities = rte[(rte[\"premise_quantities_cnt\"] == 0) | (rte[\"hypothesis_quantities_cnt\"] == 0)][[\"premise\", \"hypothesis\", \"sample_index\"]]\n",
    "rte_no_quantities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "# let's inspect these samples manually to decide if any should be discarded\n",
    "rte_no_quantities.to_excel(\"RTE_Quant_no_quantities.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's inspect the data on the annotator labels - for how many samples were there disagreements between annotators?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "annotator_unique_labels\n1    166\nName: count, dtype: int64"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte[\"annotator_unique_labels\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It appears like annotators were never in disagreement over the label of a sample."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check if the dataset contains features which are not in all datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['sentence2_tokens', 'annotator_labels', 'sentence1_dep_parse',\n       'sentence1_binary_parse', 'sentence2_binary_parse',\n       'sentence1_syntax_parse', 'premise', 'hypothesis',\n       'sentence2_syntax_parse', 'genre', 'hypothesis_pos',\n       'sentence2_dep_parse', 'label', 'premise_pos', 'sentence1_tokens',\n       'sample_index', 'premise_word_cnt', 'hypothesis_word_cnt',\n       'premise_char_cnt', 'hypothesis_char_cnt', 'premise_quantities_cnt',\n       'hypothesis_quantities_cnt', 'annotator_unique_labels'],\n      dtype='object')"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "genre\nnews    166\nName: count, dtype: int64"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte['genre'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Besides 'genre', there are no extra columns in this dataset to analyze."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## StressTest dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "                                    sentence2_tokens  \\\n0  [[if, joe, goes, with, her, more, than, 1, yea...   \n1  [[if, joe, goes, with, her, 6, years, old, twi...   \n2  [[if, joe, goes, with, her, less, than, 6, yea...   \n3  [[tim, has, less, than, 750, pounds, of, cemen...   \n4  [[tim, has, 350, pounds, of, cement, in, 100, ...   \n\n                                 sentence1_dep_parse  \\\n0  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...   \n1  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...   \n2  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...   \n3  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...   \n4  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...   \n\n                              sentence1_binary_parse  \\\n0  If Joe goes with her 6 years old twin brothers...   \n1  If Joe goes with her more than 1 years old twi...   \n2  If Joe goes with her 6 years old twin brothers...   \n3  Tim has 350 pounds of cement in 100 , 50 , and...   \n4  Tim has less than 750 pounds of cement in 100 ...   \n\n                              sentence2_binary_parse  \\\n0  If Joe goes with her more than 1 years old twi...   \n1  If Joe goes with her 6 years old twin brothers...   \n2  If Joe goes with her less than 6 years old twi...   \n3  Tim has less than 750 pounds of cement in 100 ...   \n4  Tim has 350 pounds of cement in 100 , 50 , and...   \n\n                              sentence1_syntax_parse  \\\n0  [(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...   \n1  [(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...   \n2  [(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...   \n3  [(ROOT\\n  (S\\n    (NP (NN tim))\\n    (VP (VBZ ...   \n4  [(ROOT\\n  (S\\n    (NP (NN tim))\\n    (VP (VBZ ...   \n\n                                             premise  \\\n0  If Joe goes with her 6 years old twin brothers...   \n1  If Joe goes with her more than 1 years old twi...   \n2  If Joe goes with her 6 years old twin brothers...   \n3  Tim has 350 pounds of cement in 100 , 50 , and...   \n4  Tim has less than 750 pounds of cement in 100 ...   \n\n                                          hypothesis  \\\n0  If Joe goes with her more than 1 years old twi...   \n1  If Joe goes with her 6 years old twin brothers...   \n2  If Joe goes with her less than 6 years old twi...   \n3  Tim has less than 750 pounds of cement in 100 ...   \n4  Tim has 350 pounds of cement in 100 , 50 , and...   \n\n                              sentence2_syntax_parse  \\\n0  [(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...   \n1  [(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...   \n2  [(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...   \n3  [(ROOT\\n  (S\\n    (NP (NN tim))\\n    (VP (VBZ ...   \n4  [(ROOT\\n  (S\\n    (NP (NN tim))\\n    (VP (VBZ ...   \n\n                                      hypothesis_pos  \\\n0  [[IN, NN, VBZ, IN, PRP$, JJR, IN, CD, NNS, JJ,...   \n1  [[IN, NN, VBZ, IN, PRP$, CD, NNS, JJ, NN, NNS,...   \n2  [[IN, NN, VBZ, IN, PRP$, JJR, IN, CD, NNS, JJ,...   \n3  [[NN, VBZ, JJR, IN, CD, NNS, IN, NN, IN, CD, ,...   \n4  [[NN, VBZ, CD, NNS, IN, NN, IN, CD, ,, CD, ,, ...   \n\n                                 sentence2_dep_parse          label  \\\n0  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...     entailment   \n1  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...        neutral   \n2  [[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...  contradiction   \n3  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...     entailment   \n4  [[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...        neutral   \n\n                                         premise_pos  \\\n0  [[IN, NN, VBZ, IN, PRP$, CD, NNS, JJ, NN, NNS,...   \n1  [[IN, NN, VBZ, IN, PRP$, JJR, IN, CD, NNS, JJ,...   \n2  [[IN, NN, VBZ, IN, PRP$, CD, NNS, JJ, NN, NNS,...   \n3  [[NN, VBZ, CD, NNS, IN, NN, IN, CD, ,, CD, ,, ...   \n4  [[NN, VBZ, JJR, IN, CD, NNS, IN, NN, IN, CD, ,...   \n\n                                    sentence1_tokens  sample_index  \n0  [[if, joe, goes, with, her, 6, years, old, twi...             0  \n1  [[if, joe, goes, with, her, more, than, 1, yea...             1  \n2  [[if, joe, goes, with, her, 6, years, old, twi...             2  \n3  [[tim, has, 350, pounds, of, cement, in, 100, ...             3  \n4  [[tim, has, less, than, 750, pounds, of, cemen...             4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence2_tokens</th>\n      <th>sentence1_dep_parse</th>\n      <th>sentence1_binary_parse</th>\n      <th>sentence2_binary_parse</th>\n      <th>sentence1_syntax_parse</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sentence2_syntax_parse</th>\n      <th>hypothesis_pos</th>\n      <th>sentence2_dep_parse</th>\n      <th>label</th>\n      <th>premise_pos</th>\n      <th>sentence1_tokens</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[if, joe, goes, with, her, more, than, 1, yea...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>If Joe goes with her more than 1 years old twi...</td>\n      <td>[(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...</td>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>If Joe goes with her more than 1 years old twi...</td>\n      <td>[(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...</td>\n      <td>[[IN, NN, VBZ, IN, PRP$, JJR, IN, CD, NNS, JJ,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[IN, NN, VBZ, IN, PRP$, CD, NNS, JJ, NN, NNS,...</td>\n      <td>[[if, joe, goes, with, her, 6, years, old, twi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[if, joe, goes, with, her, 6, years, old, twi...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>If Joe goes with her more than 1 years old twi...</td>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>[(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...</td>\n      <td>If Joe goes with her more than 1 years old twi...</td>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>[(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...</td>\n      <td>[[IN, NN, VBZ, IN, PRP$, CD, NNS, JJ, NN, NNS,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>neutral</td>\n      <td>[[IN, NN, VBZ, IN, PRP$, JJR, IN, CD, NNS, JJ,...</td>\n      <td>[[if, joe, goes, with, her, more, than, 1, yea...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[if, joe, goes, with, her, less, than, 6, yea...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>If Joe goes with her less than 6 years old twi...</td>\n      <td>[(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...</td>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>If Joe goes with her less than 6 years old twi...</td>\n      <td>[(ROOT\\n  (SBAR (IN if)\\n    (S\\n      (S\\n   ...</td>\n      <td>[[IN, NN, VBZ, IN, PRP$, JJR, IN, CD, NNS, JJ,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 3, 'governorGlo...</td>\n      <td>contradiction</td>\n      <td>[[IN, NN, VBZ, IN, PRP$, CD, NNS, JJ, NN, NNS,...</td>\n      <td>[[if, joe, goes, with, her, 6, years, old, twi...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[tim, has, less, than, 750, pounds, of, cemen...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>Tim has 350 pounds of cement in 100 , 50 , and...</td>\n      <td>Tim has less than 750 pounds of cement in 100 ...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN tim))\\n    (VP (VBZ ...</td>\n      <td>Tim has 350 pounds of cement in 100 , 50 , and...</td>\n      <td>Tim has less than 750 pounds of cement in 100 ...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN tim))\\n    (VP (VBZ ...</td>\n      <td>[[NN, VBZ, JJR, IN, CD, NNS, IN, NN, IN, CD, ,...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>entailment</td>\n      <td>[[NN, VBZ, CD, NNS, IN, NN, IN, CD, ,, CD, ,, ...</td>\n      <td>[[tim, has, 350, pounds, of, cement, in, 100, ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[tim, has, 350, pounds, of, cement, in, 100, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>Tim has less than 750 pounds of cement in 100 ...</td>\n      <td>Tim has 350 pounds of cement in 100 , 50 , and...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN tim))\\n    (VP (VBZ ...</td>\n      <td>Tim has less than 750 pounds of cement in 100 ...</td>\n      <td>Tim has 350 pounds of cement in 100 , 50 , and...</td>\n      <td>[(ROOT\\n  (S\\n    (NP (NN tim))\\n    (VP (VBZ ...</td>\n      <td>[[NN, VBZ, CD, NNS, IN, NN, IN, CD, ,, CD, ,, ...</td>\n      <td>[[{'dep': 'ROOT', 'dependent': 2, 'governorGlo...</td>\n      <td>neutral</td>\n      <td>[[NN, VBZ, JJR, IN, CD, NNS, IN, NN, IN, CD, ,...</td>\n      <td>[[tim, has, less, than, 750, pounds, of, cemen...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "label\nentailment       0.333333\nneutral          0.333333\ncontradiction    0.333333\nName: proportion, dtype: float64"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress['label'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "StressTest is balanced with respect to the distribution of labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no data on the annotator labels for this dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  If Joe goes with her 6 years old twin brothers...   \n1  If Joe goes with her more than 1 years old twi...   \n2  If Joe goes with her 6 years old twin brothers...   \n3  Tim has 350 pounds of cement in 100 , 50 , and...   \n4  Tim has less than 750 pounds of cement in 100 ...   \n\n                                          hypothesis          label  \\\n0  If Joe goes with her more than 1 years old twi...     entailment   \n1  If Joe goes with her 6 years old twin brothers...        neutral   \n2  If Joe goes with her less than 6 years old twi...  contradiction   \n3  Tim has less than 750 pounds of cement in 100 ...     entailment   \n4  Tim has 350 pounds of cement in 100 , 50 , and...        neutral   \n\n   premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\n0                19                   21                84   \n1                21                   19                94   \n2                19                   21                84   \n3                15                   17                60   \n4                17                   15                70   \n\n   hypothesis_char_cnt  \n0                   94  \n1                   84  \n2                   94  \n3                   70  \n4                   60  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>If Joe goes with her more than 1 years old twi...</td>\n      <td>entailment</td>\n      <td>19</td>\n      <td>21</td>\n      <td>84</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>If Joe goes with her more than 1 years old twi...</td>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>neutral</td>\n      <td>21</td>\n      <td>19</td>\n      <td>94</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If Joe goes with her 6 years old twin brothers...</td>\n      <td>If Joe goes with her less than 6 years old twi...</td>\n      <td>contradiction</td>\n      <td>19</td>\n      <td>21</td>\n      <td>84</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tim has 350 pounds of cement in 100 , 50 , and...</td>\n      <td>Tim has less than 750 pounds of cement in 100 ...</td>\n      <td>entailment</td>\n      <td>15</td>\n      <td>17</td>\n      <td>60</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Tim has less than 750 pounds of cement in 100 ...</td>\n      <td>Tim has 350 pounds of cement in 100 , 50 , and...</td>\n      <td>neutral</td>\n      <td>17</td>\n      <td>15</td>\n      <td>70</td>\n      <td>60</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_insights(stress)\n",
    "\n",
    "stress[[\"premise\", \"hypothesis\", \"label\", \"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_word_cnt  hypothesis_word_cnt  premise_char_cnt  \\\nmean         20.905082            21.247367         95.760269   \nstd           9.979773            10.002253         45.704475   \n\n      hypothesis_char_cnt  \nmean            97.432464  \nstd             45.769032  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_word_cnt</th>\n      <th>hypothesis_word_cnt</th>\n      <th>premise_char_cnt</th>\n      <th>hypothesis_char_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>20.905082</td>\n      <td>21.247367</td>\n      <td>95.760269</td>\n      <td>97.432464</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9.979773</td>\n      <td>10.002253</td>\n      <td>45.704475</td>\n      <td>45.769032</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress[[\"premise_word_cnt\", \"hypothesis_word_cnt\", \"premise_char_cnt\", \"hypothesis_char_cnt\"]].aggregate([\"mean\", \"std\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It looks like the premises and hypotheses are of almost equal length in this dataset. By inspecting some pairs, it appears that the difference between the premises and hypotheses in this dataset is a change of the quantity and/or the addition or removal of a quantifier (i.e. either the premise gives and estimate of a quantity and the hypothesis gives a fixed value or the other way around)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check for duplicates in the dataset, at a premise-hypothesis pair level. Do these duplicates have the same label? If not, which is the pair with the correct label?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "643"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress[stress.duplicated(subset=[\"premise\", \"hypothesis\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "649"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the premise and hypothesis to lowercase, to ensure we do a case-insensitive check for duplicates as well\n",
    "stress[\"premise_lower\"] = stress[\"premise\"].str.lower()\n",
    "stress[\"hypothesis_lower\"] = stress[\"hypothesis\"].str.lower()\n",
    "\n",
    "stress[stress.duplicated(subset=[\"premise_lower\", \"hypothesis_lower\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like there is a significant number of duplicates in this dataset, and case-sensitivity must be considered, as it discovers an extra 6 duplicates. let's check if these duplicates have different labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "649"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress[stress.duplicated(subset=[\"premise_lower\", \"hypothesis_lower\", \"label\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "0.08543970510795156"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what % of the total samples are duplicates that must be dropped?\n",
    "649 / stress.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 649 duplicates will be discarded from the final training / testing sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's inspect the frequency of quantities in the dataset premises and hypotheses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "      premise_quantities_cnt  hypothesis_quantities_cnt\nmean                2.098736                   2.100316\nstd                 1.275481                   1.274945\nmin                 0.000000                   0.000000\nmax                 9.000000                   9.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise_quantities_cnt</th>\n      <th>hypothesis_quantities_cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>2.098736</td>\n      <td>2.100316</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.275481</td>\n      <td>1.274945</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.000000</td>\n      <td>9.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress[[\"premise_quantities_cnt\", \"hypothesis_quantities_cnt\"]].aggregate([\"mean\", \"std\", \"min\", \"max\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        premise  \\\n1068            James took a 3 - hour bike ride   \n1069  James took a less than 4 - hour bike ride   \n1070            James took a 3 - hour bike ride   \n2343            James took a 3 - hour bike ride   \n2344  James took a less than 8 - hour bike ride   \n2345            James took a 3 - hour bike ride   \n3051            James took a 3 - hour bike ride   \n3052  James took a more than 1 - hour bike ride   \n3053            James took a 3 - hour bike ride   \n5097             Jack took a 3 - hour bike ride   \n5098   Jack took a less than 7 - hour bike ride   \n5099             Jack took a 3 - hour bike ride   \n5154            James took a 3 - hour bike ride   \n5155  James took a less than 7 - hour bike ride   \n5156            James took a 3 - hour bike ride   \n5943            James took a 3 - hour bike ride   \n5944  James took a more than 2 - hour bike ride   \n5945            James took a 3 - hour bike ride   \n7071            James took a 3 - hour bike ride   \n7072  James took a more than 1 - hour bike ride   \n7073            James took a 3 - hour bike ride   \n\n                                     hypothesis  sample_index  \n1068  James took a less than 4 - hour bike ride          1068  \n1069            James took a 3 - hour bike ride          1069  \n1070            James took a 1 - hour bike ride          1070  \n2343  James took a less than 8 - hour bike ride          2343  \n2344            James took a 3 - hour bike ride          2344  \n2345  James took a more than 3 - hour bike ride          2345  \n3051  James took a more than 1 - hour bike ride          3051  \n3052            James took a 3 - hour bike ride          3052  \n3053  James took a more than 3 - hour bike ride          3053  \n5097   Jack took a less than 7 - hour bike ride          5097  \n5098             Jack took a 3 - hour bike ride          5098  \n5099             Jack took a 2 - hour bike ride          5099  \n5154  James took a less than 7 - hour bike ride          5154  \n5155            James took a 3 - hour bike ride          5155  \n5156            James took a 8 - hour bike ride          5156  \n5943  James took a more than 2 - hour bike ride          5943  \n5944            James took a 3 - hour bike ride          5944  \n5945  James took a less than 3 - hour bike ride          5945  \n7071  James took a more than 1 - hour bike ride          7071  \n7072            James took a 3 - hour bike ride          7072  \n7073            James took a 8 - hour bike ride          7073  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>sample_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1068</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a less than 4 - hour bike ride</td>\n      <td>1068</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>James took a less than 4 - hour bike ride</td>\n      <td>James took a 3 - hour bike ride</td>\n      <td>1069</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a 1 - hour bike ride</td>\n      <td>1070</td>\n    </tr>\n    <tr>\n      <th>2343</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a less than 8 - hour bike ride</td>\n      <td>2343</td>\n    </tr>\n    <tr>\n      <th>2344</th>\n      <td>James took a less than 8 - hour bike ride</td>\n      <td>James took a 3 - hour bike ride</td>\n      <td>2344</td>\n    </tr>\n    <tr>\n      <th>2345</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a more than 3 - hour bike ride</td>\n      <td>2345</td>\n    </tr>\n    <tr>\n      <th>3051</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a more than 1 - hour bike ride</td>\n      <td>3051</td>\n    </tr>\n    <tr>\n      <th>3052</th>\n      <td>James took a more than 1 - hour bike ride</td>\n      <td>James took a 3 - hour bike ride</td>\n      <td>3052</td>\n    </tr>\n    <tr>\n      <th>3053</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a more than 3 - hour bike ride</td>\n      <td>3053</td>\n    </tr>\n    <tr>\n      <th>5097</th>\n      <td>Jack took a 3 - hour bike ride</td>\n      <td>Jack took a less than 7 - hour bike ride</td>\n      <td>5097</td>\n    </tr>\n    <tr>\n      <th>5098</th>\n      <td>Jack took a less than 7 - hour bike ride</td>\n      <td>Jack took a 3 - hour bike ride</td>\n      <td>5098</td>\n    </tr>\n    <tr>\n      <th>5099</th>\n      <td>Jack took a 3 - hour bike ride</td>\n      <td>Jack took a 2 - hour bike ride</td>\n      <td>5099</td>\n    </tr>\n    <tr>\n      <th>5154</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a less than 7 - hour bike ride</td>\n      <td>5154</td>\n    </tr>\n    <tr>\n      <th>5155</th>\n      <td>James took a less than 7 - hour bike ride</td>\n      <td>James took a 3 - hour bike ride</td>\n      <td>5155</td>\n    </tr>\n    <tr>\n      <th>5156</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a 8 - hour bike ride</td>\n      <td>5156</td>\n    </tr>\n    <tr>\n      <th>5943</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a more than 2 - hour bike ride</td>\n      <td>5943</td>\n    </tr>\n    <tr>\n      <th>5944</th>\n      <td>James took a more than 2 - hour bike ride</td>\n      <td>James took a 3 - hour bike ride</td>\n      <td>5944</td>\n    </tr>\n    <tr>\n      <th>5945</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a less than 3 - hour bike ride</td>\n      <td>5945</td>\n    </tr>\n    <tr>\n      <th>7071</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a more than 1 - hour bike ride</td>\n      <td>7071</td>\n    </tr>\n    <tr>\n      <th>7072</th>\n      <td>James took a more than 1 - hour bike ride</td>\n      <td>James took a 3 - hour bike ride</td>\n      <td>7072</td>\n    </tr>\n    <tr>\n      <th>7073</th>\n      <td>James took a 3 - hour bike ride</td>\n      <td>James took a 8 - hour bike ride</td>\n      <td>7073</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress_no_quantities = stress[(stress[\"premise_quantities_cnt\"] == 0) | (stress[\"hypothesis_quantities_cnt\"] == 0)][[\"premise\", \"hypothesis\", \"sample_index\"]]\n",
    "\n",
    "stress_no_quantities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "stress_no_quantities.to_excel(\"StressTest_no_quantities.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Manual inspection of these samples indicates that they actually contain quantities and thus should not be discarded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Let's check if the dataset contains features which are not in all datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stress.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are no extra columns to be analyzed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's generate word clouds of the premises and hypotheses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_word_cloud(text):\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "    background_color='white',\n",
    "    collocations=False,\n",
    "    max_words=20).generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RTE_Quant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "premise_list = list(rte[\"premise\"].values)\n",
    "hypothesis_list = list(rte[\"hypothesis\"].values)\n",
    "premise, hypothesis = \" \".join(premise_list), \" \".join(hypothesis_list)\n",
    "# Create and generate a word cloud image:\n",
    "generate_word_cloud(premise)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_word_cloud(hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the most frequent unigrams in this dataset, RTE_Quant seems to be based on sentences extracted from news articles. We notice that 2 and 3 are very frequent quantities, alongside the \"million\" word."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RedditNLI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "premise_list = list(reddit[\"premise\"].values)\n",
    "hypothesis_list = list(reddit[\"hypothesis\"].values)\n",
    "premise, hypothesis = \" \".join(premise_list), \" \".join(hypothesis_list)\n",
    "# Create and generate a word cloud image:\n",
    "generate_word_cloud(premise)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_word_cloud(hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The RedditNLI dataset, containing sentences extracted from financial/economic headlines on Reddit, has among the most frequent unigrams words related to the financial sector. We notice that there are no quantities among the most frequent unigrams, in contrast with other datasets. This may be because in finance, especially when we talk about stock prices, the number are usually very specific."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NewsNLI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "premise_list = list(news[\"premise\"].values)\n",
    "hypothesis_list = list(news[\"hypothesis\"].values)\n",
    "premise, hypothesis = \" \".join(premise_list), \" \".join(hypothesis_list)\n",
    "# Create and generate a word cloud image:\n",
    "generate_word_cloud(premise)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_word_cloud(hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the NewsNLI dataset, we observe a combination of common quantities (one, two, three four, million) and words likely to be used in a news article (people, police, state, say, said, killed etc)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AWPNLI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "premise_list = list(awp[\"premise\"].values)\n",
    "hypothesis_list = list(awp[\"hypothesis\"].values)\n",
    "premise, hypothesis = \" \".join(premise_list), \" \".join(hypothesis_list)\n",
    "# Create and generate a word cloud image:\n",
    "generate_word_cloud(premise)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_word_cloud(hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As AWPNLI is a dataset based on Math word problems, the most frequent words inside it are nouns usuallu used as entities in this type of exercises (apples, book, orange). We also observe words which indicate operations like addition or subtraction (left, needed, gave, picked, bought)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### StressTest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "premise_list = list(stress[\"premise\"].values)\n",
    "hypothesis_list = list(stress[\"hypothesis\"].values)\n",
    "premise, hypothesis = \" \".join(premise_list), \" \".join(hypothesis_list)\n",
    "# Create and generate a word cloud image:\n",
    "generate_word_cloud(premise)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_word_cloud(hypothesis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although the StressTest is also based on Math word problems, we notice a discrepancy between the word cloud for this set and those for AWPNLI. The StressTest seems to focus on problems with topics such as time (hour, day, age) and distance (mile, city, speed)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's take a look at what type of textual quantifiers are in the datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of most common quantifier (this list is not exhaustive)\n",
    "quantifiers = [\"at least\", \"not less than\", \"no less than\", \"minimum of\", \"equal to or greater than\", \"no fewer than\", \"not fewer than\", \"not below\", \"at most\", \"no more than\", \"not more than\", \"more than\", \"maximum of\", \"equal to or less than\", \"no greater than\", \"not above\", \"less than or equal to\", \"not exceeding\", \"up to\",\n",
    "\"greater than or equal to\", \"not fewer than\", \"down to\", \"less than\", \"below\", \"under\", \"lower than\", \"above\", \"over\", \"greater than\", \"as high as\", \"as little as\", \"exceeding\", \"around\", \"approximately\", \"roughly\", \"about\", \"near\", \"close to\"]\n",
    "\n",
    "\n",
    "def count_lookup_phrases(sentences):\n",
    "    total_cnt, premise_count = 0, 0\n",
    "    phrases_found = set()\n",
    "    for phrase in quantifiers:\n",
    "        for premise in sentences:\n",
    "            premise_counted = False\n",
    "            if phrase in premise:\n",
    "                phrases_found.add(phrase)\n",
    "                total_cnt += 1\n",
    "                if not premise_counted:\n",
    "                    premise_count += 1\n",
    "    return total_cnt, premise_count, phrases_found"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AWPNLI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(awp[\"premise\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(awp[\"hypothesis\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only few of the AWPNLI samples have quantifiers. The focus of this dataset is on the model understanding it has to do calculations based on the quantities in the premise, so he can infer a quantity that must be compared to the one in the hypothesis. Moreover, given the synthetic source of the dataset (math word problems), the lack of diversity in the user quantifiers is also understandable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RedditNLI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(reddit[\"premise\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(reddit[\"hypothesis\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also for RedditNLI, a small fraction of the samples contain quantifiers. The focus in this dataset is on direct comparisons of the quantities and understanding if the quantities in the hypothesis are related to those in the premise and if they can be inferred or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NewsNLI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(news[\"premise\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(news[\"hypothesis\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The NewsNLI dataset is more abundant in quantifiers. Given the nature of this dataset, namely news articles, the presence of quantifiers and their diversity is expected, since they play a key role in highlighting ideas and summarizing information, as well as making information easier to understand and/or remember (i.e. think of reporting the number \"2473\" compared to \"at least 2400\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RTE_Quant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(rte[\"premise\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(rte[\"hypothesis\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset contains a relatively high ratio of samples with quantifiers (at least in the premise). Given the natural language source of this dataset, the presence of quantifiers is expected."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### StressTest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(stress[\"premise\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(count_lookup_phrases(list(stress[\"hypothesis\"].values)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset contains a large fraction of samples with quantifiers. Given that this set is obtained from algebra word problems, the use of quantifiers and their diversity makes sense. The used quantifiers are basic ones, often encountered in math sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finally, let's identify the baselines for each dataset, as well as a baseline for EQUATE as a whole"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets = [rte, news, reddit, awp, stress]\n",
    "dataset_names = [\"RTE_Quant\", \"NewsNLI\", \"RedditNLI\", \"AWPNLI\", \"StressTest\"]\n",
    "\n",
    "for dataset_df, dataset_name in zip(datasets, dataset_names):\n",
    "    print(f\"###############\\n{dataset_name}\")\n",
    "    label_frequency = dataset_df['label'].value_counts(normalize=True).reset_index()  # find fraction of samples assigned to each label\n",
    "    baseline_ratio = label_frequency['proportion'].max()\n",
    "    label = label_frequency[label_frequency['proportion'] == baseline_ratio].iloc[0][\"label\"]\n",
    "    print(f\"Baseline: {round(baseline_ratio, 4)} (label: {label})\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "equate_df = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets:\n",
    "    equate_df = pd.concat([equate_df, dataset], ignore_index=True)\n",
    "\n",
    "print(equate_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_frequency = equate_df['label'].value_counts(normalize=True).reset_index()  # find fraction of samples assigned to each label\n",
    "baseline_ratio = label_frequency['proportion'].max()\n",
    "label = label_frequency[label_frequency['proportion'] == baseline_ratio].iloc[0][\"label\"]\n",
    "print(f\"Baseline: {round(baseline_ratio, 4)} (label: {label})\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
