{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ioanamazilu/PycharmProjects/quant_nli/data/equate/02_post_discarding\n"
     ]
    }
   ],
   "source": [
    "root_path = os.path.dirname(os.path.dirname(os.getcwd()))  # go 2 back, on the same level with the \"data\" directory\n",
    "\n",
    "datasets_path = os.path.join(root_path, \"data\", \"equate\", \"02_post_discarding\")\n",
    "print(datasets_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def add_dot_to_sentence_end(sentence: str):\n",
    "    \"\"\"Add a dot to the sentence end if it does not end in one of the specified chars.\"\"\"\n",
    "    check_chars = [\".\", \"!\", \"?\", \")\", \":\", \"'\", \",\"]\n",
    "    if sentence[-1] not in check_chars:\n",
    "        return f\"{sentence.strip()}.\"\n",
    "    return sentence\n",
    "\n",
    "def clean_sentence(sentence: str):\n",
    "    return re.sub(\"\\s+\", \" \", sentence).strip().replace(\" , \", \", \").replace(\" ' \", \" '\").replace(\" ` \", \" `\").replace(\" / \", \"/\").replace(\" - \", \"-\").replace(\" ( \", \" (\").replace(\" ) \", \") \").replace(\" : \", \":\").replace(\" % \", \"% \").replace(\" ?\", \"?\").replace(\" ’ \", \"’\").replace(\" '\", \"'\").replace(\" .\", \".\").replace(\"$ \", \"$\").replace(\" )\", \")\").replace(\"( \", \"(\").replace(\"`` \", \"'' \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWPNLI\n",
      "NewsNLI\n",
      "RedditNLI\n",
      "RTE_Quant\n",
      "StressTest\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(root_path, \"data\", \"equate\", \"03_cleaned\"), exist_ok=True)\n",
    "for dataset in [\"AWPNLI\", \"NewsNLI\", \"RedditNLI\", \"RTE_Quant\", \"StressTest\"]:\n",
    "    print(dataset)\n",
    "    df = pd.read_csv(os.path.join(datasets_path, f\"{dataset}.csv\"))\n",
    "    df[\"premise\"] = df[\"premise\"].apply(lambda premise: clean_sentence(premise))\n",
    "    df[\"hypothesis\"] = df[\"hypothesis\"].apply(lambda hypothesis: clean_sentence(hypothesis))\n",
    "    df[\"premise\"] = df[\"premise\"].apply(lambda premise: add_dot_to_sentence_end(premise))\n",
    "    df[\"hypothesis\"] = df[\"hypothesis\"].apply(lambda hypothesis: add_dot_to_sentence_end(hypothesis))\n",
    "    df[\"label\"] = df[\"label\"].str.lower()  # ensure case consistency\n",
    "\n",
    "    df.to_csv(os.path.join(root_path, \"data\", \"equate\", \"03_cleaned\", f\"{dataset}.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply the same cleaning functions to the LILA datasets, so we can match our samples with theirs on the premise-hypothesis subset.\n",
    "\n",
    "!!!! First, run the code in SECTION 1 of the \"train_val_test_split.ipynb\" file.\n",
    "Note: The deduplication for LILA is done in the above notebook, no need to run step 01 of the preprocessing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ioanamazilu/PycharmProjects/quant_nli/data/lila\n"
     ]
    }
   ],
   "source": [
    "datasets_path = os.path.join(root_path, \"data\", \"lila\")\n",
    "os.makedirs(os.path.join(root_path, \"data\", \"lila\", \"03_cleaned\"), exist_ok=True)\n",
    "print(datasets_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lila_test.csv\n",
      "lila_val.csv\n",
      "lila_train.csv\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\"lila_test.csv\", \"lila_val.csv\", \"lila_train.csv\"]:\n",
    "    print(dataset)\n",
    "    df = pd.read_csv(os.path.join(datasets_path, dataset))\n",
    "    df[\"premise\"] = df[\"premise\"].apply(lambda premise: clean_sentence(premise))\n",
    "    df[\"hypothesis\"] = df[\"hypothesis\"].apply(lambda hypothesis: clean_sentence(hypothesis))\n",
    "    df[\"premise\"] = df[\"premise\"].apply(lambda premise: add_dot_to_sentence_end(premise))\n",
    "    df[\"hypothesis\"] = df[\"hypothesis\"].apply(lambda hypothesis: add_dot_to_sentence_end(hypothesis))\n",
    "    df[\"label\"] = df[\"label\"].str.lower()  # ensure case consistency\n",
    "\n",
    "    df.to_csv(os.path.join(root_path, \"data\", \"lila\", \"03_cleaned\", dataset), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
