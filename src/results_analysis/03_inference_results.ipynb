{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "from typing import Optional\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [],
   "source": [
    "results_overview = dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [],
   "source": [
    "root_path = os.path.dirname(os.path.dirname(os.getcwd()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [],
   "source": [
    "## INFERENCE LLAMA2 - 7B\n",
    "# model_name = \"llama-2-chat\"\n",
    "# model_size = \"7B\"\n",
    "# model_display_name = \"Llama-2 7B\"\n",
    "## base model\n",
    "# job_id = \"5817639\"\n",
    "## finetuned\n",
    "# job_id = \"5820349_finetuned\"\n",
    "## fintuned with balanced dataset\n",
    "# job_id = \"5836402_finetuned\"\n",
    "## inputs only\n",
    "# job_id = \"6090550_finetuned\" ## (duplicate of 6153680)\n",
    "# job_id = \"6156654_finetuned\" ## different hyperparams\n",
    "# job_id = \"6220572_finetuned\"  ## (new, prompt with instructions)\n",
    "\n",
    "## INFERENCE LLAMA2 - 13B\n",
    "# model_name = \"llama-2-chat\"\n",
    "# model_size = \"13B\"\n",
    "# model_display_name = \"Llama-2 13B\"\n",
    "## base model\n",
    "# job_id = \"5822467\"\n",
    "## finetuned\n",
    "# job_id = \"5830782_finetuned\"\n",
    "\n",
    "## INFERENCE CODELLAMA - PYTHON 7B\n",
    "model_name = \"codellama-python\"\n",
    "model_size = \"7B\"\n",
    "model_display_name = \"CodeLlama-Python 7B\"\n",
    "## base model\n",
    "# job_id = \"5852469\"\n",
    "## finetuned\n",
    "job_id = \"5897597_finetuned\"\n",
    "\n",
    "## INFERENCE CODELLAMA - PYTHON 13B\n",
    "# model_name = \"codellama-python\"\n",
    "# model_size = \"13B\"\n",
    "# model_display_name = \"CodeLlama-Python 13B\"\n",
    "## base model\n",
    "# job_id = \"5919626\"\n",
    "## finetuned\n",
    "# job_id = \"6127255_finetuned\"\n",
    "## finetuned with diff LR and batch size\n",
    "# job_id = \"6138639_finetuned\"\n",
    "\n",
    "## INFERENCE CODELLAMA - INSTRUCT 7B\n",
    "# model_name = \"codellama-instruct\"\n",
    "# model_size = \"7B\"\n",
    "# model_display_name = \"CodeLlama-Instruct 7B\"\n",
    "# # base model\n",
    "# job_id = \"5858736\"\n",
    "## finetuned\n",
    "# job_id = \"5892669_finetuned\"\n",
    "## inputs only\n",
    "# job_id = \"6153680_finetuned\"  # incorrect prompt\n",
    "# job_id = \"6217554_finetuned\"\n",
    "# job_id = \"\"\n",
    "\n",
    "## INFERENCE CODELLAMA - INSTRUCT 13B\n",
    "# model_name = \"codellama-instruct\"\n",
    "# model_size = \"13B\"\n",
    "# model_display_name = \"CodeLlama-Instruct 13B\"\n",
    "# # base model\n",
    "# job_id = \"5875184\"\n",
    "# finetuned\n",
    "# job_id = \"6001211_finetuned\"\n",
    "# balanced\n",
    "# job_id = \"6361334_finetuned\"\n",
    "\n",
    "## INFERENCE LLAMA-3 8B\n",
    "# model_name = \"llama3\"\n",
    "# model_size = \"8B\"\n",
    "# # base model\n",
    "# job_id = \"6013398\"\n",
    "\n",
    "results_path = os.path.join(root_path, \"results_inference\", model_name, model_size, job_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1736\n",
      "Index(['sample_index', 'completion', 'prompt', 'source', 'input_prompt',\n",
      "       'inferred_completion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(results_path, \"evaluation_new\"), exist_ok=True)\n",
    "try:\n",
    "    test_set = pd.read_csv(os.path.join(results_path, \"test.csv\"))\n",
    "except FileNotFoundError:\n",
    "    test_set = pd.DataFrame()\n",
    "    for i in [1, 2, 3, 4]:\n",
    "        try:\n",
    "            test_split = pd.read_csv(os.path.join(results_path, f\"test_{i}.csv\"))\n",
    "            test_set = pd.concat([test_set, test_split], ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    test_set.rename(columns={\"inferred_script\": \"inferred_completion\"}, inplace=True)\n",
    "print(test_set.shape[0])\n",
    "print(test_set.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "outputs": [],
   "source": [
    "test_set.rename(columns={\"inferred_script\": \"inferred_completion\"}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[test_set.duplicated(subset=[\"sample_index\", \"source\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [
    {
     "data": {
      "text/plain": "   sample_index                                         completion  \\\n0           386  ```python\\npicked_pears_keith = 47.0\\npicked_p...   \n1           140  ```python\\ninitial_students_premise = 10.0\\nst...   \n2           199  ```python\\nrows_premise = 27.0\\nchairs_per_row...   \n3           321  ```python\\nchildren_premise = 4.0\\npencils_per...   \n4           633  ```python\\ncandy_premise = 3409.0\\neggs_premis...   \n\n                                              prompt  source  \\\n0  ### Instruction:\\n\\nYou must write Python code...  awpnli   \n1  ### Instruction:\\n\\nYou must write Python code...  awpnli   \n2  ### Instruction:\\n\\nYou must write Python code...  awpnli   \n3  ### Instruction:\\n\\nYou must write Python code...  awpnli   \n4  ### Instruction:\\n\\nYou must write Python code...  awpnli   \n\n                                        input_prompt  \\\n0  <s>[INST]\\n<<SYS>>\\nGive a response suitable t...   \n1  <s>[INST]\\n<<SYS>>\\nGive a response suitable t...   \n2  <s>[INST]\\n<<SYS>>\\nGive a response suitable t...   \n3  <s>[INST]\\n<<SYS>>\\nGive a response suitable t...   \n4  <s>[INST]\\n<<SYS>>\\nGive a response suitable t...   \n\n                                 inferred_completion  \n0  ```python\\npears_keith_premise = 47.0\\npears_m...  \n1  Ї\\n```python\\nstudents_premise = 10.0\\nstudent...  \n2  ```python\\nchairs_per_row_premise = 16.0\\nrows...  \n3  Ї\\n```python\\nchildren_premise = 4.0\\npencils_...  \n4  ```python\\ncandy_premise = 3409.0\\neggs_premis...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_index</th>\n      <th>completion</th>\n      <th>prompt</th>\n      <th>source</th>\n      <th>input_prompt</th>\n      <th>inferred_completion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>386</td>\n      <td>```python\\npicked_pears_keith = 47.0\\npicked_p...</td>\n      <td>### Instruction:\\n\\nYou must write Python code...</td>\n      <td>awpnli</td>\n      <td>&lt;s&gt;[INST]\\n&lt;&lt;SYS&gt;&gt;\\nGive a response suitable t...</td>\n      <td>```python\\npears_keith_premise = 47.0\\npears_m...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>140</td>\n      <td>```python\\ninitial_students_premise = 10.0\\nst...</td>\n      <td>### Instruction:\\n\\nYou must write Python code...</td>\n      <td>awpnli</td>\n      <td>&lt;s&gt;[INST]\\n&lt;&lt;SYS&gt;&gt;\\nGive a response suitable t...</td>\n      <td>Ї\\n```python\\nstudents_premise = 10.0\\nstudent...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>199</td>\n      <td>```python\\nrows_premise = 27.0\\nchairs_per_row...</td>\n      <td>### Instruction:\\n\\nYou must write Python code...</td>\n      <td>awpnli</td>\n      <td>&lt;s&gt;[INST]\\n&lt;&lt;SYS&gt;&gt;\\nGive a response suitable t...</td>\n      <td>```python\\nchairs_per_row_premise = 16.0\\nrows...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>321</td>\n      <td>```python\\nchildren_premise = 4.0\\npencils_per...</td>\n      <td>### Instruction:\\n\\nYou must write Python code...</td>\n      <td>awpnli</td>\n      <td>&lt;s&gt;[INST]\\n&lt;&lt;SYS&gt;&gt;\\nGive a response suitable t...</td>\n      <td>Ї\\n```python\\nchildren_premise = 4.0\\npencils_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>633</td>\n      <td>```python\\ncandy_premise = 3409.0\\neggs_premis...</td>\n      <td>### Instruction:\\n\\nYou must write Python code...</td>\n      <td>awpnli</td>\n      <td>&lt;s&gt;[INST]\\n&lt;&lt;SYS&gt;&gt;\\nGive a response suitable t...</td>\n      <td>```python\\ncandy_premise = 3409.0\\neggs_premis...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [],
   "source": [
    "def has_python_script(text: str) -> bool:\n",
    "    \"\"\"There is a complete or incomplete script in the LLM completion\"\"\"\n",
    "    return (not pd.isna(text)) and (\"```python\" in text or \"```\" in text or \"START_CODE\" in text) and (\"### Instruction\" not in text and \"### Output\" not in text)\n",
    "\n",
    "def has_non_python_script(completion: str):\n",
    "    \"\"\"CodeLlama gives Golang scripts, detect those by some common keywords\"\"\"\n",
    "    return (not pd.isna(completion)) and (\"package main\" in completion or \"Println\" in completion or \":=\" in completion)\n",
    "\n",
    "def extract_script(text: str) -> Optional[str | None]:\n",
    "    \"\"\"Extract Python scripts from the completion\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    if \"START_CODE\" in text:\n",
    "        return extract_script_between_indicators(text, \"START_CODE\", \"# END_CODE\")\n",
    "    elif \"START_SECTION\" in text and \"```\" not in text:\n",
    "        return extract_script_between_indicators(text, \"START_SECTION\", \"#END_SECTION\")\n",
    "    elif \"write your code here\" in text:\n",
    "        return extract_script_with_write_code_here_indicator(text)\n",
    "    elif \"```python\" in text:\n",
    "        return extract_script_between_indicators(text, \"```python\", \"\\n```\")\n",
    "    else:\n",
    "        script = extract_script_between_indicators(text, \"```\", \"```\")\n",
    "        if script == np.nan:\n",
    "            # sometimes the '''python part is missing\n",
    "            script = text.rsplit(\"```\", maxsplit=1)[0] + \"\\n\"\n",
    "            if script == \"\":\n",
    "                script = text.split(\"```\", maxsplit=1)[1].replace(\"python\", \"\")\n",
    "            if script.strip().strip(\"\\n\") == \"\":\n",
    "                return np.nan\n",
    "        return script\n",
    "        # code_extraction_regex, code_extraction_regex_backup = r\"```python([^`]+)```\", r\"```([^`]+)```\"\n",
    "        # scripts = re.findall(code_extraction_regex, text)\n",
    "        # if not scripts:\n",
    "        #     # rare case: max_tokens reached before \"```\" indicator is completely returned by LLM\n",
    "        #     scripts = re.findall(code_extraction_regex_backup, text)\n",
    "        #     if not scripts:\n",
    "        #         script = None\n",
    "        #         # sometimes the '''python part is missing\n",
    "        #         if \"```\" in text:\n",
    "        #             script = text.rsplit(\"```\", maxsplit=1)[0]\n",
    "        #             if script == \"\":\n",
    "        #                 script = text.split(\"```\", maxsplit=1)[1].replace(\"python\", \"\")\n",
    "        #         if script is None or script.strip().strip(\"\\n\") == \"\":\n",
    "        #             return np.nan\n",
    "        #         scripts = [script]\n",
    "\n",
    "        # return scripts[0].replace(\"`\", \"\").replace(\"python\", \"\").lstrip(\"\\n\")\n",
    "\n",
    "\n",
    "# def extract_script_with_start_code_indicators(text: str):\n",
    "#     code_extraction_regex = r\"START_CODE([^`]+)# END_CODE\"\n",
    "#     scripts = re.findall(code_extraction_regex, text)\n",
    "#     if not scripts:\n",
    "#         return np.nan\n",
    "#     return scripts[0].replace(\"START_CODE\", \"\").replace(\"# END_CODE\", \"\").lstrip(\"\\n\")\n",
    "\n",
    "def extract_script_with_write_code_here_indicator(text: str):\n",
    "    return text.split(\"write your code here\")[1].lstrip(\"\\n\")\n",
    "\n",
    "# def extract_script_with_start_section_indicators(text: str):\n",
    "#     code_extraction_regex = r\"START_SECTION([^`]+)#END_SECTION\"\n",
    "#     scripts = re.findall(code_extraction_regex, text)\n",
    "#     if not scripts:\n",
    "#         return np.nan\n",
    "#     script = scripts[0].replace(\"START_SECTION\", \"\").replace(\"#END_SECTION\", \"\").lstrip(\"\\n\")\n",
    "#     if not script.endswith(\"\\n\"):\n",
    "#         script += \"\\n\"\n",
    "#     return script\n",
    "\n",
    "def extract_script_between_indicators(text: str, start_indicator: str, end_indicator: str):\n",
    "    code_extraction_regex = fr\"{start_indicator}([^`]+){end_indicator}\"\n",
    "    scripts = re.findall(code_extraction_regex, text)\n",
    "    if not scripts:\n",
    "        return np.nan\n",
    "    script = scripts[0].replace(start_indicator, \"\").replace(end_indicator, \"\").lstrip(\"\\n\")\n",
    "    if not script.endswith(\"\\n\"):\n",
    "        script += \"\\n\"\n",
    "    return script"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "outputs": [],
   "source": [
    "# script = \"\"\"START_CODE\n",
    "# some\n",
    "# script here\n",
    "# pam pam\n",
    "# # END_CODE\n",
    "# \"\"\"\n",
    "\n",
    "# extract_script_between_indicators(script, \"```python\", \"```\")\n",
    "# extract_script_between_indicators(script, \"```\", \"```\")\n",
    "# extract_script_between_indicators(script, \"START_SECTION\", \"#END_SECTION\")\n",
    "# extract_script_between_indicators(script, \"START_CODE\", \"# END_CODE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Any empty completions?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_completions = test_set[(test_set[\"inferred_completion\"].isna()) | (test_set[\"inferred_completion\"].isin([\"\", \" \"]))].shape[0]\n",
    "results_overview[\"empty_completions\"] = empty_completions\n",
    "empty_completions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How many completions with Python scripts?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "outputs": [
    {
     "data": {
      "text/plain": "1651"
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"has_script\"] = test_set[\"inferred_completion\"].apply(lambda completion: has_python_script(completion))\n",
    "\n",
    "python_scripts = test_set[test_set[\"has_script\"] == True].shape[0]\n",
    "results_overview[\"python_scripts\"] = python_scripts\n",
    "python_scripts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How many completions with Golang scripts?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"has_non_python_script\"] = test_set[\"inferred_completion\"].apply(lambda completion: has_non_python_script(completion))\n",
    "\n",
    "non_python_scripts = test_set[test_set[\"has_non_python_script\"] == True].shape[0]\n",
    "results_overview[\"non_python_scripts\"] = non_python_scripts\n",
    "non_python_scripts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check there cannot be both Python and non-Python scripts in one completion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[(test_set[\"has_script\"]==True) & (test_set[\"has_non_python_script\"]==True)].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Per dataset, what is the split between generated (complete or not) scripts and missing scripts?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "outputs": [
    {
     "data": {
      "text/plain": "source      has_script\nawpnli      False            9\n            True           170\nnewsnli     False           46\n            True           193\nredditnli   False            6\n            True            55\nrtequant    False            5\n            True            35\nstresstest  False           19\n            True          1198\nName: sample_index, dtype: int64"
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.groupby([\"source\", \"has_script\"]).count()[\"sample_index\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Inspect some of the identified Python scripts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "outputs": [],
   "source": [
    "test_set[\"inferred_script\"] = test_set[\"inferred_completion\"].apply(lambda completion: extract_script(completion))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "outputs": [
    {
     "data": {
      "text/plain": "'pears_keith_premise = 47.0\\npears_mike_premise = 12.0\\npears_given_premise = 46.0\\npears_mike_hypothesis = 13.0\\n\\n# the hypothesis refers to the number of pears Mike has left, which is also mentioned in the premise\\n# compute the total number of pears Mike has in the premise\\npears_mike_premise = pears_mike_premise + pears_given_premise\\nif pears_mike_hypothesis!= pears_mike_premise:\\n    # check if the number of pears Mike has in the hypothesis contradicts the number of pears Mike has from the premise\\n    label = \"contradiction\"\\nelse:\\n    # if the hypothesis values and estimates do not contradict the premise values, we can infer entailment\\n    label = \"entailment\"    \\n\\nprint(label)\\n'"
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"inferred_script\"].iloc[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lines_in_script(script):\n",
    "    if pd.isna(script):\n",
    "        return -1\n",
    "    return len(script.split(\"\\n\"))\n",
    "\n",
    "test_set[\"lines_in_script\"] = test_set[\"inferred_script\"].apply(lambda script: lines_in_script(script))\n",
    "\n",
    "test_set[(test_set[\"lines_in_script\"]<=4) & (test_set[\"lines_in_script\"]>=0)].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "outputs": [
    {
     "data": {
      "text/plain": "Series([], Name: inferred_script, dtype: object)"
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[(test_set[\"lines_in_script\"]<=4) & (test_set[\"lines_in_script\"]>=0)][\"inferred_script\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We notice that some scripts only return a boolean value, but there is no reasoning/math logic to infer it, so these scripts should be skipped during the evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def returns_only_label(script: str, lines_count: int):\n",
    "    \"\"\"Detect Python scripts that return directly a label, with no reasoning\"\"\"\n",
    "    if pd.isna(script):\n",
    "        return False\n",
    "    return lines_count <=4 and lines_count >= 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set[\"only_label\"] = test_set.apply(lambda row: returns_only_label(row[\"inferred_script\"], row[\"lines_in_script\"]), axis=1)\n",
    "label_only = test_set[test_set[\"only_label\"] == True].shape[0]\n",
    "results_overview[\"label_only\"] = label_only\n",
    "label_only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### For how many samples the `max_new_tokens` limit was exceeded before the full script could not be outputted?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def incomplete_script(has_script: bool, script: Optional[str | None]) -> bool:\n",
    "    return has_script and pd.isna(script)\n",
    "\n",
    "test_set[\"tokens_exceeded\"] = test_set.apply(lambda row: incomplete_script(row[\"has_script\"], row[\"inferred_script\"]), axis=1)\n",
    "tokens_exceeded = test_set[test_set[\"tokens_exceeded\"]==True].shape[0]\n",
    "results_overview[\"tokens_exceeded\"] = tokens_exceeded\n",
    "tokens_exceeded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set[\"expected_script\"] = test_set[\"completion\"].str.replace(\"```\", \"\")\n",
    "test_set[\"expected_script\"] = test_set[\"expected_script\"].str.replace(\"python\", \"\").str.lstrip(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### COMPUTE SIMILARITY BETWEEN TARGET AND INFERRED COMPLETION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your generated text and the reference text\n",
    "generated_text = \"The weather outside is so good today!\"\n",
    "reference_text = \"Today we have such good weather outside!\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ROUGE metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a scorer object\n",
    "scorer_1 = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "scorer_2 = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "scorer_L = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def rouge1_score(reference_text: str, generated_text: str, compute_score: bool):\n",
    "    if not (pd.isna(reference_text) or pd.isna(generated_text)) and compute_score:\n",
    "        return round(scorer_1.score(reference_text, generated_text)['rouge1'].fmeasure, 4)\n",
    "    return np.nan\n",
    "\n",
    "def rouge2_score(reference_text: str, generated_text: str, compute_score: bool):\n",
    "    if not (pd.isna(reference_text) or pd.isna(generated_text)) and compute_score:\n",
    "        return round(scorer_2.score(reference_text, generated_text)['rouge2'].fmeasure, 4)\n",
    "    return np.nan\n",
    "\n",
    "def rougeL_score(reference_text: str, generated_text: str, compute_score: bool):\n",
    "    if not (pd.isna(reference_text) or pd.isna(generated_text)) and compute_score:\n",
    "        return round(scorer_L.score(reference_text, generated_text)['rougeL'].fmeasure, 4)\n",
    "    return np.nan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(rouge1_score(reference_text, generated_text, True))\n",
    "print(rouge2_score(reference_text, generated_text, True))\n",
    "print(rougeL_score(reference_text, generated_text, True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### METEOR metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "\n",
    "# Ensure that NLTK's tokenizers and taggers are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def meteor_score(reference_text: str, generated_text: str, compute_score: bool):\n",
    "    if not (pd.isna(reference_text) or pd.isna(generated_text)) and compute_score:\n",
    "        return round(single_meteor_score(reference_text.split(\" \"), generated_text.split(\" \")), 4)\n",
    "    return np.nan\n",
    "\n",
    "print(meteor_score(reference_text, generated_text, True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\n",
    "# from tree_sitter import Language, Parser\n",
    "#\n",
    "# # Load your programming language grammar, assuming Python here\n",
    "# # Language.build_library(\n",
    "# #   'build/my-languages.so',\n",
    "# #   ['python']\n",
    "# # )\n",
    "# PY_LANGUAGE = Language('parser/my-languages.so', 'python')\n",
    "# parser = Parser()\n",
    "# parser.set_language(PY_LANGUAGE)\n",
    "#\n",
    "# def get_tokens(code):\n",
    "#     \"\"\"Extract tokens from source code using tree-sitter.\"\"\"\n",
    "#     tree = parser.parse(bytes(code, \"utf8\"))\n",
    "#     root_node = tree.root_node\n",
    "#     tokens = [node.text.decode('utf8') for node in root_node.walk() if node.type == 'identifier']\n",
    "#     return tokens\n",
    "#\n",
    "# # Example codes\n",
    "# generated_code = \"def add(a, b):\\n    return a + b\"\n",
    "# reference_code = [\"def add(num1, num2):\\n    return num1 + num2\"]\n",
    "#\n",
    "# # Tokenize codes\n",
    "# tokenized_generated_code = get_tokens(generated_code)\n",
    "# tokenized_reference_code = [get_tokens(ref) for ref in reference_code]\n",
    "#\n",
    "# # Compute BLEU score\n",
    "# chencherry = SmoothingFunction()\n",
    "# bleu_score = corpus_bleu(tokenized_reference_code, [tokenized_generated_code], smoothing_function=chencherry.method1)\n",
    "#\n",
    "# print(f\"CodeBLEU (simplified, BLEU part only) score: {bleu_score}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CHRF score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "\n",
    "# Ensure that NLTK's tokenizers and data are available\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chrf_score(reference_text: str, generated_text: str, compute_score: bool):\n",
    "    if not (pd.isna(reference_text) or pd.isna(generated_text)) and compute_score:\n",
    "        return round(sentence_chrf(reference_text, generated_text), 4)\n",
    "    return np.nan\n",
    "\n",
    "print(f\"CHRF score: {chrf_score(reference_text, generated_text, True)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare inferred and expected scripts only for valid Python scripts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_script(row):\n",
    "    \"\"\"Evaluate a script only if it is a complete Python script that does not return a label directly \"\"\"\n",
    "    return not row[\"tokens_exceeded\"] and not row[\"only_label\"] and row[\"has_script\"]\n",
    "\n",
    "test_set[\"evaluate_script\"] = test_set.apply(lambda row: evaluate_script(row), axis=1)\n",
    "evaluated_scripts = test_set[test_set[\"evaluate_script\"]==True].shape[0]\n",
    "results_overview[\"evaluated_scripts\"] = evaluated_scripts\n",
    "evaluated_scripts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set[\"rouge1_score\"] = test_set.apply(lambda row: rouge1_score(row[\"expected_script\"], row[\"inferred_script\"], row[\"evaluate_script\"]), axis=1)\n",
    "test_set[\"rouge2_score\"] = test_set.apply(lambda row: rouge2_score(row[\"expected_script\"], row[\"inferred_script\"], row[\"evaluate_script\"]), axis=1)\n",
    "test_set[\"rougeL_score\"] = test_set.apply(lambda row: rougeL_score(row[\"expected_script\"], row[\"inferred_script\"], row[\"evaluate_script\"]), axis=1)\n",
    "test_set[\"meteor_score\"] = test_set.apply(lambda row: meteor_score(row[\"expected_script\"], row[\"inferred_script\"], row[\"evaluate_script\"]), axis=1)\n",
    "test_set[\"chrf_score\"] = test_set.apply(lambda row: chrf_score(row[\"expected_script\"], row[\"inferred_script\"], row[\"evaluate_script\"]), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_columns = [col for col in test_set.columns if \"_score\" in col]\n",
    "code_similarity_scores = test_set[score_columns].aggregate([\"mean\", \"std\"])\n",
    "\n",
    "code_similarity_scores.T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write the inferred scripts to python files for QNLI classification evaluation & code quality evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set[\"evaluate_script\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name_mapping = {\n",
    "    \"stresstest\": \"StressTest\",\n",
    "    \"rtequant\": \"RTE_Quant\",\n",
    "    \"awpnli\": \"AWPNLI\",\n",
    "    \"newsnli\": \"NewsNLI\",\n",
    "    \"redditnli\": \"RedditNLI\"\n",
    "}\n",
    "\n",
    "for _, dataset in dataset_name_mapping.items():\n",
    "    os.makedirs(os.path.join(results_path, \"scripts\", dataset), exist_ok=True)\n",
    "\n",
    "def write_script_to_file(sample_index: str, source_dataset: str, script: Optional[str | None], evaluate_script: bool):\n",
    "    if not evaluate_script:\n",
    "        return False\n",
    "    try:\n",
    "        with open(os.path.join(results_path, \"scripts\", dataset_name_mapping[source_dataset], f\"sample_{sample_index}.py\"), 'x') as f:\n",
    "            f.write(script)\n",
    "        return True\n",
    "    except FileExistsError:\n",
    "        with open(os.path.join(results_path, \"scripts\", dataset_name_mapping[source_dataset], f\"sample_{sample_index}.py\"), 'w') as f:\n",
    "            f.write(script)\n",
    "        return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set[\"script_to_file\"] = test_set.apply(lambda row: write_script_to_file(row[\"sample_index\"], row[\"source\"], row[\"inferred_script\"], row[\"evaluate_script\"]), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set[test_set[\"script_to_file\"] == True].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.utils import run_script\n",
    "from src.code_quality_utils import validate_code_quality\n",
    "\n",
    "count = 0\n",
    "for idx, row in test_set.iterrows():\n",
    "    print(idx)\n",
    "    qualified_for_classification = getattr(row, \"evaluate_script\")\n",
    "    if qualified_for_classification:\n",
    "        count += 1\n",
    "        sample_index, dataset_key = getattr(row, \"sample_index\"), getattr(row, \"source\")\n",
    "        dataset = dataset_name_mapping[dataset_key]\n",
    "        script_path = os.path.join(results_path, \"scripts\", dataset, f\"sample_{sample_index}.py\")\n",
    "        label, error_message = run_script(script_path)\n",
    "        test_set.loc[idx, \"inferred_label\"] = label\n",
    "        test_set.loc[idx, \"error_message\"] = error_message\n",
    "\n",
    "        code_quality_scores, code_quality_resolutions = validate_code_quality(script_path)\n",
    "        for score_key, score in code_quality_scores.items():\n",
    "            test_set.loc[idx, score_key] = score\n",
    "        test_set.loc[idx, \"cc_resolutions\"] = \"; \".join(code_quality_resolutions)\n",
    "\n",
    "print(f\"Evaluated {count} scripts.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_obtained = test_set[test_set[\"inferred_label\"].isin([\"entailment\", \"contradiction\", \"neutral\"])].shape[0]\n",
    "results_overview[\"labels_obtained\"] = labels_obtained\n",
    "labels_obtained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "code_quality_scores = ['readability', 'document_size', 'redundancy_check', 'function_size']\n",
    "\n",
    "code_quality_scores = test_set[code_quality_scores].aggregate([\"mean\", \"std\"])\n",
    "code_quality_scores.T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "code_quality_df = pd.concat([code_quality_scores.T, code_similarity_scores.T])\n",
    "code_quality_df.to_excel(os.path.join(results_path, \"evaluation_new\", \"code_quality.xlsx\"))\n",
    "code_quality_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set[~test_set[\"readability\"].isna()].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## View results overview"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_overview"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QNLI results evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset_name_mapping = {\n",
    "#     \"stresstest\": \"StressTest\",\n",
    "#     \"rtequant\": \"RTE_Quant\",\n",
    "#     \"awpnli\": \"AWPNLI\",\n",
    "#     \"newsnli\": \"NewsNLI\",\n",
    "#     \"redditnli\": \"RedditNLI\"\n",
    "# }\n",
    "#\n",
    "# merged = pd.DataFrame()\n",
    "# for dataset_key, dataset in dataset_name_mapping.items():\n",
    "#     print(dataset)\n",
    "#     df = pd.read_csv(os.path.join(root_path, \"data\", \"equate_labelled\", \"processed\", f\"{dataset}.csv\"))\n",
    "#     print(f\"All samples: {df.shape[0]}\")\n",
    "#     df[\"source\"] = dataset_key\n",
    "#     test_indices = test_set[test_set[\"source\"]==dataset_key][\"sample_index\"].unique()\n",
    "#     print(f\"Test samples: {len(test_indices)}\")\n",
    "#     test_subset = df[df[\"sample_index\"].isin(test_indices)]\n",
    "#     if merged.shape[0] == 0:\n",
    "#         merged = test_subset\n",
    "#     else:\n",
    "#         merged = pd.concat([merged, test_subset], ignore_index=True)\n",
    "#\n",
    "# merged.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merged.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merged.to_csv(os.path.join(root_path, \"data\", \"finetuning\", \"test_all_with_labels.csv\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gt_data = pd.read_csv(os.path.join(root_path, \"data\", \"finetuning\", \"test_all_with_labels.csv\"))\n",
    "gt_data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gt_data[\"source\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gt_data[\"golden_label\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gt_data.groupby('source')[\"golden_label\"].value_counts(normalize=True).reset_index().sort_values(by=[\"source\", \"golden_label\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "0.370392**2+0.323157**2+0.306452**2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df = pd.merge(test_set, gt_data, on=[\"sample_index\", \"source\"], how=\"inner\")\n",
    "qnli_results_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df.drop(\"cc_resolutions\", axis=\"columns\", inplace=True)\n",
    "# qnli_results_df.to_excel(os.path.join(results_path, \"evaluation\", \"qnli_classification_results.xlsx\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df[\"inferred_label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name_mapping = {\n",
    "    \"stresstest\": \"StressTest\",\n",
    "    \"rtequant\": \"RTE_Quant\",\n",
    "    \"awpnli\": \"AWPNLI\",\n",
    "    \"newsnli\": \"NewsNLI\",\n",
    "    \"redditnli\": \"RedditNLI\",\n",
    "    \"equate\": \"EQUATE\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_accuracies(results_df):\n",
    "    \"\"\"Get the accuracy of the model (base or fine-tuned) and GPT4 on each dataset and on EQUATE.\"\"\"\n",
    "\n",
    "    dataset_names, accuracies = [], []\n",
    "    for dataset_key, dataset in dataset_name_mapping.items():\n",
    "        # print(f\"#################{dataset}#################\")\n",
    "\n",
    "        dataset_names.append(dataset)\n",
    "\n",
    "        if dataset != \"EQUATE\":\n",
    "            filtered_df = results_df[results_df[\"source\"] == dataset_key]\n",
    "        else:\n",
    "            filtered_df = results_df\n",
    "\n",
    "        acc = accuracy_score(filtered_df['golden_label'], filtered_df['inferred_label'])\n",
    "        # print(f\"Accuracy (model): {acc}\")\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        # print(f\"Accuracy (GPT4): {accuracy_score(filtered_df['golden_label'], filtered_df['reference_label'])}\")\n",
    "\n",
    "    return pd.DataFrame({\"dataset\": dataset_names, \"accuracy\": accuracies})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classification_report_to_dataframe(clsf_report, dataset):\n",
    "    f1_score = pd.DataFrame(clsf_report)\n",
    "    f1_score = f1_score.loc[\"f1-score\"]\n",
    "\n",
    "    for label in [\"entailment\", \"contradiction\", \"neutral\"]:\n",
    "        if label not in f1_score.index:\n",
    "            f1_score.loc[label] = 0\n",
    "\n",
    "    f1_score = f1_score.loc[[\"entailment\", \"contradiction\", \"neutral\", \"weighted avg\"]]\n",
    "    f1_score_df = pd.DataFrame(dict(zip([\"F1-E\", \"F1-C\", \"F1-N\", \"weighted_F1\"], f1_score.values)), index=[1])\n",
    "    f1_score_df[\"dataset\"] = dataset\n",
    "\n",
    "    return f1_score_df\n",
    "\n",
    "\n",
    "def print_classification_reports(results_df):\n",
    "    \"\"\"Get the accuracy of the model (base or fine-tuned) and GPT4 on each dataset and on EQUATE.\"\"\"\n",
    "\n",
    "    clsf_report_dataset = pd.DataFrame()\n",
    "    gpt4_report = pd.DataFrame()\n",
    "    for dataset_key, dataset in dataset_name_mapping.items():\n",
    "        # print(f\"#################{dataset}#################\")\n",
    "\n",
    "        if dataset != \"EQUATE\":\n",
    "            filtered_df = results_df[results_df[\"source\"] == dataset_key]\n",
    "        else:\n",
    "            filtered_df = results_df\n",
    "\n",
    "        # print(\"######Base / Fine-tuned model######\")\n",
    "        clsf_report_dict = classification_report(filtered_df['golden_label'], filtered_df['inferred_label'], output_dict=True)\n",
    "        clsf_report_df = classification_report_to_dataframe(clsf_report_dict, dataset)\n",
    "        clsf_report_dataset = pd.concat([clsf_report_dataset, clsf_report_df], ignore_index=True)\n",
    "        # print(classification_report(filtered_df['golden_label'], filtered_df['inferred_label']))\n",
    "\n",
    "        # print(\"######GPT4######\")\n",
    "        clsf_report_dict = classification_report(filtered_df['golden_label'], filtered_df['reference_label'], output_dict=True)\n",
    "        clsf_report_df = classification_report_to_dataframe(clsf_report_dict, dataset)\n",
    "        gpt4_report = pd.concat([gpt4_report, clsf_report_df], ignore_index=True)\n",
    "        # print(classification_report(filtered_df['golden_label'], filtered_df['reference_label']))\n",
    "\n",
    "    return clsf_report_dataset, gpt4_report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For RTE_Quant and NewsNLI, convert \"contradiction\" to \"neutral\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df[(qnli_results_df[\"inferred_label\"] == \"contradiction\") & (qnli_results_df[\"source\"].isin([\"rtequant\", \"newsnli\"]))].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating masks based on conditions\n",
    "convert_contradiction_to_neutral_mask = qnli_results_df[\"inferred_label\"] == \"contradiction\"\n",
    "filter_dataset_mask = qnli_results_df[\"source\"].isin([\"rtequant\", \"newsnli\"])\n",
    "\n",
    "# Combining both masks\n",
    "combined_mask = convert_contradiction_to_neutral_mask & filter_dataset_mask\n",
    "\n",
    "# Correctly assigning a new value using the combined mask with .loc\n",
    "qnli_results_df.loc[combined_mask, \"inferred_label\"] = \"neutral\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check that the conversion worked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df[(qnli_results_df[\"inferred_label\"] == \"contradiction\") & (qnli_results_df[\"source\"].isin([\"rtequant\", \"newsnli\"]))].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df.to_excel(os.path.join(results_path, \"evaluation_new\", \"qnli_classification_results.xlsx\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### EVALUATION 1:  Impute majority label for each dataset where label is NaN or error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### First check how many labels we got for each dataset and for equate as a whole"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_samples_per_dataset = qnli_results_df[\"source\"].value_counts().reset_index().sort_values(by=\"source\").rename(columns={\"count\": \"total\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_samples_per_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_obtained_df = qnli_results_df[qnli_results_df[\"inferred_label\"].isin([\"entailment\", \"neutral\", \"contradiction\"])]\n",
    "print(labels_obtained_df.shape[0])\n",
    "obtained_labels_counts = labels_obtained_df[\"source\"].value_counts().reset_index().sort_values(by=\"source\")\n",
    "obtained_labels_counts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obtained_labels = pd.merge(test_samples_per_dataset, obtained_labels_counts, on=\"source\")\n",
    "obtained_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obtained_labels[\"ratio\"] = round(obtained_labels[\"count\"] / obtained_labels[\"total\"], 4)\n",
    "obtained_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obtained_labels[[\"source\", \"count\", \"ratio\"]].to_excel(os.path.join(results_path, \"evaluation_new\", \"counts_and_ratios_obtained_labels.xlsx\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for dataset, majority_label in zip([\"rtequant\", \"redditnli\", \"newsnli\", \"awpnli\", \"stresstest\"], [\"neutral\", \"entailment\", \"entailment\", \"contradiction\", \"contradiction\"]):\n",
    "#     mask_label = ~qnli_results_df[\"inferred_label\"].isin([\"entailment\", \"neutral\", \"contradiction\"])\n",
    "#     mask_source = qnli_results_df[\"source\"] == dataset\n",
    "#     qnli_results_df.loc[mask_label & mask_source, \"inferred_label\"] = majority_label\n",
    "# qnli_results_df[\"inferred_label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# accuracy_df = print_accuracies(qnli_results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f1_df, gpt_f1 = print_classification_reports(qnli_results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation_df = pd.merge(accuracy_df, f1_df, on=\"dataset\", how=\"inner\")\n",
    "# evaluation_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # gpt_f1.to_excel(\"gpt4_f1_evaluation.xlsx\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation_df.to_excel(os.path.join(results_path, \"evaluation\", \"imputed_mb_label.xlsx\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### EVALUATION 2:  Drop samples with NaN script or \"error\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# qnli_results_df = pd.merge(test_set, gt_data, on=[\"sample_index\", \"source\"], how=\"inner\")\n",
    "# qnli_results_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Creating masks based on conditions\n",
    "# convert_contradiction_to_neutral_mask = qnli_results_df[\"inferred_label\"] == \"contradiction\"\n",
    "# filter_dataset_mask = qnli_results_df[\"source\"].isin([\"rtequant\", \"newsnli\"])\n",
    "#\n",
    "# # Combining both masks\n",
    "# combined_mask = convert_contradiction_to_neutral_mask & filter_dataset_mask\n",
    "#\n",
    "# # Correctly assigning a new value using the combined mask with .loc\n",
    "# qnli_results_df.loc[combined_mask, \"inferred_label\"] = \"neutral\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mask_discard = ~qnli_results_df[\"inferred_label\"].isin([\"entailment\", \"contradiction\", \"neutral\"])\n",
    "# qnli_results_df = qnli_results_df.drop(qnli_results_df[mask_discard].index, axis=\"rows\")\n",
    "# qnli_results_df[\"inferred_label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# accuracy_df = print_accuracies(qnli_results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f1_df, gpt_f1 = print_classification_reports(qnli_results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation_df = pd.merge(accuracy_df, f1_df, on=\"dataset\", how=\"inner\")\n",
    "# evaluation_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation_df.to_excel(os.path.join(results_path, \"evaluation\", \"discard_erroneous_samples.xlsx\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### EVALUATION 3: Impute the missing labels with a stratified random strategy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INFERENCE LLAMA2 - 7B\n",
    "# model_name = \"llama-2-chat\"\n",
    "# model_size = \"7B\"\n",
    "## base model\n",
    "# job_id = \"5817639\"\n",
    "## finetuned\n",
    "# job_id = \"5820349_finetuned\"\n",
    "## fintuned with balanced dataset\n",
    "# job_id = \"5836402_finetuned\"\n",
    "\n",
    "## INFERENCE LLAMA2 - 13B\n",
    "# model_name = \"llama-2-chat\"\n",
    "# model_size = \"13B\"\n",
    "## base model\n",
    "# job_id = \"5822467\"\n",
    "## finetuned\n",
    "# job_id = \"5830782_finetuned\"\n",
    "\n",
    "## INFERENCE CODELLAMA - PYTHON 7B\n",
    "# model_name = \"codellama-python\"\n",
    "# model_size = \"7B\"\n",
    "## base model\n",
    "# job_id = \"5852469\"\n",
    "## finetuned\n",
    "# job_id = \"5897597_finetuned\"\n",
    "\n",
    "\n",
    "##### REDO MATRI FOR AWPNLI ABOVE THIS\n",
    "\n",
    "## INFERENCE CODELLAMA - PYTHON 13B\n",
    "# model_name = \"codellama-python\"\n",
    "# model_size = \"13B\"\n",
    "## base model\n",
    "# job_id = \"5919626\"\n",
    "## finetuned\n",
    "# job_id = \"\"\n",
    "\n",
    "## INFERENCE CODELLAMA - INSTRUCT 7B\n",
    "# model_name = \"codellama-instruct\"\n",
    "# model_size = \"7B\"\n",
    "# # base model\n",
    "# job_id = \"5858736\"\n",
    "## finetuned\n",
    "# job_id = \"5892669_finetuned\"\n",
    "\n",
    "## INFERENCE CODELLAMA - INSTRUCT 13B\n",
    "# model_name = \"codellama-instruct\"\n",
    "# model_size = \"13B\"\n",
    "# # base model\n",
    "# job_id = \"5875184\"\n",
    "# finetuned\n",
    "# job_id = \"6001211_finetuned\"\n",
    "\n",
    "## INFERENCE LLAMA-3 8B\n",
    "# model_name = \"llama3\"\n",
    "# model_size = \"8B\"\n",
    "# # base model\n",
    "# job_id = \"6013398\"\n",
    "\n",
    "# results_path = os.path.join(root_path, \"results_inference\", model_name, model_size, job_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ## CONFUSION MATRICES FOR GPT4\n",
    "#\n",
    "# qnli_results_df = pd.read_excel(os.path.join(results_path, \"evaluation\", \"qnli_classification_results.xlsx\"))\n",
    "#\n",
    "# # for dataset in [\"rtequant\", \"newsnli\", \"awpnli\", \"stresstest\", \"redditnli\"]:  #\n",
    "# #     filtered_df = qnli_results_df[qnli_results_df[\"source\"] == dataset]\n",
    "# #     if dataset in [\"rtequant\", \"newsnli\"]:\n",
    "# #         labels = [\"entailment\", \"neutral\"]\n",
    "# #     elif dataset == \"awpnli\":\n",
    "# #         labels = [\"contradiction\", \"entailment\"]\n",
    "# #     else:\n",
    "# #         labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "# #     cf = confusion_matrix(filtered_df[\"golden_label\"], filtered_df[\"reference_label\"], labels=labels, normalize=\"true\")\n",
    "# #     display = ConfusionMatrixDisplay(cf, display_labels=labels)\n",
    "# #     display.plot(cmap=custom_cmap)\n",
    "# #     plt.title(f\"GPT4 - {dataset_name_mapping[dataset]}\")\n",
    "# #     # Increase font size of the numbers within the squares\n",
    "# #     for text in display.text_.ravel():\n",
    "# #         text.set_fontsize(24)  # Adjust font size as needed\n",
    "# #     plt.savefig(os.path.join(root_path, \"results_inference\", \"gpt4\", f\"{dataset}_confusion_matrix.pdf\"))\n",
    "#\n",
    "# cf = confusion_matrix(qnli_results_df[\"golden_label\"], qnli_results_df[\"reference_label\"], labels = [\"contradiction\", \"entailment\", \"neutral\"], normalize=\"true\")\n",
    "# display = ConfusionMatrixDisplay(cf, display_labels=[\"C\", \"E\", \"N\"])\n",
    "# display.plot(cmap=custom_cmap)\n",
    "# # Increase font size of the numbers within the squares\n",
    "# for text in display.text_.ravel():\n",
    "#     text.set_fontsize(24)  # Adjust font size as needed\n",
    "# # Increase label size\n",
    "# plt.gca().tick_params(axis='both', which='major', labelsize=20)\n",
    "# plt.title(\"GPT4 - EQUATE\")\n",
    "# # Increase font size of the axis labels\n",
    "# plt.xlabel('Predicted label', fontsize=16)  # X-axis label\n",
    "# plt.ylabel('True label', fontsize=16)       # Y-axis label\n",
    "# # plt.savefig(os.path.join(root_path, \"results_inference\", \"gpt4\", f\"EQUATE_confusion_matrix_test.pdf\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# qnli_results_df = pd.merge(test_set, gt_data, on=[\"sample_index\", \"source\"], how=\"inner\")\n",
    "# qnli_results_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import LinearSegmentedColormap\n",
    "#\n",
    "# # Define custom colormap from white to black\n",
    "# cmap_colors = [(0, 0, 0), (1, 1, 1)]  # White to Black\n",
    "# custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", cmap_colors)\n",
    "#\n",
    "# qnli_results_df = pd.read_excel(os.path.join(results_path, \"evaluation\", \"qnli_classification_results.xlsx\"))\n",
    "# filtered_results = qnli_results_df[qnli_results_df[\"inferred_label\"].isin([\"entailment\", \"contradiction\", \"neutral\"])]\n",
    "#\n",
    "# for dataset in [\"awpnli\", \"stresstest\", \"redditnli\", \"rtequant\", \"newsnli\",]:\n",
    "#     filtered_df = filtered_results[filtered_results[\"source\"] == dataset]\n",
    "#     if dataset in [\"rtequant\", \"newsnli\"]:\n",
    "#         labels = [\"entailment\", \"neutral\"]\n",
    "#     elif dataset == \"awpnli\":\n",
    "#         labels = [\"contradiction\", \"entailment\"]\n",
    "#     else:\n",
    "#         labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "#     cf = confusion_matrix(filtered_df[\"golden_label\"], filtered_df[\"inferred_label\"], labels=labels, normalize=\"true\")\n",
    "#     display = ConfusionMatrixDisplay(cf, display_labels=labels)\n",
    "#     # Increase label size within the squares\n",
    "#     display.plot(cmap=custom_cmap)\n",
    "#     # Increase font size of the numbers within the squares\n",
    "#     for text in display.text_.ravel():\n",
    "#         text.set_fontsize(24)  # Adjust font size as needed\n",
    "#     plt.title(f\"CodeLlama-Instruct 13B - {dataset_name_mapping[dataset]}\")\n",
    "#     plt.savefig(os.path.join(results_path, f\"{dataset}_confusion_matrix.pdf\"))\n",
    "#\n",
    "# cf = confusion_matrix(filtered_results[\"golden_label\"], filtered_results[\"inferred_label\"], labels = [\"contradiction\", \"entailment\", \"neutral\"], normalize=\"true\")\n",
    "# display = ConfusionMatrixDisplay(cf, display_labels=[\"contradiction\", \"entailment\", \"neutral\"])\n",
    "# display.plot(cmap=custom_cmap)\n",
    "# # Increase font size of the numbers within the squares\n",
    "# for text in display.text_.ravel():\n",
    "#     text.set_fontsize(24)  # Adjust font size as needed\n",
    "# plt.title(\"CodeLlama-Instruct 13B - EQUATE\")\n",
    "# plt.savefig(os.path.join(results_path, f\"EQUATE_confusion_matrix.pdf\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def impute_rsb(df):\n",
    "#     for dataset in [\"rtequant\", \"redditnli\", \"newsnli\", \"awpnli\", \"stresstest\"]:\n",
    "#         mask_label = ~df[\"inferred_label\"].isin([\"entailment\", \"neutral\", \"contradiction\"])\n",
    "#         mask_source = df[\"source\"] == dataset\n",
    "#         nan_indices_len = df.loc[mask_label & mask_source].shape[0]\n",
    "#         label_counts = df[df[\"source\"]==dataset][\"golden_label\"].value_counts(normalize=True)\n",
    "#         imputed_labels = np.random.choice(label_counts.index, size=nan_indices_len, p=label_counts.values)\n",
    "#         df.loc[mask_label & mask_source, \"inferred_label\"] = imputed_labels\n",
    "#     return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# accuracies, f1_scores = pd.DataFrame(), pd.DataFrame()\n",
    "# for rep in range(100):\n",
    "#     qnli_results_df = pd.read_excel(os.path.join(results_path, \"evaluation\", \"qnli_classification_results.xlsx\"))\n",
    "#     imputed_df = impute_rsb(qnli_results_df)\n",
    "#     accuracies = pd.concat([accuracies, print_accuracies(imputed_df)], ignore_index=True)\n",
    "#     f1_scores_df, _ = print_classification_reports(imputed_df)\n",
    "#     f1_scores = pd.concat([f1_scores, f1_scores_df], ignore_index=True)\n",
    "#\n",
    "# avg_accuracy_df = accuracies.groupby(\"dataset\")[\"accuracy\"].aggregate([\"mean\", \"std\"]).reset_index().rename(columns={\"mean\": \"acc_mean\", \"std\": \"acc_std\"})\n",
    "# avg_f1_df = f1_scores.groupby(\"dataset\").aggregate([\"mean\", \"std\"])\n",
    "# avg_f1_df.columns = [' '.join(col).strip() for col in avg_f1_df.columns.values]\n",
    "# avg_f1_df.reset_index(inplace=True)\n",
    "# evaluation_df = pd.merge(avg_accuracy_df, avg_f1_df, on=\"dataset\", how=\"inner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# accuracy_df = print_accuracies(qnli_results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f1_df, gpt_f1 = print_classification_reports(qnli_results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation_df = pd.merge(accuracy_df, f1_df, on=\"dataset\", how=\"inner\")\n",
    "# evaluation_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation_df.to_excel(os.path.join(results_path, \"evaluation\", \"srb_imputed.xlsx\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gpt_f1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# list(map(lambda f1: round(f1*100, 2), gpt_f1[\"micro_F1\"].values))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perform some automatic quality tests on the scripts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset_function_name_map = {\n",
    "#     \"stresstest\": \"entailment_or_contradiction_or_neutral(\",\n",
    "#     \"redditnli\": \"entailment_or_contradiction_or_neutral(\",\n",
    "#     \"awpnli\": \"entailment_or_contradiction(\",\n",
    "#     \"newsnli\": \"entailment_or_neutral(\",\n",
    "#     \"rtequant\": \"entailment_or_neutral(\"\n",
    "# }\n",
    "#\n",
    "# def correct_function_name(script: str, dataset: str):\n",
    "#     return (not pd.isna(script)) and dataset_function_name_map[dataset] in script\n",
    "#\n",
    "# test_set[\"correct_function_name\"] = test_set.apply(lambda row: correct_function_name(row[\"inferred_script\"], row[\"source\"]), axis=1)\n",
    "#\n",
    "# scripts_with_correct_function_name = test_set[test_set[\"correct_function_name\"]==True].shape[0]\n",
    "# results_overview[\"correct_function_name\"] = scripts_with_correct_function_name\n",
    "# scripts_with_correct_function_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def script_length_in_lines(script: str):\n",
    "#     if pd.isna(script):\n",
    "#         return 0\n",
    "#     return len(script.split(\"\\n\"))\n",
    "#\n",
    "# test_set[\"expected_script_length\"] = test_set[\"expected_script\"].apply(lambda script: script_length_in_lines(script))\n",
    "# test_set[\"inferred_script_length\"] = test_set[\"inferred_script\"].apply(lambda script: script_length_in_lines(script))\n",
    "#\n",
    "# test_set[test_set[\"expected_script_length\"] == test_set[\"inferred_script_length\"]].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test_set[test_set[\"expected_script_length\"] != test_set[\"inferred_script_length\"]].head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# results = dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# qnli_results_df = pd.read_excel(os.path.join(results_path, \"evaluation\", \"qnli_classification_results.xlsx\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# results[\"tokens_exceeded\"] = qnli_results_df[qnli_results_df[\"tokens_exceeded\"]].shape[0]\n",
    "# results[\"missing_script\"] = qnli_results_df[qnli_results_df[\"has_script\"]==False].shape[0]\n",
    "# # results[\"invalid_script\"] = qnli_results_df[qnli_results_df[\"\"]==True].shape[0]\n",
    "# results[\"evaluated_scripts\"] = qnli_results_df[qnli_results_df[\"evaluate_script\"]==True].shape[0]\n",
    "# results[\"labels_obtained\"] = qnli_results_df[qnli_results_df[\"inferred_label\"].isin([\"entailment\", \"neutral\", \"contradiction\"])].shape[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# qnli_results_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# qnli_results_df[qnli_results_df[\"has_script\"]==False][\"inferred_completion\"].head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute the baseline micro-F1 score for each sub-dataset and for EQUATE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(qnli_results_df[qnli_results_df[\"source\"]==\"awpnli\"][\"golden_label\"].value_counts())\n",
    "# print(qnli_results_df[qnli_results_df[\"source\"]==\"redditnli\"][\"golden_label\"].value_counts())\n",
    "# print(qnli_results_df[qnli_results_df[\"source\"]==\"newsnli\"][\"golden_label\"].value_counts())\n",
    "# print(qnli_results_df[qnli_results_df[\"source\"]==\"stresstest\"][\"golden_label\"].value_counts())\n",
    "# print(qnli_results_df[qnli_results_df[\"source\"]==\"rtequant\"][\"golden_label\"].value_counts())\n",
    "# print(qnli_results_df[\"golden_label\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def weighted_avg_f1(f1_scores, num_samples_per_class):\n",
    "#     total_samples = sum(num_samples_per_class)\n",
    "#     weighted_sum = 0.0\n",
    "#     total_weight = 0.0\n",
    "#\n",
    "#     for f1, num_samples in zip(f1_scores, num_samples_per_class):\n",
    "#         weight = num_samples / total_samples\n",
    "#         weighted_sum += f1 * weight\n",
    "#         total_weight += weight\n",
    "#\n",
    "#     return round(weighted_sum * 100 / total_weight, 2)\n",
    "#\n",
    "# # E, C, N\n",
    "# # awpnli\n",
    "# print(weighted_avg_f1([0.4972, 0.5028], [89, 80]))\n",
    "# # redditnli\n",
    "# print(weighted_avg_f1([0.5738, 0.0819, 0.3443], [35, 5, 21]))\n",
    "# # newsnli\n",
    "# print(weighted_avg_f1([0.5021, 0.4979], [120, 119]))\n",
    "# # stresstest\n",
    "# print(weighted_avg_f1([0.3139, 0.3829, 0.3032], [382, 466, 369]))\n",
    "# # rtequant\n",
    "# print(weighted_avg_f1([0.4250, 0.5750], [17, 23]))\n",
    "# # equate\n",
    "# print(weighted_avg_f1([0.3704, 0.3232, 0.3064], [643, 561, 532]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EVALUATION 4 - consider missing labels as the wrong label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## INFERENCE LLAMA2 - 7B\n",
    "# model_name = \"llama-2-chat\"\n",
    "# model_display_name = \"Llama-2 7B\"\n",
    "# model_size = \"7B\"\n",
    "## base model\n",
    "# job_id = \"5817639\"\n",
    "## finetuned\n",
    "# job_id = \"5820349_finetuned\"\n",
    "## fintuned with balanced dataset\n",
    "# job_id = \"5836402_finetuned\"\n",
    "\n",
    "## INFERENCE LLAMA2 - 13B\n",
    "# model_name = \"llama-2-chat\"\n",
    "# model_display_name = \"Llama-2 13B\"\n",
    "# model_size = \"13B\"\n",
    "## base model\n",
    "# job_id = \"5822467\"\n",
    "## finetuned\n",
    "# job_id = \"5830782_finetuned\"\n",
    "\n",
    "## INFERENCE CODELLAMA - PYTHON 7B\n",
    "# model_name = \"codellama-python\"\n",
    "# model_display_name = \"CodeLlama-Python 7B\"\n",
    "# model_size = \"7B\"\n",
    "## base model\n",
    "# job_id = \"5852469\"\n",
    "## finetuned\n",
    "# job_id = \"5897597_finetuned\"\n",
    "\n",
    "\n",
    "##### REDO MATRI FOR AWPNLI ABOVE THIS\n",
    "\n",
    "## INFERENCE CODELLAMA - PYTHON 13B\n",
    "# model_name = \"codellama-python\"\n",
    "# model_display_name = \"CodeLlama-Python 13B\"\n",
    "# model_size = \"13B\"\n",
    "## base model\n",
    "# job_id = \"5919626\"\n",
    "## finetuned\n",
    "# job_id = \"\"\n",
    "\n",
    "## INFERENCE CODELLAMA - INSTRUCT 7B\n",
    "# model_name = \"codellama-instruct\"\n",
    "# model_display_name = \"CodeLlama-Instruct 7B\"\n",
    "# model_size = \"7B\"\n",
    "# # base model\n",
    "# job_id = \"5858736\"\n",
    "## finetuned\n",
    "# job_id = \"5892669_finetuned\"\n",
    "\n",
    "## INFERENCE CODELLAMA - INSTRUCT 13B\n",
    "# model_name = \"codellama-instruct\"\n",
    "# model_display_name = \"CodeLlama-Instruct 13B\"\n",
    "# model_size = \"13B\"\n",
    "# # base model\n",
    "# job_id = \"5875184\"\n",
    "# finetuned\n",
    "# job_id = \"6001211_finetuned\"\n",
    "\n",
    "## INFERENCE LLAMA-3 8B\n",
    "# model_name = \"llama3\"\n",
    "# model_size = \"8B\"\n",
    "# # base model\n",
    "# job_id = \"6013398\"\n",
    "\n",
    "results_path = os.path.join(root_path, \"results_inference\", model_name, model_size, job_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df = pd.read_excel(os.path.join(results_path, \"evaluation_new\", \"qnli_classification_results.xlsx\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_label_mask = ~qnli_results_df[\"inferred_label\"].isin([\"contradiction\", \"entailment\", \"neutral\"])\n",
    "qnli_results_df.loc[missing_label_mask, \"inferred_label\"] = \"unknown\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df[\"golden_label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df[\"golden_label\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df[\"inferred_label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qnli_results_df[\"inferred_label\"].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def get_micro_scores(y_true, y_pred):\n",
    "    micro_precision = precision_score(y_true, y_pred, average='micro')\n",
    "    micro_recall = recall_score(y_true, y_pred, average='micro')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    return round(micro_precision, 4), round(micro_recall, 4), round(micro_f1, 4)\n",
    "\n",
    "    # precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    # recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    # f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    # return round(precision, 4), round(recall, 4), round(f1, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_df = print_accuracies(qnli_results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_df, gpt_f1 = print_classification_reports(qnli_results_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation_df = pd.merge(accuracy_df, f1_df, on=\"dataset\", how=\"inner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rows = []\n",
    "# for dataset in list(qnli_results_df[\"source\"].unique()):\n",
    "#     filtered_df = qnli_results_df[qnli_results_df[\"source\"] == dataset]\n",
    "#     p, r, f1 = get_micro_scores(filtered_df[\"golden_label\"], filtered_df[\"inferred_label\"])\n",
    "#     rows.append([dataset_name_mapping[dataset], p, r, f1])\n",
    "#\n",
    "# p, r, f1 =  get_micro_scores(qnli_results_df[\"golden_label\"], qnli_results_df[\"inferred_label\"])\n",
    "# rows.append([\"EQUATE\", p, r, f1])\n",
    "#\n",
    "# micro_scores_df = pd.DataFrame(rows, columns = [\"dataset\", \"micro_P\", \"micro_R\", \"micro_F1\"])\n",
    "# micro_scores_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation_df = pd.merge(evaluation_df, micro_scores_df, on=\"dataset\", how=\"inner\")\n",
    "# evaluation_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation_df.to_excel(os.path.join(results_path, \"evaluation_new\", \"evaluation.xlsx\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "os.makedirs(os.path.join(results_path, \"evaluation_new\", \"cf\"), exist_ok=True)\n",
    "\n",
    "# Define custom colormap from white to black\n",
    "cmap_colors = [(1, 1, 1), (0, 0, 0)]  # White to Black\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", cmap_colors)\n",
    "\n",
    "for dataset in [\"awpnli\", \"stresstest\", \"redditnli\", \"rtequant\", \"newsnli\",]:\n",
    "    filtered_df = qnli_results_df[qnli_results_df[\"source\"] == dataset]\n",
    "    if dataset in [\"rtequant\", \"newsnli\"]:\n",
    "        labels = [\"entailment\", \"neutral\"]\n",
    "    elif dataset == \"awpnli\":\n",
    "        labels = [\"contradiction\", \"entailment\"]\n",
    "    else:\n",
    "        labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "    if \"unknown\" in filtered_df[\"inferred_label\"].unique():\n",
    "        labels.append(\"unknown\")\n",
    "    cf = confusion_matrix(filtered_df[\"golden_label\"], filtered_df[\"inferred_label\"], labels=labels, normalize=\"true\")\n",
    "    display = ConfusionMatrixDisplay(cf, display_labels=[label[0].upper() for label in labels])\n",
    "    # Increase label size within the squares\n",
    "    display.plot(cmap=custom_cmap)\n",
    "    # Increase font size of the numbers within the squares\n",
    "    for text in display.text_.ravel():\n",
    "        text.set_fontsize(20)  # Adjust font size as needed\n",
    "    # Increase font size of the axis labels\n",
    "    plt.xlabel('Predicted Label', fontsize=14)  # X-axis label\n",
    "    plt.ylabel('True Label', fontsize=14)       # Y-axis label\n",
    "    # Increase label size within the squares\n",
    "    plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "    plt.title(f\"{model_display_name} - {dataset_name_mapping[dataset]}\")\n",
    "    plt.savefig(os.path.join(results_path, \"evaluation_new\", \"cf\", f\"{dataset}_confusion_matrix.pdf\"))\n",
    "\n",
    "\n",
    "labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "if \"unknown\" in qnli_results_df[\"inferred_label\"].unique():\n",
    "    labels.append(\"unknown\")\n",
    "cf = confusion_matrix(qnli_results_df[\"golden_label\"], qnli_results_df[\"inferred_label\"], labels=labels, normalize=\"true\")\n",
    "display = ConfusionMatrixDisplay(cf, display_labels=[label[0].upper() for label in labels])\n",
    "display.plot(cmap=custom_cmap)\n",
    "# Increase font size of the numbers within the squares\n",
    "for text in display.text_.ravel():\n",
    "    text.set_fontsize(20)  # Adjust font size as needed\n",
    "# Increase font size of the axis labels\n",
    "plt.xlabel('Predicted Label', fontsize=14)  # X-axis label\n",
    "plt.ylabel('True Label', fontsize=14)       # Y-axis label\n",
    "# Increase label size within the squares\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.title(f\"{model_display_name} - EQUATE\")\n",
    "plt.savefig(os.path.join(results_path, \"evaluation_new\", \"cf\", f\"EQUATE_confusion_matrix.pdf\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BASELINE SCORES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from numpy.random import Generator, BitGenerator\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "#\n",
    "# ## Get the baseline micro metrics for the sub-datasets and for EQUATE\n",
    "#\n",
    "# scores_map = {ds_name: {\"accuracy\": [], \"F1-E\": [], \"F1-N\": [], \"F1-C\": [], \"weighted-F1\": []} for ds_name in dataset_name_mapping.keys()}\n",
    "#\n",
    "# for i in range(2):\n",
    "#     for dataset in list(qnli_results_df[\"source\"].unique()):\n",
    "#         mask_source = qnli_results_df[\"source\"] == dataset\n",
    "#         nan_indices_len = qnli_results_df.loc[mask_source].shape[0]\n",
    "#         label_counts = qnli_results_df[qnli_results_df[\"source\"]==dataset][\"golden_label\"].value_counts(normalize=True)\n",
    "#         bit_generator = np.random.PCG64(i)\n",
    "#         generator = np.random.Generator(bit_generator)\n",
    "#         imputed_labels = generator.choice(label_counts.index, size=nan_indices_len, p=label_counts.values)\n",
    "#         # Assign the sampled indices to the corresponding positions\n",
    "#         qnli_results_df.loc[mask_source, \"random_label\"] = imputed_labels\n",
    "#         filtered_df = qnli_results_df[mask_source]\n",
    "#         true_y, pred_y = filtered_df[\"golden_label\"], filtered_df[\"inferred_label\"]\n",
    "#         scores_map[dataset][\"accuracy\"].append(accuracy_score(true_y, pred_y))\n",
    "#         scores_map[dataset][\"weighted-F1\"].append(f1_score(true_y, pred_y, average=\"weighted\"))\n",
    "#         if dataset in [\"rtequant\", \"newsnli\"]:\n",
    "#             labels = [\"entailment\", \"neutral\"]\n",
    "#         elif dataset == \"awpnli\":\n",
    "#             labels = [\"contradiction\", \"entailment\"]\n",
    "#         else:\n",
    "#             labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "#         _, _, clsf_f1_scores, _ = precision_recall_fscore_support(true_y, pred_y)\n",
    "#         for label, f1 in zip (labels, clsf_f1_scores):\n",
    "#             scores_map[dataset][f\"F1-{label[0].upper()}\"].append(f1)\n",
    "#     true_y, pred_y = qnli_results_df[\"golden_label\"], qnli_results_df[\"inferred_label\"]\n",
    "#     scores_map[\"equate\"][\"accuracy\"].append(accuracy_score(true_y, pred_y))\n",
    "#     scores_map[\"equate\"][\"weighted-F1\"].append(f1_score(true_y, pred_y, average=\"weighted\"))\n",
    "#     labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "#     _, _, clsf_f1_scores, _ = precision_recall_fscore_support(true_y, pred_y)\n",
    "#     for label, f1 in zip (labels, clsf_f1_scores):\n",
    "#         scores_map[\"equate\"][f\"F1-{label[0].upper()}\"].append(f1)\n",
    "#\n",
    "# scores_map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for dataset, metrics in scores_map.items():\n",
    "#     print(f\"###############{dataset}##############\")\n",
    "#     for metric, scores_list in metrics.items():\n",
    "#         print(f\"{metric}: {round(np.mean(scores_list), 4)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# import numpy as np\n",
    "#\n",
    "# # Example true labels and predicted labels\n",
    "# true_labels = qnli_results_df[\"golden_label\"]  # True class labels\n",
    "# predicted_labels = qnli_results_df[\"inferred_label\"]  # Predicted class labels\n",
    "#\n",
    "# # Example class frequencies (number of samples for each class)\n",
    "# print(qnli_results_df[\"golden_label\"].value_counts(normalize=True))\n",
    "# class_frequencies = qnli_results_df[\"golden_label\"].value_counts().values  # List containing the number of samples for each class\n",
    "#\n",
    "# # Compute F1 score for each class\n",
    "# f1_scores = []\n",
    "# for class_id in range(len(class_frequencies)):\n",
    "#     mask = np.array(true_labels) == class_id\n",
    "#     f1_scores.append(f1_score(np.array(true_labels)[mask], np.array(predicted_labels)[mask]))\n",
    "#\n",
    "# # Calculate weighted F1 score\n",
    "# weighted_f1_score = np.sum(np.array(f1_scores) * np.array(class_frequencies)) / np.sum(class_frequencies)\n",
    "#\n",
    "# print(\"Weighted F1 Score:\", weighted_f1_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
